#!/usr/bin/env python3
"""
æ‰¹é‡å®éªŒè„šæœ¬ï¼šç³»ç»Ÿæ€§æµ‹è¯•TimeXeræ¨¡å‹åœ¨ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½
æµ‹è¯•é…ç½®ï¼š
1. æ— GCN + æ— æ–°é—»
2. æœ‰GCN + æ— æ–°é—»  
3. æ— GCN + æœ‰æ–°é—»
4. æœ‰GCN + æœ‰æ–°é—»

æ¯ä¸ªé…ç½®è¿è¡Œå¤šæ¬¡ä»¥è¯„ä¼°ç¨³å®šæ€§
"""

import os
import sys
import subprocess
import json
import pandas as pd
from datetime import datetime
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.join(current_dir, '..', '..')
sys.path.append(project_root)

# å®éªŒé…ç½®
EXPERIMENT_CONFIGS = [
    {"use_gcn": False, "use_news": False, "name": "baseline"},
    {"use_gcn": True, "use_news": False, "name": "gcn_only"},
    {"use_gcn": False, "use_news": True, "name": "news_only"},
    {"use_gcn": True, "use_news": True, "name": "gcn_news"},
]

# å®éªŒå‚æ•°
NUM_RUNS_PER_CONFIG = 5  # æ¯ä¸ªé…ç½®è¿è¡Œ5æ¬¡
RESULTS_DIR = "experiments/batch_results"
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")

def create_modified_training_script(config, run_id):
    """åˆ›å»ºä¿®æ”¹åçš„è®­ç»ƒè„šæœ¬"""
    script_path = f"scripts/training/train_timexer_temp_{config['name']}_run{run_id}.py"
    
    # è¯»å–åŸå§‹è„šæœ¬
    with open("scripts/training/train_timexer.py", "r") as f:
        content = f.read()
    
    # ä¿®æ”¹é…ç½®
    content = content.replace(
        "USE_GCN = True", 
        f"USE_GCN = {config['use_gcn']}"
    )
    content = content.replace(
        "USE_NEWS_FEATURES = False", 
        f"USE_NEWS_FEATURES = {config['use_news']}"
    )
    
    # ä¿®æ”¹éšæœºç§å­ä»¥ç¡®ä¿æ¯æ¬¡è¿è¡Œéƒ½ä¸åŒ
    content = content.replace(
        "RANDOM_SEED = 42", 
        f"RANDOM_SEED = {42 + run_id}"
    )
    
    # ä¿®æ”¹æ¨¡å‹ä¿å­˜è·¯å¾„
    content = content.replace(
        'BEST_MODEL_NAME = "best_timexer_model.pt"',
        f'BEST_MODEL_NAME = "best_timexer_{config["name"]}_run{run_id}.pt"'
    )
    
    # å†™å…¥ä¸´æ—¶è„šæœ¬
    with open(script_path, "w") as f:
        f.write(content)
    
    return script_path

def run_experiment(config, run_id):
    """è¿è¡Œå•ä¸ªå®éªŒ"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ è¿è¡Œå®éªŒ: {config['name']} - Run {run_id+1}")
    print(f"   GCN: {config['use_gcn']}, News: {config['use_news']}")
    print(f"{'='*60}")
    
    # åˆ›å»ºä¿®æ”¹åçš„è®­ç»ƒè„šæœ¬
    script_path = create_modified_training_script(config, run_id)
    
    try:
        # è¿è¡Œè®­ç»ƒè„šæœ¬
        result = subprocess.run(
            [sys.executable, script_path],
            cwd=project_root,
            capture_output=True,
            text=True,
            timeout=3600  # 1å°æ—¶è¶…æ—¶
        )
        
        # è§£æç»“æœ
        if result.returncode == 0:
            # ä»è¾“å‡ºä¸­æå–æµ‹è¯•æŒ‡æ ‡
            output_lines = result.stdout.split('\n')
            test_metrics = extract_test_metrics(output_lines)
            test_metrics['success'] = True
            test_metrics['error'] = None
        else:
            print(f"âŒ å®éªŒå¤±è´¥: {result.stderr}")
            test_metrics = {
                'success': False,
                'error': result.stderr,
                'mae': None,
                'mse': None,
                'rmse': None,
                'r2': None,
                'mape': None,
                'new_mae': None,
                'normalized_mae': None,
                'normalized_mse': None,
                'normalized_rmse': None
            }
    
    except subprocess.TimeoutExpired:
        print(f"â° å®éªŒè¶…æ—¶")
        test_metrics = {
            'success': False,
            'error': 'Timeout',
            'mae': None,
            'mse': None,
            'rmse': None,
            'r2': None,
            'mape': None,
            'new_mae': None,
            'normalized_mae': None,
            'normalized_mse': None,
            'normalized_rmse': None
        }
    
    except Exception as e:
        print(f"âŒ å®éªŒå¼‚å¸¸: {e}")
        test_metrics = {
            'success': False,
            'error': str(e),
            'mae': None,
            'mse': None,
            'rmse': None,
            'r2': None,
            'mape': None,
            'new_mae': None,
            'normalized_mae': None,
            'normalized_mse': None,
            'normalized_rmse': None
        }
    
    finally:
        # æ¸…ç†ä¸´æ—¶è„šæœ¬
        if os.path.exists(script_path):
            os.remove(script_path)
    
    return test_metrics

def extract_test_metrics(output_lines):
    """ä»è¾“å‡ºä¸­æå–æµ‹è¯•æŒ‡æ ‡"""
    metrics = {}

    for line in output_lines:
        line = line.strip()
        # åŸæœ‰æŒ‡æ ‡
        if "- MAE:" in line:
            try: metrics['mae'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- MSE:" in line:
            try: metrics['mse'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- RMSE:" in line:
            try: metrics['rmse'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- R2:" in line:
            try: metrics['r2'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- MAPE:" in line:
            try: metrics['mape'] = float(line.split(":")[-1].strip())
            except: pass
        # æ–°å¢æŒ‡æ ‡
        elif "- NEW_MAE:" in line:
            try: metrics['new_mae'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- NORMALIZED_MAE:" in line:
            try: metrics['normalized_mae'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- NORMALIZED_MSE:" in line:
            try: metrics['normalized_mse'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- NORMALIZED_RMSE:" in line:
            try: metrics['normalized_rmse'] = float(line.split(":")[-1].strip())
            except: pass

    return metrics

def save_results(all_results):
    """ä¿å­˜å®éªŒç»“æœ"""
    os.makedirs(RESULTS_DIR, exist_ok=True)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = os.path.join(RESULTS_DIR, f"batch_experiment_{TIMESTAMP}.json")
    with open(results_file, "w") as f:
        json.dump(all_results, f, indent=2)
    
    # åˆ›å»ºæ±‡æ€»è¡¨æ ¼
    summary_data = []
    for config_name, runs in all_results.items():
        successful_runs = [r for r in runs if r['success']]
        if successful_runs:
            # é‡ç‚¹å…³æ³¨çš„æŒ‡æ ‡
            priority_metrics = ['new_mae', 'r2', 'mape', 'normalized_mae', 'normalized_mse', 'normalized_rmse']
            # å¤‡ç”¨æŒ‡æ ‡
            backup_metrics = ['mae', 'mse', 'rmse']

            summary_row = {'config': config_name}

            # ä¼˜å…ˆå¤„ç†é‡ç‚¹æŒ‡æ ‡
            for metric in priority_metrics:
                values = [r[metric] for r in successful_runs if r.get(metric) is not None]
                if values:
                    summary_row[f'{metric}_mean'] = np.mean(values)
                    summary_row[f'{metric}_std'] = np.std(values)
                    summary_row[f'{metric}_min'] = np.min(values)
                    summary_row[f'{metric}_max'] = np.max(values)

            # å¦‚æœæ²¡æœ‰é‡ç‚¹æŒ‡æ ‡ï¼Œä½¿ç”¨å¤‡ç”¨æŒ‡æ ‡
            if not any(f'{m}_mean' in summary_row for m in priority_metrics):
                for metric in backup_metrics:
                    values = [r[metric] for r in successful_runs if r.get(metric) is not None]
                    if values:
                        summary_row[f'{metric}_mean'] = np.mean(values)
                        summary_row[f'{metric}_std'] = np.std(values)
                        summary_row[f'{metric}_min'] = np.min(values)
                        summary_row[f'{metric}_max'] = np.max(values)

            summary_row['success_rate'] = len(successful_runs) / len(runs)
            summary_data.append(summary_row)
    
    # ä¿å­˜æ±‡æ€»è¡¨æ ¼
    summary_df = pd.DataFrame(summary_data)
    summary_file = os.path.join(RESULTS_DIR, f"batch_summary_{TIMESTAMP}.csv")
    summary_df.to_csv(summary_file, index=False)
    
    print(f"\nğŸ“Š ç»“æœå·²ä¿å­˜:")
    print(f"   è¯¦ç»†ç»“æœ: {results_file}")
    print(f"   æ±‡æ€»è¡¨æ ¼: {summary_file}")
    
    return summary_df

def main():
    """ä¸»å‡½æ•°"""
    print(f"ğŸ”¬ å¼€å§‹æ‰¹é‡å®éªŒ - {TIMESTAMP}")
    print(f"ğŸ“‹ å®éªŒé…ç½®: {len(EXPERIMENT_CONFIGS)} ä¸ªé…ç½®ï¼Œæ¯ä¸ªè¿è¡Œ {NUM_RUNS_PER_CONFIG} æ¬¡")
    
    all_results = {}
    
    for config in EXPERIMENT_CONFIGS:
        config_results = []
        
        for run_id in range(NUM_RUNS_PER_CONFIG):
            result = run_experiment(config, run_id)
            result['config'] = config['name']
            result['run_id'] = run_id
            result['use_gcn'] = config['use_gcn']
            result['use_news'] = config['use_news']
            config_results.append(result)
        
        all_results[config['name']] = config_results
    
    # ä¿å­˜å’Œåˆ†æç»“æœ
    summary_df = save_results(all_results)
    
    # æ‰“å°æ±‡æ€»
    print(f"\n{'='*80}")
    print("ğŸ“ˆ å®éªŒæ±‡æ€» (é‡ç‚¹æŒ‡æ ‡)")
    print(f"{'='*80}")

    if not summary_df.empty:
        for _, row in summary_df.iterrows():
            config_name = row['config']
            success_rate = row['success_rate']

            # ä¼˜å…ˆæ˜¾ç¤ºNEW_MAE
            if 'new_mae_mean' in row and pd.notna(row['new_mae_mean']):
                print(f"{config_name:15} | NEW_MAE: {row['new_mae_mean']:.4f} Â± {row['new_mae_std']:.4f} | æˆåŠŸç‡: {success_rate:.1%}")
            elif 'mae_mean' in row and pd.notna(row['mae_mean']):
                print(f"{config_name:15} | MAE: {row['mae_mean']:.4f} Â± {row['mae_std']:.4f} | æˆåŠŸç‡: {success_rate:.1%}")
            else:
                print(f"{config_name:15} | æ— æœ‰æ•ˆæŒ‡æ ‡ | æˆåŠŸç‡: {success_rate:.1%}")

            # æ˜¾ç¤ºå…¶ä»–é‡è¦æŒ‡æ ‡
            additional_metrics = []
            if 'r2_mean' in row and pd.notna(row['r2_mean']):
                additional_metrics.append(f"RÂ²: {row['r2_mean']:.4f}")
            if 'mape_mean' in row and pd.notna(row['mape_mean']):
                additional_metrics.append(f"MAPE: {row['mape_mean']*100:.2f}%")
            if 'normalized_mae_mean' in row and pd.notna(row['normalized_mae_mean']):
                additional_metrics.append(f"NORM_MAE: {row['normalized_mae_mean']:.4f}")

            if additional_metrics:
                print(f"{'':15}   {' | '.join(additional_metrics)}")

    print(f"\nâœ… æ‰¹é‡å®éªŒå®Œæˆ!")

if __name__ == "__main__":
    main()
