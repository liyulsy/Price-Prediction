#!/usr/bin/env python3
"""
æ‰¹é‡è¿è¡Œæ‰€æœ‰ç»Ÿä¸€çš„è®­ç»ƒè„šæœ¬å¹¶æ¯”è¾ƒç»“æœ
"""

import subprocess
import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.join(current_dir, '..', '..')
sys.path.append(project_root)

# é…ç½®
MODELS_TO_RUN = [
    'train_timemixer.py',
    'train_timexer.py', 
    'train_cnn.py',
    'train_lstm.py'
]

RESULTS_DIR = "experiments/cache/unified_comparison"

def run_training_script(script_name):
    """è¿è¡Œå•ä¸ªè®­ç»ƒè„šæœ¬"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ Running {script_name}")
    print(f"{'='*60}")
    
    script_path = os.path.join(current_dir, script_name)
    
    if not os.path.exists(script_path):
        print(f"âŒ Script not found: {script_path}")
        return False, None
    
    start_time = time.time()
    
    try:
        # è¿è¡Œè„šæœ¬
        result = subprocess.run([
            sys.executable, script_path
        ], capture_output=True, text=True, cwd=project_root)
        
        end_time = time.time()
        duration = end_time - start_time
        
        if result.returncode == 0:
            print(f"âœ… {script_name} completed successfully in {duration:.1f}s")
            return True, {
                'script': script_name,
                'success': True,
                'duration': duration,
                'stdout': result.stdout,
                'stderr': result.stderr
            }
        else:
            print(f"âŒ {script_name} failed with return code {result.returncode}")
            print(f"Error output: {result.stderr}")
            return False, {
                'script': script_name,
                'success': False,
                'duration': duration,
                'stdout': result.stdout,
                'stderr': result.stderr
            }
            
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        print(f"âŒ Exception running {script_name}: {str(e)}")
        return False, {
            'script': script_name,
            'success': False,
            'duration': duration,
            'error': str(e)
        }

def extract_metrics_from_output(output_text, script_name):
    """ä»è¾“å‡ºä¸­æå–æŒ‡æ ‡"""
    metrics = {
        'model': script_name.replace('train_', '').replace('.py', ''),
        'script': script_name
    }
    
    lines = output_text.split('\n')
    
    # æŸ¥æ‰¾æµ‹è¯•ç»“æœéƒ¨åˆ†
    in_test_results = False
    for line in lines:
        line = line.strip()
        
        if "âœ… Test Results:" in line or "Test Results:" in line:
            in_test_results = True
            continue
            
        if in_test_results and line.startswith("- "):
            # è§£ææŒ‡æ ‡è¡Œï¼Œæ ¼å¼å¦‚: "- MAE: 0.1234"
            if ":" in line:
                metric_name = line.split(":")[0].replace("- ", "").strip().lower()
                try:
                    metric_value = float(line.split(":")[1].strip())
                    metrics[metric_name] = metric_value
                except:
                    pass
        
        # å¦‚æœé‡åˆ°å…¶ä»–éƒ¨åˆ†ï¼Œåœæ­¢è§£æ
        if in_test_results and ("---" in line and "Per-Coin" in line):
            break
    
    return metrics

def compare_results(all_results):
    """æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹çš„ç»“æœ"""
    print(f"\n{'='*80}")
    print("ğŸ“Š MODEL COMPARISON RESULTS")
    print(f"{'='*80}")
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(RESULTS_DIR, exist_ok=True)
    
    # æå–æŒ‡æ ‡
    metrics_list = []
    for result in all_results:
        if result['success']:
            metrics = extract_metrics_from_output(result['stdout'], result['script'])
            metrics['duration'] = result['duration']
            metrics_list.append(metrics)
        else:
            print(f"âŒ {result['script']} failed, skipping from comparison")
    
    if not metrics_list:
        print("âŒ No successful runs to compare")
        return
    
    # åˆ›å»ºæ¯”è¾ƒè¡¨
    comparison_df = pd.DataFrame(metrics_list)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    detailed_file = os.path.join(RESULTS_DIR, f"detailed_comparison_{timestamp}.csv")
    comparison_df.to_csv(detailed_file, index=False)
    
    # æ˜¾ç¤ºä¸»è¦æŒ‡æ ‡æ¯”è¾ƒ
    key_metrics = ['mae', 'new_mae', 'r2', 'mse', 'mape']
    available_metrics = [m for m in key_metrics if m in comparison_df.columns]
    
    if available_metrics:
        print("\nğŸ† RANKING BY KEY METRICS:")
        print("-" * 60)
        
        for metric in available_metrics:
            print(f"\nğŸ“ˆ {metric.upper()} Results:")
            if metric in ['mae', 'mse', 'mape']:  # Lower is better
                sorted_df = comparison_df.sort_values(metric)
                print("   (Lower is better)")
            elif metric == 'new_mae':  # Closer to 1.0 is better
                comparison_df['new_mae_diff'] = abs(comparison_df[metric] - 1.0)
                sorted_df = comparison_df.sort_values('new_mae_diff')
                print("   (Closer to 1.0 is better)")
            else:  # Higher is better (r2)
                sorted_df = comparison_df.sort_values(metric, ascending=False)
                print("   (Higher is better)")
            
            for i, (_, row) in enumerate(sorted_df.iterrows(), 1):
                model_name = row['model'].upper()
                value = row[metric]
                duration = row.get('duration', 0)
                print(f"   {i}. {model_name:12} {value:8.6f} ({duration:.1f}s)")
    
    # ç»¼åˆæ’å
    print(f"\nğŸ¯ OVERALL PERFORMANCE SUMMARY:")
    print("-" * 60)
    
    # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆç®€å•å¹³å‡æ’åï¼‰
    ranking_scores = {}
    
    for _, row in comparison_df.iterrows():
        model = row['model']
        ranking_scores[model] = []
    
    # ä¸ºæ¯ä¸ªæŒ‡æ ‡è®¡ç®—æ’å
    for metric in available_metrics:
        if metric in ['mae', 'mse', 'mape']:
            sorted_models = comparison_df.sort_values(metric)['model'].tolist()
        elif metric == 'new_mae':
            comparison_df['new_mae_diff'] = abs(comparison_df[metric] - 1.0)
            sorted_models = comparison_df.sort_values('new_mae_diff')['model'].tolist()
        else:  # r2
            sorted_models = comparison_df.sort_values(metric, ascending=False)['model'].tolist()
        
        for i, model in enumerate(sorted_models):
            ranking_scores[model].append(i + 1)  # æ’åä»1å¼€å§‹
    
    # è®¡ç®—å¹³å‡æ’å
    avg_rankings = {}
    for model, ranks in ranking_scores.items():
        if ranks:
            avg_rankings[model] = np.mean(ranks)
    
    # æŒ‰å¹³å‡æ’åæ’åº
    sorted_models = sorted(avg_rankings.items(), key=lambda x: x[1])
    
    print("\nOverall Ranking (based on average rank across all metrics):")
    for i, (model, avg_rank) in enumerate(sorted_models, 1):
        model_row = comparison_df[comparison_df['model'] == model].iloc[0]
        duration = model_row.get('duration', 0)
        print(f"   {i}. {model.upper():12} (avg rank: {avg_rank:.1f}, time: {duration:.1f}s)")
    
    # æ¨èæœ€ä½³æ¨¡å‹
    if sorted_models:
        best_model = sorted_models[0][0]
        print(f"\nğŸ† RECOMMENDED MODEL: {best_model.upper()}")
        
        best_row = comparison_df[comparison_df['model'] == best_model].iloc[0]
        print(f"   Performance highlights:")
        for metric in available_metrics:
            if metric in best_row:
                print(f"   - {metric.upper()}: {best_row[metric]:.6f}")
    
    print(f"\nğŸ“ Detailed results saved to: {detailed_file}")
    
    # ä¿å­˜ç®€åŒ–çš„æ’åç»“æœ
    ranking_file = os.path.join(RESULTS_DIR, f"model_ranking_{timestamp}.csv")
    ranking_data = []
    for i, (model, avg_rank) in enumerate(sorted_models, 1):
        model_row = comparison_df[comparison_df['model'] == model].iloc[0]
        ranking_data.append({
            'rank': i,
            'model': model,
            'avg_rank': avg_rank,
            'duration': model_row.get('duration', 0),
            **{metric: model_row.get(metric, 0) for metric in available_metrics}
        })
    
    ranking_df = pd.DataFrame(ranking_data)
    ranking_df.to_csv(ranking_file, index=False)
    print(f"ğŸ“ Model ranking saved to: {ranking_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Starting Unified Model Comparison")
    print(f"Models to run: {MODELS_TO_RUN}")
    
    all_results = []
    successful_runs = 0
    
    start_time = time.time()
    
    # è¿è¡Œæ‰€æœ‰æ¨¡å‹
    for script in MODELS_TO_RUN:
        success, result = run_training_script(script)
        all_results.append(result)
        if success:
            successful_runs += 1
    
    total_time = time.time() - start_time
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ EXECUTION SUMMARY")
    print(f"{'='*60}")
    print(f"Total scripts: {len(MODELS_TO_RUN)}")
    print(f"Successful runs: {successful_runs}")
    print(f"Failed runs: {len(MODELS_TO_RUN) - successful_runs}")
    print(f"Total time: {total_time:.1f}s")
    
    # æ¯”è¾ƒç»“æœ
    if successful_runs > 0:
        compare_results(all_results)
    else:
        print("âŒ No successful runs to compare")
    
    print(f"\nâœ… Unified model comparison completed!")

if __name__ == "__main__":
    main()
