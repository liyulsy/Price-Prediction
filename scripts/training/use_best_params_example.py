#!/usr/bin/env python3
"""
ä½¿ç”¨æœ€ä½³å‚æ•°çš„ç¤ºä¾‹è„šæœ¬

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•åŠ è½½å’Œä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–å¾—åˆ°çš„æœ€ä½³å‚æ•°æ¥è®­ç»ƒWPMixeræ¨¡å‹ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/training/use_best_params_example.py
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
import pandas as pd
from datetime import datetime

# Add the project root to Python path
current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()
project_root = os.path.join(current_dir, '..', '..')
sys.path.append(project_root)

from models.MixModel.unified_wpmixer import UnifiedWPMixer
from scripts.analysis.crypto_new_analyzer.unified_dataset import UnifiedCryptoDataset
from scripts.training.load_best_params import (
    load_best_params_json, create_wpmixer_config_from_params, 
    create_training_config_from_params, find_latest_params_file
)

def load_best_parameters():
    """åŠ è½½æœ€ä½³å‚æ•°"""
    print("ğŸ“‚ åŠ è½½æœ€ä½³å‚æ•°...")
    
    # æŸ¥æ‰¾æœ€æ–°çš„å‚æ•°æ–‡ä»¶
    params_file = find_latest_params_file()
    
    if params_file is None:
        print("âŒ æœªæ‰¾åˆ°å‚æ•°æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
        return None
    
    print(f"âœ… æ‰¾åˆ°å‚æ•°æ–‡ä»¶: {params_file}")
    
    # åŠ è½½å‚æ•°
    params = load_best_params_json(params_file)
    
    if params is None:
        print("âŒ å‚æ•°åŠ è½½å¤±è´¥")
        return None
    
    print("âœ… å‚æ•°åŠ è½½æˆåŠŸ")
    return params

def create_model_with_best_params(params):
    """ä½¿ç”¨æœ€ä½³å‚æ•°åˆ›å»ºæ¨¡å‹"""
    print("ğŸ—ï¸ åˆ›å»ºWPMixeræ¨¡å‹...")
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    configs = create_wpmixer_config_from_params(params)
    
    # åˆ›å»ºæ¨¡å‹
    model = UnifiedWPMixer(
        configs=configs,
        use_gcn=False,  # ä¸ä½¿ç”¨GCN
        gcn_config='improved_light',
        news_feature_dim=None,  # ä¸ä½¿ç”¨æ–°é—»ç‰¹å¾
        gcn_hidden_dim=256,
        gcn_output_dim=128,
        news_processed_dim=64,
        mlp_hidden_dim_1=params.get('mlp_hidden_dim_1', 1024),
        mlp_hidden_dim_2=params.get('mlp_hidden_dim_2', 512),
        num_classes=1
    )
    
    print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    return model, configs

def prepare_data_with_best_params(params):
    """ä½¿ç”¨æœ€ä½³å‚æ•°å‡†å¤‡æ•°æ®"""
    print("ğŸ“Š å‡†å¤‡æ•°æ®...")
    
    # æ•°æ®è·¯å¾„
    price_csv_path = 'scripts/analysis/crypto_analysis/data/processed_data/1H/all_1H.csv'
    
    if not os.path.exists(price_csv_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {price_csv_path}")
        return None, None, None
    
    # åŠ è½½ä»·æ ¼æ•°æ®
    price_df_raw = pd.read_csv(price_csv_path, index_col=0, parse_dates=True)
    coin_names = ['BTC', 'ETH', 'BNB', 'XRP', 'LTC', 'DOGE', 'SOL', 'AVAX']
    rename_map = {f"{coin}-USDT": coin for coin in coin_names}
    price_df_full = price_df_raw.rename(columns=rename_map)[coin_names]
    
    # ç¡®ä¿æ—¶é—´ç´¢å¼•æ˜¯å‡åºæ’åˆ—
    if not price_df_full.index.is_monotonic_increasing:
        price_df_full = price_df_full.sort_index()
    
    # åˆ›å»ºæ•°æ®é›†
    seq_len = params.get('price_seq_len', 60)
    dataset = UnifiedCryptoDataset(
        price_data_df=price_df_full,
        news_data_dict=None,
        seq_len=seq_len,
        processed_news_features_path=None,
        force_recompute_news=False,
        time_encoding_enabled=True,
        time_freq='h',
    )
    
    # æ•°æ®é›†åˆ’åˆ†
    total_size = len(dataset)
    test_size = int(0.15 * total_size)
    val_size = int(0.15 * total_size)
    train_size = total_size - test_size - val_size
    
    train_indices = list(range(0, train_size))
    val_indices = list(range(train_size, train_size + val_size))
    test_indices = list(range(train_size + val_size, total_size))
    
    # åˆ›å»ºæ•°æ®å­é›†
    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)
    test_dataset = Subset(dataset, test_indices)
    
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
    print(f"   åºåˆ—é•¿åº¦: {seq_len}")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)}")
    print(f"   éªŒè¯é›†: {len(val_dataset)}")
    print(f"   æµ‹è¯•é›†: {len(test_dataset)}")
    
    return train_dataset, val_dataset, test_dataset

def train_model_with_best_params(model, train_dataset, val_dataset, params):
    """ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹"""
    print("ğŸƒ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    training_config = create_training_config_from_params(params)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=training_config['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=training_config['batch_size'], shuffle=False)
    
    # è®¾ç½®è®­ç»ƒç»„ä»¶
    criterion = nn.MSELoss()
    optimizer = optim.Adam(
        model.parameters(), 
        lr=training_config['learning_rate'], 
        weight_decay=training_config['weight_decay']
    )
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.8)
    
    print(f"ğŸ“‹ è®­ç»ƒé…ç½®:")
    for key, value in training_config.items():
        if key in ['learning_rate', 'weight_decay']:
            print(f"   {key}: {value:.8f}")
        else:
            print(f"   {key}: {value}")
    
    # è®­ç»ƒå¾ªç¯ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä»…è®­ç»ƒå‡ ä¸ªepochä½œä¸ºç¤ºä¾‹ï¼‰
    num_epochs = training_config['epochs']  # ä½¿ç”¨æœ€ä½³å‚æ•°ä¸­çš„å®Œæ•´epochæ•°
    print(f"ğŸ”„ å¼€å§‹è®­ç»ƒ {num_epochs} ä¸ªepochï¼ˆç¤ºä¾‹ï¼‰...")
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        
        for batch_idx, batch_data in enumerate(train_loader):
            price_seq = batch_data['price_seq'].to(device)
            target_data = batch_data['target_price'].to(device)
            
            optimizer.zero_grad()
            outputs = model(price_data=price_seq)
            outputs = outputs.squeeze(-1)
            
            loss = criterion(outputs, target_data)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
            
            # åªå¤„ç†å‰å‡ ä¸ªbatchä½œä¸ºç¤ºä¾‹
            if batch_idx >= 10:
                break
        
        avg_train_loss = train_loss / min(len(train_loader), 11)
        
        # éªŒè¯é˜¶æ®µï¼ˆç®€åŒ–ï¼‰
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for batch_idx, batch_data in enumerate(val_loader):
                price_seq = batch_data['price_seq'].to(device)
                target_data = batch_data['target_price'].to(device)
                
                outputs = model(price_data=price_seq)
                outputs = outputs.squeeze(-1)
                
                loss = criterion(outputs, target_data)
                val_loss += loss.item()
                
                # åªå¤„ç†å‰å‡ ä¸ªbatchä½œä¸ºç¤ºä¾‹
                if batch_idx >= 5:
                    break
        
        avg_val_loss = val_loss / min(len(val_loader), 6)
        scheduler.step(avg_val_loss)
        
        print(f"Epoch {epoch+1}/{num_epochs}: è®­ç»ƒæŸå¤±={avg_train_loss:.6f}, éªŒè¯æŸå¤±={avg_val_loss:.6f}")
    
    print("âœ… è®­ç»ƒå®Œæˆï¼ˆç¤ºä¾‹ï¼‰")
    return model

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒWPMixerç¤ºä¾‹")
    print("="*50)
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. åŠ è½½æœ€ä½³å‚æ•°
    params = load_best_parameters()
    if params is None:
        print("âŒ æ— æ³•åŠ è½½å‚æ•°ï¼Œé€€å‡º")
        return
    
    # 2. åˆ›å»ºæ¨¡å‹
    model, configs = create_model_with_best_params(params)
    
    # 3. å‡†å¤‡æ•°æ®
    train_dataset, val_dataset, test_dataset = prepare_data_with_best_params(params)
    if train_dataset is None:
        print("âŒ æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œé€€å‡º")
        return
    
    # 4. è®­ç»ƒæ¨¡å‹
    trained_model = train_model_with_best_params(model, train_dataset, val_dataset, params)
    
    print(f"\nğŸ‰ ç¤ºä¾‹å®Œæˆ!")
    print(f"â° ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"\nğŸ’¡ è¿™åªæ˜¯ä¸€ä¸ªä½¿ç”¨æœ€ä½³å‚æ•°çš„ç¤ºä¾‹ã€‚")
    print(f"   å®é™…ä½¿ç”¨æ—¶ï¼Œè¯·æ ¹æ®éœ€è¦è°ƒæ•´è®­ç»ƒè½®æ•°å’Œå…¶ä»–è®¾ç½®ã€‚")
    print(f"\nğŸ“ æœ€ä½³å‚æ•°æ–‡ä»¶ä½ç½®:")
    params_file = find_latest_params_file()
    if params_file:
        print(f"   {params_file}")

if __name__ == '__main__':
    main()
