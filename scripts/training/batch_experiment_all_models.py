#!/usr/bin/env python3
"""
æ‰¹é‡å®éªŒè„šæœ¬ï¼šæµ‹è¯•æ‰€æœ‰æ¨¡å‹åœ¨ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½
æµ‹è¯•æ¨¡å‹ï¼šTimeXer, LSTM, CNN, TimeMixer
æµ‹è¯•é…ç½®ï¼šæ— GCN+æ— æ–°é—», æœ‰GCN+æ— æ–°é—», æ— GCN+æœ‰æ–°é—», æœ‰GCN+æœ‰æ–°é—»
"""

import os
import sys
import subprocess
import json
import pandas as pd
from datetime import datetime
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.join(current_dir, '..', '..')
sys.path.append(project_root)

# æ¨¡å‹é…ç½®
MODELS = {
    'timexer': 'scripts/training/train_timexer.py',
    'lstm': 'scripts/training/train_lstm.py', 
    'cnn': 'scripts/training/train_cnn.py',
    'timemixer': 'scripts/training/train_timemixer.py'
}

# å®éªŒé…ç½®
EXPERIMENT_CONFIGS = [
    {"use_gcn": False, "use_news": False, "name": "baseline"},
    {"use_gcn": True, "use_news": False, "name": "gcn_only"},
    {"use_gcn": False, "use_news": True, "name": "news_only"},
    {"use_gcn": True, "use_news": True, "name": "gcn_news"},
]

# å®éªŒå‚æ•°
NUM_RUNS_PER_CONFIG = 3  # æ¯ä¸ªé…ç½®è¿è¡Œ3æ¬¡
RESULTS_DIR = "experiments/batch_results_all_models"
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")

def create_modified_training_script(model_name, script_path, config, run_id):
    """åˆ›å»ºä¿®æ”¹åçš„è®­ç»ƒè„šæœ¬"""
    temp_script_path = f"scripts/training/train_{model_name}_temp_{config['name']}_run{run_id}.py"
    
    # è¯»å–åŸå§‹è„šæœ¬
    with open(script_path, "r") as f:
        content = f.read()
    
    # ä¿®æ”¹é…ç½®
    content = content.replace(
        "USE_GCN = True", 
        f"USE_GCN = {config['use_gcn']}"
    )
    content = content.replace(
        "USE_NEWS_FEATURES = False", 
        f"USE_NEWS_FEATURES = {config['use_news']}"
    )
    content = content.replace(
        "USE_NEWS_FEATURES = True", 
        f"USE_NEWS_FEATURES = {config['use_news']}"
    )
    
    # ä¿®æ”¹éšæœºç§å­
    content = content.replace(
        "RANDOM_SEED = 42", 
        f"RANDOM_SEED = {42 + run_id}"
    )
    
    # ä¿®æ”¹æ¨¡å‹ä¿å­˜è·¯å¾„
    if "best_timexer_model.pt" in content:
        content = content.replace(
            'BEST_MODEL_NAME = "best_timexer_model.pt"',
            f'BEST_MODEL_NAME = "best_{model_name}_{config["name"]}_run{run_id}.pt"'
        )
    elif "best_lstm_model.pt" in content:
        content = content.replace(
            'BEST_MODEL_NAME = "best_lstm_model.pt"',
            f'BEST_MODEL_NAME = "best_{model_name}_{config["name"]}_run{run_id}.pt"'
        )
    elif "best_cnn_model.pt" in content:
        content = content.replace(
            'BEST_MODEL_NAME = "best_cnn_model.pt"',
            f'BEST_MODEL_NAME = "best_{model_name}_{config["name"]}_run{run_id}.pt"'
        )
    elif "best_unified_multiscale.pt" in content:
        content = content.replace(
            'BEST_MODEL_NAME = "best_unified_multiscale.pt"',
            f'BEST_MODEL_NAME = "best_{model_name}_{config["name"]}_run{run_id}.pt"'
        )
    
    # å†™å…¥ä¸´æ—¶è„šæœ¬
    with open(temp_script_path, "w") as f:
        f.write(content)
    
    return temp_script_path

def run_experiment(model_name, script_path, config, run_id):
    """è¿è¡Œå•ä¸ªå®éªŒ"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ è¿è¡Œå®éªŒ: {model_name.upper()} - {config['name']} - Run {run_id+1}")
    print(f"   GCN: {config['use_gcn']}, News: {config['use_news']}")
    print(f"{'='*80}")
    
    # åˆ›å»ºä¿®æ”¹åçš„è®­ç»ƒè„šæœ¬
    temp_script_path = create_modified_training_script(model_name, script_path, config, run_id)
    
    try:
        # è¿è¡Œè®­ç»ƒè„šæœ¬
        result = subprocess.run(
            [sys.executable, temp_script_path],
            cwd=project_root,
            capture_output=True,
            text=True,
            timeout=3600  # 1å°æ—¶è¶…æ—¶
        )
        
        # è§£æç»“æœ
        if result.returncode == 0:
            output_lines = result.stdout.split('\n')
            test_metrics = extract_test_metrics(output_lines)
            test_metrics['success'] = True
            test_metrics['error'] = None
            print(f"âœ… å®éªŒæˆåŠŸå®Œæˆ")
        else:
            print(f"âŒ å®éªŒå¤±è´¥: {result.stderr[:200]}...")
            test_metrics = {
                'success': False,
                'error': result.stderr[:500],
                'mae': None, 'mse': None, 'rmse': None, 'r2': None, 'mape': None,
                'new_mae': None, 'normalized_mae': None, 'normalized_mse': None, 'normalized_rmse': None
            }
    
    except subprocess.TimeoutExpired:
        print(f"â° å®éªŒè¶…æ—¶")
        test_metrics = {
            'success': False, 'error': 'Timeout',
            'mae': None, 'mse': None, 'rmse': None, 'r2': None, 'mape': None,
            'new_mae': None, 'normalized_mae': None, 'normalized_mse': None, 'normalized_rmse': None
        }
    
    except Exception as e:
        print(f"âŒ å®éªŒå¼‚å¸¸: {e}")
        test_metrics = {
            'success': False, 'error': str(e),
            'mae': None, 'mse': None, 'rmse': None, 'r2': None, 'mape': None,
            'new_mae': None, 'normalized_mae': None, 'normalized_mse': None, 'normalized_rmse': None
        }
    
    finally:
        # æ¸…ç†ä¸´æ—¶è„šæœ¬
        if os.path.exists(temp_script_path):
            os.remove(temp_script_path)
    
    return test_metrics

def extract_test_metrics(output_lines):
    """ä»è¾“å‡ºä¸­æå–æµ‹è¯•æŒ‡æ ‡"""
    metrics = {}

    for line in output_lines:
        line = line.strip()
        # åŸæœ‰æŒ‡æ ‡
        if "- MAE:" in line:
            try: metrics['mae'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- MSE:" in line:
            try: metrics['mse'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- RMSE:" in line:
            try: metrics['rmse'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- R2:" in line:
            try: metrics['r2'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- MAPE:" in line:
            try: metrics['mape'] = float(line.split(":")[-1].strip())
            except: pass
        # æ–°å¢é‡ç‚¹æŒ‡æ ‡
        elif "- NEW_MAE:" in line:
            try: metrics['new_mae'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- NORMALIZED_MAE:" in line:
            try: metrics['normalized_mae'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- NORMALIZED_MSE:" in line:
            try: metrics['normalized_mse'] = float(line.split(":")[-1].strip())
            except: pass
        elif "- NORMALIZED_RMSE:" in line:
            try: metrics['normalized_rmse'] = float(line.split(":")[-1].strip())
            except: pass

    return metrics

def save_results(all_results):
    """ä¿å­˜å®éªŒç»“æœ"""
    os.makedirs(RESULTS_DIR, exist_ok=True)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = os.path.join(RESULTS_DIR, f"all_models_experiment_{TIMESTAMP}.json")
    with open(results_file, "w") as f:
        json.dump(all_results, f, indent=2)
    
    # åˆ›å»ºæ±‡æ€»è¡¨æ ¼
    summary_data = []
    for model_name, model_results in all_results.items():
        for config_name, runs in model_results.items():
            successful_runs = [r for r in runs if r['success']]
            if successful_runs:
                summary_row = {
                    'model': model_name,
                    'config': config_name,
                    'success_rate': len(successful_runs) / len(runs)
                }

                # é‡ç‚¹æŒ‡æ ‡ç»Ÿè®¡ï¼ˆä¼˜å…ˆçº§é¡ºåºï¼‰
                priority_metrics = ['new_mae', 'r2', 'mape', 'normalized_mae', 'normalized_mse', 'normalized_rmse']
                backup_metrics = ['mae', 'mse', 'rmse']

                # å¤„ç†é‡ç‚¹æŒ‡æ ‡
                for metric in priority_metrics:
                    values = [r[metric] for r in successful_runs if r.get(metric) is not None]
                    if values:
                        summary_row[f'{metric}_mean'] = np.mean(values)
                        summary_row[f'{metric}_std'] = np.std(values)
                        summary_row[f'{metric}_min'] = np.min(values)
                        summary_row[f'{metric}_max'] = np.max(values)

                # å¦‚æœæ²¡æœ‰é‡ç‚¹æŒ‡æ ‡ï¼Œä½¿ç”¨å¤‡ç”¨æŒ‡æ ‡
                has_priority_metrics = any(f'{m}_mean' in summary_row for m in priority_metrics)
                if not has_priority_metrics:
                    for metric in backup_metrics:
                        values = [r[metric] for r in successful_runs if r.get(metric) is not None]
                        if values:
                            summary_row[f'{metric}_mean'] = np.mean(values)
                            summary_row[f'{metric}_std'] = np.std(values)
                            summary_row[f'{metric}_min'] = np.min(values)
                            summary_row[f'{metric}_max'] = np.max(values)

                summary_data.append(summary_row)
    
    # ä¿å­˜æ±‡æ€»è¡¨æ ¼
    summary_df = pd.DataFrame(summary_data)
    summary_file = os.path.join(RESULTS_DIR, f"all_models_summary_{TIMESTAMP}.csv")
    summary_df.to_csv(summary_file, index=False)
    
    print(f"\nğŸ“Š ç»“æœå·²ä¿å­˜:")
    print(f"   è¯¦ç»†ç»“æœ: {results_file}")
    print(f"   æ±‡æ€»è¡¨æ ¼: {summary_file}")
    
    return summary_df

def main():
    """ä¸»å‡½æ•°"""
    print(f"ğŸ”¬ å¼€å§‹æ‰€æœ‰æ¨¡å‹æ‰¹é‡å®éªŒ - {TIMESTAMP}")
    print(f"ğŸ“‹ æµ‹è¯•æ¨¡å‹: {list(MODELS.keys())}")
    print(f"ğŸ“‹ å®éªŒé…ç½®: {len(EXPERIMENT_CONFIGS)} ä¸ªé…ç½®ï¼Œæ¯ä¸ªè¿è¡Œ {NUM_RUNS_PER_CONFIG} æ¬¡")
    
    all_results = {}
    
    for model_name, script_path in MODELS.items():
        print(f"\nğŸ”§ å¼€å§‹æµ‹è¯•æ¨¡å‹: {model_name.upper()}")
        model_results = {}
        
        for config in EXPERIMENT_CONFIGS:
            config_results = []
            
            for run_id in range(NUM_RUNS_PER_CONFIG):
                result = run_experiment(model_name, script_path, config, run_id)
                result['model'] = model_name
                result['config'] = config['name']
                result['run_id'] = run_id
                result['use_gcn'] = config['use_gcn']
                result['use_news'] = config['use_news']
                config_results.append(result)
            
            model_results[config['name']] = config_results
        
        all_results[model_name] = model_results
    
    # ä¿å­˜å’Œåˆ†æç»“æœ
    summary_df = save_results(all_results)
    
    # æ‰“å°æ±‡æ€»
    print(f"\n{'='*100}")
    print("ğŸ“ˆ æ‰€æœ‰æ¨¡å‹å®éªŒæ±‡æ€» (é‡ç‚¹æŒ‡æ ‡)")
    print(f"{'='*100}")

    if not summary_df.empty:
        for model in MODELS.keys():
            print(f"\nğŸ”§ {model.upper()}:")
            model_data = summary_df[summary_df['model'] == model]
            for _, row in model_data.iterrows():
                config_name = row['config']
                success_rate = row['success_rate']

                # ä¼˜å…ˆæ˜¾ç¤ºNEW_MAE
                if 'new_mae_mean' in row and pd.notna(row['new_mae_mean']):
                    print(f"  {config_name:12} | NEW_MAE: {row['new_mae_mean']:.4f} Â± {row['new_mae_std']:.4f} | æˆåŠŸç‡: {success_rate:.1%}")
                elif 'mae_mean' in row and pd.notna(row['mae_mean']):
                    print(f"  {config_name:12} | MAE: {row['mae_mean']:.4f} Â± {row['mae_std']:.4f} | æˆåŠŸç‡: {success_rate:.1%}")
                else:
                    print(f"  {config_name:12} | æ— æœ‰æ•ˆæŒ‡æ ‡ | æˆåŠŸç‡: {success_rate:.1%}")

                # æ˜¾ç¤ºå…¶ä»–é‡è¦æŒ‡æ ‡
                additional_metrics = []
                if 'r2_mean' in row and pd.notna(row['r2_mean']):
                    additional_metrics.append(f"RÂ²: {row['r2_mean']:.4f}")
                if 'mape_mean' in row and pd.notna(row['mape_mean']):
                    additional_metrics.append(f"MAPE: {row['mape_mean']*100:.2f}%")
                if 'normalized_mae_mean' in row and pd.notna(row['normalized_mae_mean']):
                    additional_metrics.append(f"NORM_MAE: {row['normalized_mae_mean']:.4f}")

                if additional_metrics:
                    print(f"  {'':12}   {' | '.join(additional_metrics)}")

    # æ·»åŠ æœ€ä½³æ¨¡å‹åˆ†æ
    print(f"\n{'='*100}")
    print("ğŸ† æœ€ä½³æ¨¡å‹åˆ†æ")
    print(f"{'='*100}")

    if not summary_df.empty:
        # æ‰¾åˆ°æ¯ç§é…ç½®ä¸‹çš„æœ€ä½³æ¨¡å‹
        for config in ['baseline', 'gcn_only', 'news_only', 'gcn_news']:
            config_data = summary_df[summary_df['config'] == config]
            if not config_data.empty:
                # ä¼˜å…ˆä½¿ç”¨NEW_MAEï¼Œå¦åˆ™ä½¿ç”¨MAE
                if 'new_mae_mean' in config_data.columns and config_data['new_mae_mean'].notna().any():
                    best_row = config_data.loc[config_data['new_mae_mean'].idxmin()]
                    metric_name = 'NEW_MAE'
                    metric_value = best_row['new_mae_mean']
                elif 'mae_mean' in config_data.columns and config_data['mae_mean'].notna().any():
                    best_row = config_data.loc[config_data['mae_mean'].idxmin()]
                    metric_name = 'MAE'
                    metric_value = best_row['mae_mean']
                else:
                    continue

                print(f"ğŸ“Š {config:12} | æœ€ä½³: {best_row['model'].upper()} ({metric_name}: {metric_value:.4f})")

    print(f"\nâœ… æ‰€æœ‰æ¨¡å‹æ‰¹é‡å®éªŒå®Œæˆ!")

if __name__ == "__main__":
    main()
