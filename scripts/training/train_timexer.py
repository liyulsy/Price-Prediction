import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
import pandas as pd
from tqdm import tqdm
import os
import sys
import numpy as np
import random
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
import csv
from datetime import datetime
 
# Add the project root to Python path
current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()
project_root = os.path.join(current_dir, '..', '..')
sys.path.append(project_root)

# --- Model and Dataset Imports ---
# å¯¼å…¥TimeXeræ¨¡å‹ï¼šç»“åˆäº†æ—¶é—´åºåˆ—é¢„æµ‹å’Œå›¾ç¥ç»ç½‘ç»œçš„ç»Ÿä¸€æ¨¡å‹
from models.MixModel.unified_timexer_gcn import UnifiedTimexerGCN
# å¯¼å…¥æ•°æ®é›†å¤„ç†ç±»ï¼šå¤„ç†åŠ å¯†è´§å¸ä»·æ ¼å’Œæ–°é—»æ•°æ®
from scripts.analysis.crypto_new_analyzer.unified_dataset import UnifiedCryptoDataset, load_news_data
# å¯¼å…¥å›¾æ„å»ºå·¥å…·ï¼šç”¨äºæ„å»ºåŠ å¯†è´§å¸ä¹‹é—´çš„å…³ç³»å›¾
from dataloader.gnn_loader import generate_edge_index, generate_advanced_edge_index, analyze_graph_properties

# --- Configuration ---
# è®¾å¤‡é…ç½®ï¼šä¼˜å…ˆä½¿ç”¨GPUï¼Œå¦‚æœæ²¡æœ‰GPUåˆ™ä½¿ç”¨CPU
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# --- Master Switches ---
# ä¸»è¦å¼€å…³ï¼šæ§åˆ¶æ¨¡å‹çš„æ ¸å¿ƒåŠŸèƒ½
# é¢„æµ‹ç›®æ ‡é…ç½®ï¼š
# - 'price': é¢„æµ‹ç»å¯¹ä»·æ ¼ (ä»…å›å½’)
# - 'diff': é¢„æµ‹ä»·æ ¼å·®åˆ† (ä»…åˆ†ç±»)
# - 'return': é¢„æµ‹ä»·æ ¼å˜åŒ–ç‡ (ä»…åˆ†ç±»)
PREDICTION_TARGET = 'diff'

# ä»»åŠ¡ç±»å‹è‡ªåŠ¨ç¡®å®š
TASK_TYPE = 'regression' if PREDICTION_TARGET == 'price' else 'classification'
USE_GCN = True                 # æ˜¯å¦ä½¿ç”¨å›¾å·ç§¯ç½‘ç»œï¼šTrue=å¯ç”¨GCN, False=ä¸ä½¿ç”¨GCN
USE_NEWS_FEATURES = False       # æ˜¯å¦ä½¿ç”¨æ–°é—»ç‰¹å¾ï¼šTrue=åŒ…å«æ–°é—»æ•°æ®, False=ä»…ä½¿ç”¨ä»·æ ¼æ•°æ®

# --- Graph Construction Configuration ---
# å›¾æ„å»ºé…ç½®ï¼šå®šä¹‰å¦‚ä½•æ„å»ºåŠ å¯†è´§å¸ä¹‹é—´çš„å…³ç³»å›¾
# åŸºäºå®éªŒç»“æœï¼ŒåŸå§‹æ–¹æ³•è¡¨ç°æœ€ä½³ï¼
GRAPH_METHOD = 'original'  # å›¾æ„å»ºæ–¹æ³•é€‰æ‹©
# å¯é€‰æ–¹æ³•ï¼š'original'(åŸºäºç›¸å…³æ€§), 'multi_layer'(å¤šå±‚å›¾), 'dynamic'(åŠ¨æ€å›¾),
#          'domain_knowledge'(é¢†åŸŸçŸ¥è¯†), 'attention_based'(æ³¨æ„åŠ›æœºåˆ¶)

# ä¸åŒå›¾æ„å»ºæ–¹æ³•çš„å‚æ•°é…ç½®
GRAPH_PARAMS = {
    'original': {'threshold': 0.6},  # åŸå§‹æ–¹æ³•ï¼šç›¸å…³æ€§é˜ˆå€¼0.6
    'multi_layer': {  # å¤šå±‚å›¾æ–¹æ³•ï¼šä½¿ç”¨å¤šç§æŒ‡æ ‡
        'correlation_threshold': 0.3,   # ç›¸å…³æ€§é˜ˆå€¼
        'volatility_threshold': 0.5,    # æ³¢åŠ¨æ€§é˜ˆå€¼
        'trend_threshold': 0.4          # è¶‹åŠ¿é˜ˆå€¼
    },
    'dynamic': {  # åŠ¨æ€å›¾æ–¹æ³•ï¼šæ—¶é—´çª—å£æ»‘åŠ¨
        'window_size': 168,  # çª—å£å¤§å°ï¼š168å°æ—¶(7å¤©)
        'overlap': 24        # é‡å å¤§å°ï¼š24å°æ—¶(1å¤©)
    },
    'domain_knowledge': {  # é¢†åŸŸçŸ¥è¯†æ–¹æ³•ï¼šåŸºäºå¸ç§ç±»å‹
        'coin_names': ['BTC', 'ETH', 'BNB', 'XRP', 'LTC', 'DOGE', 'SOL', 'AVAX']
    },
    'attention_based': {  # æ³¨æ„åŠ›æ–¹æ³•ï¼šå­¦ä¹ é‡è¦è¿æ¥
        'top_k': 3,          # ä¿ç•™å‰3ä¸ªæœ€é‡è¦çš„è¿æ¥
        'use_returns': True  # ä½¿ç”¨æ”¶ç›Šç‡æ•°æ®
    }
}

# --- GCN Configuration ---
# GCNé…ç½®ï¼šé€‰æ‹©å›¾å·ç§¯ç½‘ç»œçš„æ¶æ„ç±»å‹
GCN_CONFIG = 'improved_light'  # GCNæ¶æ„é€‰æ‹©
# å¯é€‰é…ç½®ï¼š'basic'(åŸºç¡€GCN), 'improved_light'(è½»é‡æ”¹è¿›), 'improved_gelu'(GELUæ¿€æ´»),
#          'gat_attention'(å›¾æ³¨æ„åŠ›), 'adaptive'(è‡ªé€‚åº”GCN)

# --- Data & Cache Paths ---
# æ•°æ®å’Œç¼“å­˜è·¯å¾„é…ç½®
PRICE_CSV_PATH = 'scripts/analysis/crypto_analysis/data/processed_data/1H/all_1H.csv'  # ä»·æ ¼æ•°æ®æ–‡ä»¶è·¯å¾„
NEWS_FEATURES_FOLDER = 'scripts/analysis/crypto_new_analyzer/features'                # æ–°é—»ç‰¹å¾æ–‡ä»¶å¤¹è·¯å¾„
CACHE_DIR = "experiments/caches"        # ç¼“å­˜ç›®å½•ï¼šå­˜å‚¨æ¨¡å‹å’Œä¸­é—´ç»“æœ
BEST_MODEL_NAME = "best_timexer_model.pt"  # æœ€ä½³æ¨¡å‹æ–‡ä»¶å

# --- Dataset Parameters ---
# æ•°æ®é›†å‚æ•°é…ç½®
COIN_NAMES = ['BTC', 'ETH', 'BNB', 'XRP', 'LTC', 'DOGE', 'SOL', 'AVAX']  # è¦åˆ†æçš„åŠ å¯†è´§å¸åˆ—è¡¨
PRICE_SEQ_LEN = 90              # ä»·æ ¼åºåˆ—é•¿åº¦ï¼šä½¿ç”¨è¿‡å»90ä¸ªæ—¶é—´ç‚¹çš„æ•°æ®è¿›è¡Œé¢„æµ‹
THRESHOLD = 0.6                 # å›¾æ„å»ºé˜ˆå€¼ï¼šç›¸å…³æ€§è¶…è¿‡0.6æ‰å»ºç«‹è¿æ¥
NORM_TYPE = 'standard'            # æ•°æ®å½’ä¸€åŒ–æ–¹å¼ï¼š'minmax'(0-1å½’ä¸€åŒ–) æˆ– 'standard'(æ ‡å‡†åŒ–)
TIME_ENCODING_ENABLED_IN_DATASET = True  # æ˜¯å¦å¯ç”¨æ—¶é—´ç¼–ç ï¼šåŒ…å«å°æ—¶ã€æ˜ŸæœŸç­‰æ—¶é—´ç‰¹å¾
TIME_FREQ_IN_DATASET = 'h'      # æ—¶é—´é¢‘ç‡ï¼š'h'(å°æ—¶), 'd'(å¤©), 'w'(å‘¨)

# --- TimeXer Model Parameters ---
# TimeXeræ¨¡å‹æ¶æ„å‚æ•°
NEWS_PROCESSED_DIM = 64         # æ–°é—»ç‰¹å¾å¤„ç†åçš„ç»´åº¦ï¼šå°†åŸå§‹æ–°é—»ç‰¹å¾å‹ç¼©åˆ°64ç»´
GCN_HIDDEN_DIM = 256           # GCNéšè—å±‚ç»´åº¦ï¼šå›¾å·ç§¯ç½‘ç»œçš„ä¸­é—´å±‚å¤§å°
GCN_OUTPUT_DIM = 128           # GCNè¾“å‡ºç»´åº¦ï¼šå›¾å·ç§¯ç½‘ç»œçš„è¾“å‡ºç‰¹å¾ç»´åº¦
MLP_HIDDEN_DIM_1 = 256         # MLPç¬¬ä¸€éšè—å±‚ç»´åº¦ï¼šå‡å°ä»¥é¿å…TimeXerç‰¹å¾ä¸»å¯¼
MLP_HIDDEN_DIM_2 = 256         # MLPç¬¬äºŒéšè—å±‚ç»´åº¦ï¼šå¤šå±‚æ„ŸçŸ¥æœºçš„ç¬¬äºŒå±‚å¤§å°
NUM_CLASSES = 1 if TASK_TYPE == 'regression' else 2  # è¾“å‡ºç±»åˆ«æ•°ï¼šå›å½’ä»»åŠ¡=1ï¼Œåˆ†ç±»ä»»åŠ¡=2

# --- Training Parameters ---
# è®­ç»ƒå‚æ•°é…ç½®
BATCH_SIZE = 32                    # æ‰¹æ¬¡å¤§å°ï¼šæ¯æ¬¡è®­ç»ƒä½¿ç”¨32ä¸ªæ ·æœ¬
EPOCHS = 50                        # è®­ç»ƒè½®æ•°ï¼šæœ€å¤§è®­ç»ƒ50ä¸ªepochï¼ˆå¯èƒ½å› æ—©åœè€Œæå‰ç»“æŸï¼‰
LEARNING_RATE = 0.001             # å­¦ä¹ ç‡ï¼šæ§åˆ¶å‚æ•°æ›´æ–°çš„æ­¥é•¿ï¼Œæé«˜åˆå§‹å­¦ä¹ ç‡
WEIGHT_DECAY = 1e-5               # æƒé‡è¡°å‡ï¼šL2æ­£åˆ™åŒ–ç³»æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
VALIDATION_SPLIT_RATIO = 0.15     # éªŒè¯é›†æ¯”ä¾‹ï¼š15%çš„æ•°æ®ç”¨äºéªŒè¯
TEST_SPLIT_RATIO = 0.15           # æµ‹è¯•é›†æ¯”ä¾‹ï¼š15%çš„æ•°æ®ç”¨äºæœ€ç»ˆæµ‹è¯•
FORCE_RECOMPUTE_NEWS = False      # æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—æ–°é—»ç‰¹å¾ï¼šFalse=ä½¿ç”¨ç¼“å­˜
RANDOM_SEED = 42                  # éšæœºç§å­ï¼šç¡®ä¿å®éªŒå¯é‡ç°

# --- Early Stopping Parameters ---
# æ—©åœæœºåˆ¶å‚æ•°ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼ŒèŠ‚çœè®­ç»ƒæ—¶é—´
EARLY_STOPPING_PATIENCE = 20     # æ—©åœè€å¿ƒå€¼ï¼šè¿ç»­20ä¸ªepochæ²¡æœ‰æ”¹å–„å°±åœæ­¢è®­ç»ƒ
MIN_DELTA = 1e-6                  # æœ€å°æ”¹å–„é˜ˆå€¼ï¼šæ”¹å–„å¿…é¡»å¤§äº0.000001æ‰ç®—æœ‰æ•ˆ

def set_random_seeds(seed=42):
    """
    è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°

    Args:
        seed (int): éšæœºç§å­å€¼

    åŠŸèƒ½ï¼š
        1. è®¾ç½®Pythonå†…ç½®randomæ¨¡å—çš„ç§å­
        2. è®¾ç½®NumPyéšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­
        3. è®¾ç½®PyTorch CPUéšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­
        4. è®¾ç½®PyTorch GPUéšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­
        5. ç¡®ä¿CUDAæ“ä½œçš„ç¡®å®šæ€§
        6. ç¦ç”¨CUDAçš„æ€§èƒ½ä¼˜åŒ–ï¼ˆä¸ºäº†ç¡®å®šæ€§ï¼‰
        7. è®¾ç½®Pythonå“ˆå¸ŒéšæœºåŒ–ç§å­
    """
    random.seed(seed)                              # Pythonå†…ç½®éšæœºæ•°
    np.random.seed(seed)                          # NumPyéšæœºæ•°
    torch.manual_seed(seed)                       # PyTorch CPUéšæœºæ•°
    torch.cuda.manual_seed(seed)                  # å½“å‰GPUéšæœºæ•°
    torch.cuda.manual_seed_all(seed)              # æ‰€æœ‰GPUéšæœºæ•°
    # ç¡®ä¿CUDAæ“ä½œçš„ç¡®å®šæ€§ï¼ˆå¯èƒ½ä¼šé™ä½æ€§èƒ½ä½†ä¿è¯å¯é‡ç°æ€§ï¼‰
    torch.backends.cudnn.deterministic = True     # å¼ºåˆ¶ä½¿ç”¨ç¡®å®šæ€§ç®—æ³•
    torch.backends.cudnn.benchmark = False        # ç¦ç”¨è‡ªåŠ¨ä¼˜åŒ–ï¼ˆä¸ºäº†ç¡®å®šæ€§ï¼‰
    # è®¾ç½®Pythonå“ˆå¸ŒéšæœºåŒ–ç¯å¢ƒå˜é‡
    os.environ['PYTHONHASHSEED'] = str(seed)      # ç¡®ä¿å­—å…¸ç­‰æ•°æ®ç»“æ„çš„é¡ºåºä¸€è‡´

# --- Dynamic File Path ---
model_variant = ['TimeXer', TASK_TYPE]
model_variant.append("with_gcn" if USE_GCN else "no_gcn")
model_variant.append("with_news" if USE_NEWS_FEATURES else "no_news")
model_variant_str = "_".join(model_variant)
BEST_MODEL_PATH = os.path.join(CACHE_DIR, f"{model_variant_str}_{BEST_MODEL_NAME}")
print(f"--- Configuration: {model_variant_str} ---")
print(f"Best model will be saved to: {BEST_MODEL_PATH}")

def save_test_predictions(all_preds, all_targets, coin_names, model_name):
    """
    ä¿å­˜æµ‹è¯•é›†çš„é¢„æµ‹å€¼å’ŒçœŸå®å€¼åˆ°CSVæ–‡ä»¶

    åŠŸèƒ½ï¼š
        å°†æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹ç»“æœä¿å­˜ä¸ºä¸¤ä¸ªCSVæ–‡ä»¶ï¼š
        1. è¯¦ç»†é¢„æµ‹ç»“æœï¼šæ¯ä¸ªæ ·æœ¬æ¯ä¸ªå¸ç§çš„é¢„æµ‹å€¼ã€çœŸå®å€¼å’Œè¯¯å·®
        2. ç»Ÿè®¡ä¿¡æ¯ï¼šæ¯ä¸ªå¸ç§çš„ç»Ÿè®¡æŒ‡æ ‡æ±‡æ€»

    Args:
        all_preds: æ‰€æœ‰é¢„æµ‹å€¼ [num_samples, num_coins]
        all_targets: æ‰€æœ‰çœŸå®å€¼ [num_samples, num_coins]
        coin_names: å¸ç§åç§°åˆ—è¡¨
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰

    è¾“å‡ºæ–‡ä»¶ï¼š
        - test_predictions_{model_name}.csv: è¯¦ç»†é¢„æµ‹ç»“æœ
        - test_statistics_{model_name}.csv: ç»Ÿè®¡ä¿¡æ¯æ±‡æ€»
    """
    # === åˆ›å»ºä¿å­˜ç›®å½• ===
    save_dir = "experiments/caches/test_predictions"
    os.makedirs(save_dir, exist_ok=True)  # å¦‚æœç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º

    # === ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ ===
    # åŒ…å«æ¯ä¸ªæ ·æœ¬æ¯ä¸ªå¸ç§çš„è¯¦ç»†é¢„æµ‹ä¿¡æ¯
    predictions_file = os.path.join(save_dir, f"test_predictions_{model_name}.csv")
    with open(predictions_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        # å†™å…¥è¡¨å¤´
        writer.writerow(['sample_idx', 'coin', 'true_value', 'predicted_value', 'absolute_error', 'percentage_error'])

        # === éå†æ‰€æœ‰æ ·æœ¬å’Œå¸ç§ ===
        for sample_idx in range(len(all_preds)):
            for coin_idx, coin_name in enumerate(coin_names):
                # === æå–å½“å‰æ ·æœ¬å½“å‰å¸ç§çš„é¢„æµ‹å€¼å’ŒçœŸå®å€¼ ===
                true_val = all_targets[sample_idx, coin_idx]
                pred_val = all_preds[sample_idx, coin_idx]

                # === è®¡ç®—è¯¯å·®æŒ‡æ ‡ ===
                abs_error = abs(true_val - pred_val)  # ç»å¯¹è¯¯å·®
                # è®¡ç®—ç™¾åˆ†æ¯”è¯¯å·®ï¼Œé¿å…é™¤é›¶é”™è¯¯
                pct_error = (abs_error / abs(true_val)) * 100 if abs(true_val) > 1e-8 else float('inf')

                # === å†™å…¥ä¸€è¡Œæ•°æ® ===
                writer.writerow([sample_idx, coin_name, true_val, pred_val, abs_error, pct_error])

    # === ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ ===
    # åŒ…å«æ¯ä¸ªå¸ç§çš„ç»Ÿè®¡æŒ‡æ ‡æ±‡æ€»
    statistics_file = os.path.join(save_dir, f"test_statistics_{model_name}.csv")
    with open(statistics_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        # å†™å…¥è¡¨å¤´ï¼šåŒ…å«å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å€¼ã€MAEã€MAPEç­‰ç»Ÿè®¡æŒ‡æ ‡
        writer.writerow(['coin', 'mean_true', 'mean_pred', 'std_true', 'std_pred',
                        'min_true', 'min_pred', 'max_true', 'max_pred', 'mae', 'mape'])

        # === éå†æ¯ä¸ªå¸ç§è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡ ===
        for coin_idx, coin_name in enumerate(coin_names):
            # === æå–å½“å‰å¸ç§çš„æ‰€æœ‰é¢„æµ‹å€¼å’ŒçœŸå®å€¼ ===
            true_vals = all_targets[:, coin_idx]
            pred_vals = all_preds[:, coin_idx]

            # === è®¡ç®—è¯„ä¼°æŒ‡æ ‡ ===
            mae = mean_absolute_error(true_vals, pred_vals)  # å¹³å‡ç»å¯¹è¯¯å·®
            # è®¡ç®—å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼Œé¿å…é™¤é›¶é”™è¯¯
            mape = np.mean(np.abs((true_vals - pred_vals) / np.where(np.abs(true_vals) > 1e-8, true_vals, 1e-8))) * 100

            # === å†™å…¥ç»Ÿè®¡ä¿¡æ¯ ===
            writer.writerow([
                coin_name,                    # å¸ç§åç§°
                np.mean(true_vals), np.mean(pred_vals),    # å‡å€¼
                np.std(true_vals), np.std(pred_vals),      # æ ‡å‡†å·®
                np.min(true_vals), np.min(pred_vals),      # æœ€å°å€¼
                np.max(true_vals), np.max(pred_vals),      # æœ€å¤§å€¼
                mae, mape                     # MAEå’ŒMAPE
            ])

    # === æ‰“å°ä¿å­˜ä¿¡æ¯ ===
    print(f"æµ‹è¯•é›†é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°:")
    print(f"  è¯¦ç»†ç»“æœ: {predictions_file}")
    print(f"  ç»Ÿè®¡ä¿¡æ¯: {statistics_file}")

    return predictions_file, statistics_file

def evaluate_model(model, data_loader, criterion, edge_index, edge_weights, device, task_type, scaler=None):
    """
    æ¨¡å‹è¯„ä¼°å‡½æ•°

    åŠŸèƒ½ï¼š
        åœ¨ç»™å®šæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œè®¡ç®—æŸå¤±å’Œå„ç§è¯„ä¼°æŒ‡æ ‡

    Args:
        model: è¦è¯„ä¼°çš„æ¨¡å‹
        data_loader: æ•°æ®åŠ è½½å™¨ï¼ˆéªŒè¯é›†æˆ–æµ‹è¯•é›†ï¼‰
        criterion: æŸå¤±å‡½æ•°
        edge_index: å›¾çš„è¾¹ç´¢å¼•
        edge_weights: å›¾çš„è¾¹æƒé‡
        device: è®¡ç®—è®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰
        task_type: ä»»åŠ¡ç±»å‹ï¼ˆ'classification' æˆ– 'regression'ï¼‰
        scaler: æ•°æ®å½’ä¸€åŒ–å™¨ï¼ˆç”¨äºåå½’ä¸€åŒ–ï¼‰

    Returns:
        metrics: åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        all_preds: æ‰€æœ‰é¢„æµ‹å€¼
        all_targets: æ‰€æœ‰çœŸå®å€¼
    """
    # === è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ ===
    model.eval()  # ç¦ç”¨dropoutå’Œbatch normalizationçš„è®­ç»ƒè¡Œä¸º
    total_loss, all_preds, all_targets = 0.0, [], []

    # === ç¦ç”¨æ¢¯åº¦è®¡ç®—ä»¥èŠ‚çœå†…å­˜å’ŒåŠ é€Ÿ ===
    with torch.no_grad():
        # === éå†æ•°æ®åŠ è½½å™¨çš„æ¯ä¸ªæ‰¹æ¬¡ ===
        for batch_data in tqdm(data_loader, desc="Evaluating"):
            # === æå–æ‰¹æ¬¡æ•°æ®å¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ ===
            price_seq = batch_data['price_seq'].to(device)        # ä»·æ ¼åºåˆ—æ•°æ®
            target_data = batch_data['target_price'].to(device)   # ç›®æ ‡ä»·æ ¼æ•°æ®

            # === å¤„ç†å¯é€‰çš„æ—¶é—´ç¼–ç ç‰¹å¾ ===
            x_mark_enc = batch_data.get('price_seq_mark')
            if x_mark_enc is not None:
                x_mark_enc = x_mark_enc.to(device)

            # === å¤„ç†å¯é€‰çš„æ–°é—»ç‰¹å¾ ===
            news_features = batch_data.get('news_features')
            if news_features is not None:
                news_features = news_features.to(device)

            # === æ¨¡å‹å‰å‘ä¼ æ’­ ===
            outputs = model(
                price_seq,              # ä»·æ ¼åºåˆ—
                x_mark_enc,            # æ—¶é—´ç¼–ç 
                edge_index=edge_index,  # å›¾çš„è¾¹ç´¢å¼•
                edge_weight=edge_weights,  # å›¾çš„è¾¹æƒé‡
                news_features=news_features  # æ–°é—»ç‰¹å¾
            )

            # === æ ¹æ®ä»»åŠ¡ç±»å‹å¤„ç†ç›®æ ‡å’ŒæŸå¤± ===
            if task_type == 'classification':
                # === åˆ†ç±»ä»»åŠ¡ï¼šå°†ä»·æ ¼å˜åŒ–è½¬æ¢ä¸ºæ¶¨è·Œæ ‡ç­¾ ===
                if PREDICTION_TARGET == 'price':
                    # ç›´æ¥é¢„æµ‹ä»·æ ¼ï¼šåˆ¤æ–­ä»·æ ¼æ˜¯å¦å¤§äºæŸä¸ªé˜ˆå€¼
                    targets = (target_data > 0).long()  # æ¶¨=1, è·Œ=0
                elif PREDICTION_TARGET in ('diff', 'return'):
                    # é¢„æµ‹ä»·å·®/å˜åŒ–ç‡ï¼šæ•°æ®é›†ä¸­ target å·²æ˜¯â€œä¸‹ä¸€æ­¥çš„å·®å€¼/æ”¶ç›Šç‡â€ï¼›ç›´æ¥åˆ¤æ–­æ­£è´Ÿ
                    targets = (target_data > 0).long()  # ä¸Šæ¶¨=1, ä¸‹è·Œ=0

                    # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
                    if batch_idx == 0:
                        target_stats = {
                            'total_samples': target_data.numel(),
                            'positive_samples': (target_data > 0).sum().item(),
                            'negative_samples': (target_data <= 0).sum().item(),
                            'target_mean': target_data.mean().item(),
                            'target_std': target_data.std().item(),
                            'target_min': target_data.min().item(),
                            'target_max': target_data.max().item()
                        }
                        print(f"ğŸ” éªŒè¯é›†æ ‡ç­¾ç»Ÿè®¡: {target_stats}")
                        print(f"ğŸ” ä¸Šæ¶¨æ¯”ä¾‹: {target_stats['positive_samples']/target_stats['total_samples']:.1%}")
                        print(f"ğŸ” ä¸‹è·Œæ¯”ä¾‹: {target_stats['negative_samples']/target_stats['total_samples']:.1%}")
                else:
                    raise ValueError(f"PREDICTION_TARGET '{PREDICTION_TARGET}' not supported for classification")

                # ç¡®ä¿è¾“å‡ºå’Œç›®æ ‡çš„å½¢çŠ¶åŒ¹é…
                if len(outputs.shape) == 3:  # [batch_size, num_nodes, num_classes]
                    outputs_flat = outputs.view(-1, NUM_CLASSES)
                    targets_flat = targets.view(-1)
                elif len(outputs.shape) == 2:  # [batch_size * num_nodes, num_classes]
                    outputs_flat = outputs
                    targets_flat = targets.view(-1)
                else:
                    raise ValueError(f"Unexpected output shape in evaluate_model: {outputs.shape}")

                loss = criterion(outputs_flat, targets_flat)
                preds = torch.argmax(outputs, dim=-1)  # è·å–é¢„æµ‹ç±»åˆ«

                # === éªŒè¯æ—¶çš„è°ƒè¯•ä¿¡æ¯ ===
                if batch_idx == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªbatchæ‰“å°éªŒè¯è°ƒè¯•ä¿¡æ¯
                    with torch.no_grad():
                        # åˆ†æéªŒè¯é›†çš„é¢„æµ‹åˆ†å¸ƒ
                        pred_classes = preds
                        class_0_count = (pred_classes == 0).sum().item()
                        class_1_count = (pred_classes == 1).sum().item()
                        total_preds = pred_classes.numel()

                        print(f"ğŸ” éªŒè¯é›†é¢„æµ‹åˆ†å¸ƒ: ä¸‹è·Œ={class_0_count}/{total_preds} ({class_0_count/total_preds:.1%}), ä¸Šæ¶¨={class_1_count}/{total_preds} ({class_1_count/total_preds:.1%})")
            else:
                # === å›å½’ä»»åŠ¡ï¼šä»…æ”¯æŒä»·æ ¼é¢„æµ‹ ===
                if PREDICTION_TARGET == 'price':
                    # ç›´æ¥é¢„æµ‹ä»·æ ¼
                    targets = target_data
                else:
                    raise ValueError(f"PREDICTION_TARGET '{PREDICTION_TARGET}' not supported for regression")
                loss = criterion(outputs, targets)
                preds = outputs

            # === ç´¯è®¡æŸå¤±å’Œé¢„æµ‹ç»“æœ ===
            total_loss += loss.item() * price_seq.size(0)  # åŠ æƒç´¯è®¡æŸå¤±
            all_preds.append(preds.cpu().numpy())          # æ”¶é›†é¢„æµ‹å€¼
            all_targets.append(targets.cpu().numpy())      # æ”¶é›†çœŸå®å€¼
            
    avg_loss = total_loss / len(data_loader.dataset)
    all_preds = np.concatenate(all_preds, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    
    metrics = {'loss': avg_loss}

    # === è®¡ç®—æŒ‡æ ‡ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹è®¡ç®—ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡ ===
    if task_type == 'classification':
        # === åˆ†ç±»ä»»åŠ¡æŒ‡æ ‡è®¡ç®— ===
        # å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬è®¡ç®—å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰æŒ‡æ ‡
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

        # all_preds æ˜¯é¢„æµ‹çš„ç±»åˆ« (0 æˆ– 1)
        # all_targets æ˜¯çœŸå®çš„ç±»åˆ« (0 æˆ– 1)

        # æ•´ä½“åˆ†ç±»æŒ‡æ ‡
        overall_accuracy = accuracy_score(all_targets.flatten(), all_preds.flatten())
        overall_precision = precision_score(all_targets.flatten(), all_preds.flatten(), average='weighted', zero_division=0)
        overall_recall = recall_score(all_targets.flatten(), all_preds.flatten(), average='weighted', zero_division=0)
        overall_f1 = f1_score(all_targets.flatten(), all_preds.flatten(), average='weighted', zero_division=0)

        # æŒ‰ç±»åˆ«åˆ†ç±»æŒ‡æ ‡
        precision_per_class = precision_score(all_targets.flatten(), all_preds.flatten(), average=None, zero_division=0)
        recall_per_class = recall_score(all_targets.flatten(), all_preds.flatten(), average=None, zero_division=0)
        f1_per_class = f1_score(all_targets.flatten(), all_preds.flatten(), average=None, zero_division=0)

        # æ¯ä¸ªå¸ç§çš„åˆ†ç±»æŒ‡æ ‡
        per_coin_metrics = {}
        coin_accuracies = []
        coin_precisions = []
        coin_recalls = []
        coin_f1s = []

        all_targets = all_targets.squeeze(1)
        for i, coin_name in enumerate(COIN_NAMES):
            coin_targets = all_targets[:, i]
            coin_preds = all_preds[:, i]

            # è®¡ç®—æ¯ä¸ªå¸ç§çš„åˆ†ç±»æŒ‡æ ‡
            coin_accuracy = accuracy_score(coin_targets, coin_preds)
            coin_precision = precision_score(coin_targets, coin_preds, average='weighted', zero_division=0)
            coin_recall = recall_score(coin_targets, coin_preds, average='weighted', zero_division=0)
            coin_f1 = f1_score(coin_targets, coin_preds, average='weighted', zero_division=0)

            coin_accuracies.append(coin_accuracy)
            coin_precisions.append(coin_precision)
            coin_recalls.append(coin_recall)
            coin_f1s.append(coin_f1)

            per_coin_metrics[coin_name] = {
                'accuracy': coin_accuracy,
                'precision': coin_precision,
                'recall': coin_recall,
                'f1_score': coin_f1
            }

        # è®¡ç®—æ··æ·†çŸ©é˜µï¼Œç¡®ä¿æ˜¯2x2
        conf_matrix = confusion_matrix(all_targets.flatten(), all_preds.flatten(), labels=[0, 1])

        # æ›´æ–°æŒ‡æ ‡å­—å…¸
        metrics.update({
            'accuracy': overall_accuracy,
            'precision': overall_precision,
            'recall': overall_recall,
            'f1_score': overall_f1,
            'avg_accuracy': np.mean(coin_accuracies),
            'avg_precision': np.mean(coin_precisions),
            'avg_recall': np.mean(coin_recalls),
            'avg_f1_score': np.mean(coin_f1s),
            'precision_class_0': precision_per_class[0] if len(precision_per_class) > 0 else 0,
            'precision_class_1': precision_per_class[1] if len(precision_per_class) > 1 else 0,
            'recall_class_0': recall_per_class[0] if len(recall_per_class) > 0 else 0,
            'recall_class_1': recall_per_class[1] if len(recall_per_class) > 1 else 0,
            'f1_class_0': f1_per_class[0] if len(f1_per_class) > 0 else 0,
            'f1_class_1': f1_per_class[1] if len(f1_per_class) > 1 else 0,
            'confusion_matrix': conf_matrix.tolist()
        })

        metrics['per_coin_metrics'] = per_coin_metrics

    elif task_type == 'regression':
        # Denormalize for metrics calculation
        if PREDICTION_TARGET == 'price':
            if scaler:
                num_coins = all_preds.shape[1]
                original_preds = scaler.inverse_transform(all_preds.reshape(-1, num_coins))
                original_targets = scaler.inverse_transform(all_targets.reshape(-1, num_coins))
            else:
                original_preds = all_preds
                original_targets = all_targets
        else:
            original_preds = all_preds
            original_targets = all_targets

        # Calculate per-coin means for normalization
        coin_means = np.mean(original_targets, axis=0)
        
        # Initialize lists to store per-coin metrics
        coin_maes = []
        coin_mses = []
        coin_mapes = []
        coin_r2s = []
        
        # Per-coin regression metrics
        per_coin_metrics = {}
        for i, coin_name in enumerate(COIN_NAMES):
            coin_targets = original_targets[:, i]
            coin_preds = original_preds[:, i]
            coin_mean = coin_means[i]
            
            # Calculate normalized errors
            norm_targets = coin_targets / coin_mean
            norm_preds = coin_preds / coin_mean
            
            # Calculate metrics
            mae = mean_absolute_error(norm_targets, norm_preds)
            mse = mean_squared_error(norm_targets, norm_preds)
            rmse = np.sqrt(mse)
            mape = mean_absolute_percentage_error(coin_targets, coin_preds)
            r2 = r2_score(coin_targets, coin_preds)
            
            coin_maes.append(mae)
            coin_mses.append(mse)
            coin_mapes.append(mape)
            coin_r2s.append(r2)
            
            per_coin_metrics[coin_name] = {
                'normalized_mae': mae,
                'normalized_mse': mse,
                'normalized_rmse': rmse,
                'mape': mape,
                'r2': r2,
                'mae': mean_absolute_error(coin_targets, coin_preds),
                'mse': mean_squared_error(coin_targets, coin_preds),
                'rmse': np.sqrt(mean_squared_error(coin_targets, coin_preds))
            }
        
        # Calculate new MAE: sum of all true values / sum of all predicted values
        total_true_sum = np.sum(original_targets)
        total_pred_sum = np.sum(original_preds)
        new_mae = total_true_sum / total_pred_sum if total_pred_sum != 0 else float('inf')

        # Calculate overall metrics
        metrics.update({
            'mae': mean_absolute_error(original_targets, original_preds),
            'new_mae': new_mae,
            'mse': mean_squared_error(original_targets, original_preds),
            'rmse': np.sqrt(mean_squared_error(original_targets, original_preds)),
            'r2': r2_score(original_targets, original_preds),
            'mape': mean_absolute_percentage_error(original_targets, original_preds),
            'normalized_mae': np.mean(coin_maes),
            'normalized_mse': np.mean(coin_mses),
            'normalized_rmse': np.sqrt(np.mean(coin_mses)),
            'avg_mape': np.mean(coin_mapes),
            'avg_r2': np.mean(coin_r2s),
            'median_mape': np.median(coin_mapes),
            'worst_mape': np.max(coin_mapes),
            'best_mape': np.min(coin_mapes)
        })
        
        metrics['per_coin_metrics'] = per_coin_metrics

    return metrics, all_preds, all_targets

if __name__ == '__main__':
    """
    ä¸»è®­ç»ƒæµç¨‹

    æ•´ä½“æµç¨‹ï¼š
    1. åˆå§‹åŒ–è®¾ç½®ï¼ˆéšæœºç§å­ã€è®¾å¤‡ã€ç›®å½•ï¼‰
    2. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    3. å›¾æ„å»ºï¼ˆå¦‚æœä½¿ç”¨GCNï¼‰
    4. æ•°æ®å½’ä¸€åŒ–
    5. æ•°æ®é›†åˆ›å»ºå’Œåˆ’åˆ†
    6. æ¨¡å‹åˆå§‹åŒ–
    7. è®­ç»ƒå¾ªç¯ï¼ˆåŒ…å«æ—©åœæœºåˆ¶ï¼‰
    8. æµ‹è¯•å’Œç»“æœä¿å­˜
    """

    # === æ­¥éª¤1: åˆå§‹åŒ–è®¾ç½® ===
    # è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
    set_random_seeds(RANDOM_SEED)
    print(f"ğŸ¯ è®¾ç½®éšæœºç§å­: {RANDOM_SEED}")
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    # åˆ›å»ºç¼“å­˜ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(CACHE_DIR, exist_ok=True)

    # === æ­¥éª¤2: æ•°æ®åŠ è½½å’Œé¢„å¤„ç† ===
    print("ğŸ“Š åŠ è½½ä»·æ ¼æ•°æ®...")
    # åŠ è½½åŸå§‹ä»·æ ¼æ•°æ®ï¼ˆCSVæ ¼å¼ï¼Œæ—¶é—´ä¸ºç´¢å¼•ï¼‰
    price_df_raw = pd.read_csv(PRICE_CSV_PATH, index_col=0, parse_dates=True)
    # é‡å‘½ååˆ—ï¼šä»"BTC-USDT"æ ¼å¼æ”¹ä¸º"BTC"æ ¼å¼
    rename_map = {f"{coin}-USDT": coin for coin in COIN_NAMES}
    # é€‰æ‹©éœ€è¦çš„å¸ç§æ•°æ®
    price_df_full = price_df_raw.rename(columns=rename_map)[COIN_NAMES]

    # ç¡®ä¿æ—¶é—´ç´¢å¼•æ˜¯å‡åºæ’åˆ—ï¼ˆä»æ—©åˆ°æ™šï¼‰
    if not price_df_full.index.is_monotonic_increasing:
        print(f"âš ï¸ ä»·æ ¼æ•°æ®æ—¶é—´ç´¢å¼•ä¸æ˜¯å‡åºï¼Œæ­£åœ¨æ’åº...")
        price_df_full = price_df_full.sort_index()
        print(f"âœ… ä»·æ ¼æ•°æ®å·²æŒ‰æ—¶é—´å‡åºæ’åˆ—")

    # === æ­¥éª¤3: å›¾æ„å»ºï¼ˆä»…åœ¨ä½¿ç”¨GCNæ—¶ï¼‰ ===
    if USE_GCN:
        print(f"ğŸ”— æ„å»ºå›¾ç»“æ„ï¼Œä½¿ç”¨æ–¹æ³•: {GRAPH_METHOD}")

        if GRAPH_METHOD == 'original':
            # åŸå§‹æ–¹æ³•ï¼šåŸºäºç›¸å…³æ€§æ„å»ºå›¾ï¼Œç°åœ¨ä¹Ÿæ”¯æŒè¾¹æƒé‡
            edge_index, edge_weights = generate_edge_index(
                price_df_full,
                return_weights=True,  # è¯·æ±‚è¿”å›è¾¹æƒé‡
                **GRAPH_PARAMS[GRAPH_METHOD]
            )
            # å°†å›¾æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
            edge_index = edge_index.to(DEVICE)
            edge_weights = edge_weights.to(DEVICE) if edge_weights is not None else None
        else:
            # é«˜çº§æ–¹æ³•ï¼šä½¿ç”¨æ›´å¤æ‚çš„å›¾æ„å»ºç®—æ³•
            edge_index, edge_weights = generate_advanced_edge_index(
                price_df_full,                    # ä»·æ ¼æ•°æ®
                method=GRAPH_METHOD,              # å›¾æ„å»ºæ–¹æ³•
                **GRAPH_PARAMS[GRAPH_METHOD]      # æ–¹æ³•ç‰¹å®šå‚æ•°
            )
            # å°†å›¾æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
            edge_index = edge_index.to(DEVICE)
            edge_weights = edge_weights.to(DEVICE) if edge_weights is not None else None

        # åˆ†æå¹¶æ‰“å°å›¾çš„å±æ€§ï¼ˆè¿æ¥æ•°ã€å¯†åº¦ç­‰ï¼‰
        graph_properties = analyze_graph_properties(edge_index, edge_weights, len(COIN_NAMES))
        print(f"ğŸ“ˆ å›¾å±æ€§åˆ†æ:")
        for key, value in graph_properties.items():
            # æ ¼å¼åŒ–è¾“å‡ºï¼šæµ®ç‚¹æ•°ä¿ç•™4ä½å°æ•°ï¼Œå…¶ä»–ç›´æ¥è¾“å‡º
            print(f"  {key}: {value:.4f}" if isinstance(value, float) else f"  {key}: {value}")
    else:
        # ä¸ä½¿ç”¨GCNæ—¶ï¼Œå›¾ç›¸å…³å˜é‡è®¾ä¸ºNone
        print("ğŸš« æœªå¯ç”¨GCNï¼Œè·³è¿‡å›¾æ„å»º")
        edge_index = None
        edge_weights = None

    # === æ­¥éª¤4: æ•°æ®é¢„å¤„ç†ä¸å½’ä¸€åŒ– ===
    print(f"ğŸ”„ æ•°æ®é¢„å¤„ç†: target={PREDICTION_TARGET}, norm={NORM_TYPE}")

    # å…ˆæ ¹æ®ç›®æ ‡æ„é€ è¦å½’ä¸€åŒ–çš„æ•°æ®
    if PREDICTION_TARGET == 'diff':
        df_to_scale = price_df_full.diff().dropna()
    elif PREDICTION_TARGET == 'return':
        # ä½¿ç”¨æ”¶ç›Šç‡ï¼ˆç™¾åˆ†æ¯”å˜åŒ–ï¼‰ï¼Œæ›´ç¨³å®š
        df_to_scale = price_df_full.pct_change().dropna()
    else:  # 'price'
        df_to_scale = price_df_full

    # é€‰æ‹©å½’ä¸€åŒ–å™¨ï¼ˆå·®åˆ†/å˜åŒ–ç‡æ•°æ®éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
    if NORM_TYPE == 'none':
        scaler = None
    elif PREDICTION_TARGET in ('diff', 'return'):
        # å¯¹äºå·®åˆ†/å˜åŒ–ç‡ï¼Œå½’ä¸€åŒ–å¯èƒ½ç ´åæ­£è´Ÿåˆ†å¸ƒ
        print(f"âš ï¸  å·®åˆ†/å˜åŒ–ç‡æ•°æ®ä½¿ç”¨å½’ä¸€åŒ–ï¼Œè¯·æ³¨æ„æ­£è´Ÿæ ·æœ¬åˆ†å¸ƒ")
        if NORM_TYPE == 'standard':
            scaler = StandardScaler()
        elif NORM_TYPE == 'minmax':
            print(f"âš ï¸  MinMaxå½’ä¸€åŒ–å¯èƒ½ä¸é€‚åˆå·®åˆ†æ•°æ®ï¼Œå»ºè®®ä½¿ç”¨ 'standard' æˆ– 'none'")
            scaler = MinMaxScaler()
        else:
            scaler = None
    else:
        # ä»·æ ¼æ•°æ®æ­£å¸¸å½’ä¸€åŒ–
        scaler = StandardScaler() if NORM_TYPE == 'standard' else MinMaxScaler() if NORM_TYPE == 'minmax' else None

    # æ‰§è¡Œå½’ä¸€åŒ–ï¼ˆå¦‚é€‰æ‹©äº†å½’ä¸€åŒ–å™¨ï¼‰
    if scaler:
        values = scaler.fit_transform(df_to_scale)
        price_df = pd.DataFrame(values, columns=df_to_scale.columns, index=df_to_scale.index)
        print(f"âœ… æ•°æ®å½’ä¸€åŒ–å®Œæˆï¼Œæ–¹æ³•: {NORM_TYPE}")

        # æ£€æŸ¥å·®åˆ†æ•°æ®å½’ä¸€åŒ–åçš„åˆ†å¸ƒ
        if PREDICTION_TARGET in ('diff', 'return'):
            pos_ratio = (price_df > 0).sum().sum() / price_df.size
            print(f"ğŸ” å½’ä¸€åŒ–åæ­£å€¼æ¯”ä¾‹: {pos_ratio:.1%}")
            if pos_ratio < 0.1 or pos_ratio > 0.9:
                print(f"âš ï¸  æ­£è´Ÿæ ·æœ¬ä¸¥é‡ä¸å¹³è¡¡ï¼å»ºè®®ä½¿ç”¨ NORM_TYPE='none'")
    else:
        price_df = df_to_scale
        print(f"âœ… è·³è¿‡å½’ä¸€åŒ–ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")

    # === æ­¥éª¤5: åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ ===
    print("ğŸ“° åŠ è½½æ–°é—»æ•°æ®..." if USE_NEWS_FEATURES else "ğŸš« è·³è¿‡æ–°é—»æ•°æ®åŠ è½½")

    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åŠ è½½æ–°é—»æ•°æ®
    news_data_dict = load_news_data(NEWS_FEATURES_FOLDER, COIN_NAMES) if USE_NEWS_FEATURES else None

    if USE_NEWS_FEATURES:
        print(f"ï¿½ å°†ä»ç¼“å­˜æ–‡ä»¶åŠ è½½é¢„å¤„ç†çš„æ–°é—»ç‰¹å¾")
        print(f"ğŸ“ ç¼“å­˜è·¯å¾„: {os.path.join(CACHE_DIR, 'all_processed_news_feature_new10days.pt')}")

        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        cache_file = os.path.join(CACHE_DIR, "all_processed_news_feature_new10days.pt")
        if os.path.exists(cache_file):
            print(f"âœ… æ–°é—»ç‰¹å¾ç¼“å­˜æ–‡ä»¶å­˜åœ¨")
        else:
            print(f"âŒ æ–°é—»ç‰¹å¾ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
            print(f"ï¿½ è¯·ç¡®ä¿å·²é¢„å…ˆç”Ÿæˆæ–°é—»ç‰¹å¾æ–‡ä»¶ï¼Œæˆ–è®¾ç½® USE_NEWS_FEATURES = False")

    if USE_NEWS_FEATURES:
        processed_news_path = os.path.join(CACHE_DIR, "all_processed_news_feature_new10days.pt")
        # å¯¹äºdiff/returnï¼Œå…ˆå°è¯•è‡ªåŠ¨å¯¹é½ï¼Œå¤±è´¥æ—¶æ‰é‡æ–°è®¡ç®—
        if PREDICTION_TARGET in ('diff', 'return'):
            print(f"ğŸ”„ å·®åˆ†/å˜åŒ–ç‡æ¨¡å¼ï¼šå°†å°è¯•è‡ªåŠ¨å¯¹é½ç°æœ‰æ–°é—»ç‰¹å¾")
            # ä¸å¼ºåˆ¶é‡æ–°è®¡ç®—ï¼Œè®©æ•°æ®é›†å°è¯•è‡ªåŠ¨å¯¹é½
            # FORCE_RECOMPUTE_NEWS = True

            # ä¸´æ—¶æ–¹æ¡ˆï¼šå¦‚æœè‡ªåŠ¨å¯¹é½å¤±è´¥ï¼Œå¯ä»¥ç¦ç”¨æ–°é—»ç‰¹å¾
            # USE_NEWS_FEATURES = False
            # print(f"âš ï¸  ä¸´æ—¶ç¦ç”¨æ–°é—»ç‰¹å¾ä»¥é¿å…ç´¢å¼•ä¸åŒ¹é…é—®é¢˜")
    else:
        processed_news_path = None

    print(f"ğŸ”„ åˆ›å»ºæ•°æ®é›†...")
    print(f"  ä»·æ ¼æ•°æ®å½¢çŠ¶: {price_df.shape}")
    print(f"  ä»·æ ¼æ•°æ®æ—¶é—´èŒƒå›´: {price_df.index[0]} åˆ° {price_df.index[-1]}")
    print(f"  ä»·æ ¼æ•°æ®æ—¶é—´é¡ºåº: {'å‡åº' if price_df.index[0] < price_df.index[-1] else 'é™åº'}")
    print(f"  æ–°é—»æ•°æ®: {'å·²åŠ è½½' if news_data_dict else 'æœªåŠ è½½'}")
    print(f"  å¼ºåˆ¶é‡æ–°è®¡ç®—æ–°é—»: {FORCE_RECOMPUTE_NEWS}")

    # æ£€æŸ¥ä»·æ ¼æ•°æ®çš„æ—¶é—´é¡ºåº
    if len(price_df.index) > 1:
        time_diff = (price_df.index[1] - price_df.index[0]).total_seconds()
        if time_diff > 0:
            print(f"  âœ… ä»·æ ¼æ•°æ®æ—¶é—´é¡ºåºæ­£ç¡®ï¼šä»æ—©åˆ°æ™š")
        else:
            print(f"  âš ï¸ ä»·æ ¼æ•°æ®æ—¶é—´é¡ºåºï¼šä»æ™šåˆ°æ—©ï¼Œéœ€è¦æ£€æŸ¥æ˜¯å¦å½±å“æ–°é—»ç‰¹å¾å¯¹é½")
            print(f"  ğŸ’¡ å»ºè®®ï¼šç¡®ä¿æ–°é—»ç‰¹å¾ä¹ŸæŒ‰ç›¸åŒé¡ºåºæ’åˆ—")

    dataset = UnifiedCryptoDataset(
        price_data_df=price_df,
        news_data_dict=news_data_dict,
        seq_len=PRICE_SEQ_LEN,
        processed_news_features_path=processed_news_path,
        force_recompute_news=FORCE_RECOMPUTE_NEWS,
        time_encoding_enabled=TIME_ENCODING_ENABLED_IN_DATASET,
        time_freq=TIME_FREQ_IN_DATASET,
    )

    # ä½¿ç”¨æ—¶é—´åºåˆ—æ­£ç¡®çš„åˆ’åˆ†æ–¹å¼ï¼Œé¿å…æ•°æ®æ³„éœ²
    total_size = len(dataset)
    test_size = int(TEST_SPLIT_RATIO * total_size)
    val_size = int(VALIDATION_SPLIT_RATIO * total_size)
    train_size = total_size - test_size - val_size

    print(f"ğŸ“Š æ—¶é—´åºåˆ—æ•°æ®åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: 0 åˆ° {train_size-1} ({train_size} æ ·æœ¬)")
    print(f"  éªŒè¯é›†: {train_size} åˆ° {train_size+val_size-1} ({val_size} æ ·æœ¬)")
    print(f"  æµ‹è¯•é›†: {train_size+val_size} åˆ° {total_size-1} ({test_size} æ ·æœ¬)")

    # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†ï¼Œé¿å…éšæœºåˆ’åˆ†å¯¼è‡´çš„æ•°æ®æ³„éœ²
    train_indices = list(range(0, train_size))
    val_indices = list(range(train_size, train_size + val_size))
    test_indices = list(range(train_size + val_size, total_size))

    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)
    test_dataset = Subset(dataset, test_indices)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªæ ·æœ¬")
    print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_dataset)} ä¸ªæ ·æœ¬")
    print(f"ğŸ“Š éªŒè¯é›†: {len(val_dataset)} ä¸ªæ ·æœ¬")
    print(f"ğŸ“Š æµ‹è¯•é›†: {len(test_dataset)} ä¸ªæ ·æœ¬")

    # # === æ–°é—»ç‰¹å¾åˆ†æ ===
    # if USE_NEWS_FEATURES:
    #     print(f"\nğŸ” æ–°é—»ç‰¹å¾è¯¦ç»†åˆ†æ:")
    #     print(f"  æ–°é—»ç‰¹å¾ç»´åº¦: {dataset.news_feature_dim}")
    #     print(f"  æ–°é—»ç‰¹å¾å½¢çŠ¶: {dataset.processed_news_features.shape}")

    #     # è·å–ä¸€ä¸ªæ ·æœ¬æŸ¥çœ‹æ–°é—»ç‰¹å¾
    #     sample = dataset[10000]
    #     if 'news_features' in sample:
    #         news_sample = sample['news_features']
    #         print(f"  å•ä¸ªæ ·æœ¬æ–°é—»ç‰¹å¾å½¢çŠ¶: {news_sample.shape}")
    #         print(f"  æ–°é—»ç‰¹å¾ç»Ÿè®¡:")
    #         print(f"    å‡å€¼: {news_sample.mean().item():.6f}")
    #         print(f"    æ ‡å‡†å·®: {news_sample.std().item():.6f}")
    #         print(f"    æœ€å°å€¼: {news_sample.min().item():.6f}")
    #         print(f"    æœ€å¤§å€¼: {news_sample.max().item():.6f}")
    #         print(f"    é›¶å€¼æ¯”ä¾‹: {(news_sample == 0).float().mean().item():.1%}")

    #         # æŸ¥çœ‹æ¯ä¸ªå¸ç§çš„æ–°é—»ç‰¹å¾
    #         print(f"  å„å¸ç§æ–°é—»ç‰¹å¾å¼ºåº¦:")
    #         for i, coin_name in enumerate(COIN_NAMES):
    #             coin_news = news_sample[i]
    #             coin_norm = torch.norm(coin_news).item()
    #             coin_nonzero = (coin_news != 0).sum().item()
    #             print(f"    {coin_name}: èŒƒæ•°={coin_norm:.4f}, éé›¶å…ƒç´ ={coin_nonzero}/{len(coin_news)}")
    #     else:
    #         print(f"  âš ï¸ æ ·æœ¬ä¸­æ²¡æœ‰æ–°é—»ç‰¹å¾")
    # else:
    #     print(f"\nğŸš« æ–°é—»ç‰¹å¾å·²ç¦ç”¨")

    # 5. Initialize TimeXer Model
    print(f"ğŸš€ åˆå§‹åŒ–TimeXeræ¨¡å‹ï¼Œä»»åŠ¡ç±»å‹: {TASK_TYPE}, GCNé…ç½®: {GCN_CONFIG}")

    # åˆ›å»ºæ­£ç¡®çš„é…ç½®å¯¹è±¡
    class TimeXerConfigs:
        def __init__(self):
            self.enc_in = dataset.num_coins
            self.seq_len = PRICE_SEQ_LEN
            self.pred_len = 1
            self.d_model = 64
            self.d_ff = 128
            self.n_heads = 4
            self.e_layers = 2
            self.dropout = 0.3
            self.task_type = TASK_TYPE  # é‡è¦ï¼šæ­£ç¡®è®¾ç½®ä»»åŠ¡ç±»å‹
            self.use_norm = True
            self.patch_len = 16
            self.stride = 8
            self.individual = False
            self.act = 'gelu'
            self.down_sampling_layers = 3
            self.down_sampling_window = 2
            self.down_sampling_method = 'avg'
            self.embed = 'timeF'
            self.freq = 'h'
            self.factor = 1
            self.output_attention = False
            self.activation = 'gelu'
            self.num_time_features = 6

    timexer_configs = TimeXerConfigs()

    model = UnifiedTimexerGCN(
        configs=timexer_configs,  # ä¼ é€’æ­£ç¡®çš„é…ç½®
        gcn_hidden_dim=GCN_HIDDEN_DIM,
        gcn_output_dim=GCN_OUTPUT_DIM,
        use_gcn=USE_GCN,
        gcn_config=GCN_CONFIG,  # æ–°å¢ï¼šä¼ é€’GCNé…ç½®
        news_feature_dim=dataset.news_feature_dim if USE_NEWS_FEATURES else None,
        news_processed_dim=NEWS_PROCESSED_DIM,
        mlp_hidden_dim_1=MLP_HIDDEN_DIM_1,
        mlp_hidden_dim_2=MLP_HIDDEN_DIM_2,
        num_classes=NUM_CLASSES
    ).to(DEVICE)

    print(model)
    print(f"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # === æ­¥éª¤6: è®¾ç½®æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ ===
    print("âš™ï¸ é…ç½®è®­ç»ƒç»„ä»¶...")
    # æŸå¤±å‡½æ•°ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©
    if TASK_TYPE == 'classification':
        # # æ·»åŠ ç±»åˆ«æƒé‡æ¥å¤„ç†ä¸å¹³è¡¡æ•°æ®
        # class_weights = torch.tensor([0.4, 0.6]).to(DEVICE)  # [ä¸‹è·Œæƒé‡, ä¸Šæ¶¨æƒé‡]
        # criterion = nn.CrossEntropyLoss(weight=class_weights)
        # print(f"ğŸ¯ ä½¿ç”¨åŠ æƒäº¤å‰ç†µæŸå¤±ï¼Œç±»åˆ«æƒé‡: ä¸‹è·Œ={class_weights[0]:.1f}, ä¸Šæ¶¨={class_weights[1]:.1f}")
        criterion = nn.CrossEntropyLoss()
    else:
        criterion = nn.MSELoss()
    # ä¼˜åŒ–å™¨ï¼šAdamä¼˜åŒ–å™¨ï¼ŒåŒ…å«æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šå½“éªŒè¯æŸå¤±ä¸å†ä¸‹é™æ—¶è‡ªåŠ¨é™ä½å­¦ä¹ ç‡
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,           # è¦è°ƒæ•´çš„ä¼˜åŒ–å™¨
        'min',              # ç›‘æ§æŒ‡æ ‡çš„æ¨¡å¼ï¼š'min'è¡¨ç¤ºè¶Šå°è¶Šå¥½
        patience=10,        # ç­‰å¾…10ä¸ªepochæ²¡æœ‰æ”¹å–„å†é™ä½å­¦ä¹ ç‡ï¼ˆå¢åŠ è€å¿ƒï¼‰
        factor=0.8,         # å­¦ä¹ ç‡è¡°å‡å› å­ï¼šæ¯æ¬¡å‡å°‘20%ï¼ˆæ›´æ¸©å’Œï¼‰
        min_lr=1e-7         # æœ€å°å­¦ä¹ ç‡ï¼šé˜²æ­¢å­¦ä¹ ç‡è¿‡å°
    )
    print(f"ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®: patience=10, factor=0.8, min_lr=1e-7")

    # === æ­¥éª¤7: è®­ç»ƒå¾ªç¯ï¼ˆåŒ…å«æ—©åœæœºåˆ¶ï¼‰ ===
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œæœ€å¤§è½®æ•°: {EPOCHS}")
    print(f"ğŸ“Š æ—©åœé…ç½®: è€å¿ƒå€¼={EARLY_STOPPING_PATIENCE}, æœ€å°æ”¹å–„={MIN_DELTA}")

    # æ—©åœæœºåˆ¶å˜é‡
    if TASK_TYPE == 'classification':
        best_val_metric = 0.0  # F1åˆ†æ•°è¶Šé«˜è¶Šå¥½
    else:
        best_val_metric = float('inf')  # æŸå¤±è¶Šä½è¶Šå¥½
    patience_counter = 0              # è€å¿ƒè®¡æ•°å™¨ï¼ˆè®°å½•è¿ç»­æ²¡æœ‰æ”¹å–„çš„epochæ•°ï¼‰

    # å¼€å§‹è®­ç»ƒå¾ªç¯
    for epoch in range(EPOCHS):
        # === 7.1: è®­ç»ƒé˜¶æ®µ ===
        model.train()                 # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨dropoutã€batch normç­‰ï¼‰
        epoch_loss = 0.0             # ç´¯è®¡æœ¬epochçš„è®­ç»ƒæŸå¤±

        # åˆ›å»ºè¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} Training")

        # === éå†è®­ç»ƒæ•°æ®çš„æ¯ä¸ªæ‰¹æ¬¡ ===
        for batch_idx, batch_data in enumerate(train_pbar):
            # === æ•°æ®å‡†å¤‡ï¼šä»æ‰¹æ¬¡æ•°æ®ä¸­æå–è¾“å…¥ç‰¹å¾ï¼Œå¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ ===
            price_seq = batch_data['price_seq'].to(DEVICE)        # ä»·æ ¼åºåˆ—æ•°æ®ï¼š[batch_size, seq_len, num_nodes]
            target_data = batch_data['target_price'].to(DEVICE)   # ç›®æ ‡ä»·æ ¼æ•°æ®ï¼š[batch_size, num_nodes]

            # === å¤„ç†å¯é€‰çš„æ—¶é—´ç¼–ç ç‰¹å¾ ===
            # æ—¶é—´ç¼–ç åŒ…å«å°æ—¶ã€æ˜ŸæœŸã€æœˆä»½ç­‰æ—¶é—´ä¿¡æ¯ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£æ—¶é—´æ¨¡å¼
            x_mark_enc = batch_data.get('price_seq_mark')
            if x_mark_enc is not None:
                x_mark_enc = x_mark_enc.to(DEVICE)

            # === å¤„ç†å¯é€‰çš„æ–°é—»ç‰¹å¾ ===
            # æ–°é—»ç‰¹å¾åŒ…å«æƒ…æ„Ÿåˆ†æã€å…³é”®è¯ç­‰ä¿¡æ¯ï¼Œæä¾›é¢å¤–çš„å¸‚åœºä¿¡å·
            news_features = batch_data.get('news_features')
            if news_features is not None:
                news_features = news_features.to(DEVICE)

            # === å‰å‘ä¼ æ’­é˜¶æ®µ ===
            optimizer.zero_grad()  # æ¸…é›¶ä¸Šä¸€æ­¥çš„æ¢¯åº¦ç¼“å­˜ï¼Œé˜²æ­¢æ¢¯åº¦ç´¯ç§¯

            # === æ¨¡å‹å‰å‘ä¼ æ’­ï¼šå¤šæ¨¡æ€è¾“å…¥èåˆ ===
            # å°†ä»·æ ¼åºåˆ—ã€æ—¶é—´ç¼–ç ã€å›¾ç»“æ„ã€æ–°é—»ç‰¹å¾è¾“å…¥æ¨¡å‹
            outputs = model(
                price_seq,              # ä»·æ ¼åºåˆ—ï¼šä¸»è¦çš„æ—¶åºç‰¹å¾
                x_mark_enc,            # æ—¶é—´ç¼–ç ï¼šæ—¶é—´æ¨¡å¼ç‰¹å¾
                edge_index=edge_index,  # å›¾çš„è¾¹ç´¢å¼•ï¼šå®šä¹‰å¸ç§é—´è¿æ¥å…³ç³»
                edge_weight=edge_weights,  # å›¾çš„è¾¹æƒé‡ï¼šè¿æ¥å¼ºåº¦
                news_features=news_features  # æ–°é—»ç‰¹å¾ï¼šå¸‚åœºæƒ…æ„Ÿå’Œäº‹ä»¶ä¿¡æ¯
            )

            # === æŸå¤±è®¡ç®—ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹è®¡ç®—ä¸åŒçš„æŸå¤± ===
            if TASK_TYPE == 'classification':
                # === åˆ†ç±»ä»»åŠ¡ï¼šé¢„æµ‹ä»·æ ¼æ¶¨è·Œæ–¹å‘ ===
                if PREDICTION_TARGET in ('diff', 'return'):
                    # é¢„æµ‹ä»·å·®/å˜åŒ–ç‡ï¼šæ•°æ®é›†ä¸­ target å·²ç»æ˜¯â€œä¸‹ä¸€æ­¥çš„å·®å€¼/æ”¶ç›Šç‡â€ï¼›ç›´æ¥åˆ¤æ–­æ­£è´Ÿ
                    targets = (target_data > 0).long()  # ä¸Šæ¶¨=1, ä¸‹è·Œ=0

                    # è®­ç»ƒæ—¶çš„è°ƒè¯•ä¿¡æ¯ï¼ˆæ¯ä¸ªepochåªæ‰“å°ä¸€æ¬¡ï¼‰
                    if batch_idx == 0:
                        target_stats = {
                            'total_samples': target_data.numel(),
                            'positive_samples': (target_data > 0).sum().item(),
                            'negative_samples': (target_data <= 0).sum().item(),
                            'target_mean': target_data.mean().item(),
                            'target_std': target_data.std().item()
                        }
                        print(f"ğŸ” è®­ç»ƒé›†æ ‡ç­¾ç»Ÿè®¡: ä¸Šæ¶¨={target_stats['positive_samples']}/{target_stats['total_samples']} ({target_stats['positive_samples']/target_stats['total_samples']:.1%})")

                        # æ£€æŸ¥æ¨¡å‹è¾“å‡ºåˆ†å¸ƒ
                        with torch.no_grad():
                            pred_probs = torch.softmax(outputs, dim=-1)
                            pred_classes = torch.argmax(outputs, dim=-1)
                            class_0_pred = (pred_classes == 0).sum().item()
                            class_1_pred = (pred_classes == 1).sum().item()
                            total_pred = pred_classes.numel()
                            print(f"ğŸ” æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ: ä¸‹è·Œ={class_0_pred}/{total_pred} ({class_0_pred/total_pred:.1%}), ä¸Šæ¶¨={class_1_pred}/{total_pred} ({class_1_pred/total_pred:.1%})")
                else:
                    raise ValueError(f"PREDICTION_TARGET '{PREDICTION_TARGET}' not supported for classification")

                # ç¡®ä¿è¾“å‡ºå’Œç›®æ ‡çš„å½¢çŠ¶åŒ¹é…
                if len(outputs.shape) == 3:  # [batch_size, num_nodes, num_classes]
                    outputs_flat = outputs.view(-1, NUM_CLASSES)
                    targets_flat = targets.view(-1)
                elif len(outputs.shape) == 2:  # [batch_size * num_nodes, num_classes]
                    outputs_flat = outputs
                    targets_flat = targets.view(-1)
                else:
                    raise ValueError(f"Unexpected output shape: {outputs.shape}")

                loss = criterion(outputs_flat, targets_flat)
            else:
                # å›å½’ä»»åŠ¡ï¼šä»…æ”¯æŒä»·æ ¼é¢„æµ‹
                if PREDICTION_TARGET == 'price':
                    # ç›´æ¥é¢„æµ‹ä»·æ ¼
                    targets = target_data
                else:
                    raise ValueError(f"PREDICTION_TARGET '{PREDICTION_TARGET}' not supported for regression")
                loss = criterion(outputs, targets)

            # === åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–° ===
            loss.backward()        # è®¡ç®—æ¢¯åº¦

            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()       # æ›´æ–°æ¨¡å‹å‚æ•°
            # ç´¯è®¡æŸå¤±ï¼ˆåŠ æƒå¹³å‡ï¼Œæƒé‡ä¸ºæ‰¹æ¬¡å¤§å°ï¼‰
            epoch_loss += loss.item() * price_seq.size(0)

        # === 7.2: éªŒè¯é˜¶æ®µ ===
        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        avg_train_loss = epoch_loss / len(train_dataset)
        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
        val_metrics, _, _ = evaluate_model(
            model, val_loader, criterion, edge_index, edge_weights, DEVICE, TASK_TYPE, scaler
        )
        # é€‰æ‹©ä¸åŒçš„æŒ‡æ ‡ç”¨äºå­¦ä¹ ç‡è°ƒåº¦å’Œæ—©åœ
        val_metric_for_scheduler = val_metrics['loss']  # å­¦ä¹ ç‡è°ƒåº¦ä»ä½¿ç”¨æŸå¤±
        if TASK_TYPE == 'classification':
            val_metric_for_early_stopping = val_metrics['f1_score']  # æ—©åœä½¿ç”¨F1åˆ†æ•°
        else:
            val_metric_for_early_stopping = val_metrics['loss']  # å›å½’ä»»åŠ¡ä½¿ç”¨æŸå¤±
        # æ ¹æ®éªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡
        scheduler.step(val_metric_for_scheduler)

        # === 7.3: æ‰“å°è®­ç»ƒè¿›åº¦ ===
        current_lr = optimizer.param_groups[0]['lr']
        print(f"\nğŸ“Š Epoch {epoch+1}/{EPOCHS} | è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | å­¦ä¹ ç‡: {current_lr:.6f}")

        # æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡å°
        if current_lr < 1e-6:
            print(f"âš ï¸ å­¦ä¹ ç‡è¿‡å° ({current_lr:.2e})ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ")

        print("--- éªŒè¯é›†æŒ‡æ ‡ ---")
        for name, value in val_metrics.items():
            if not isinstance(value, dict):  # è·³è¿‡åµŒå¥—å­—å…¸ï¼ˆå¦‚per_coin_metricsï¼‰
                if isinstance(value, (int, float)):
                    # ä¸ºä¸åŒæŒ‡æ ‡æ·»åŠ ä¸­æ–‡æ³¨é‡Š
                    if name == 'accuracy':
                        print(f"  - {name.upper()}: {value:.4f}  # æ•´ä½“å‡†ç¡®ç‡ - é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ¯”ä¾‹")
                    elif name == 'precision':
                        print(f"  - {name.upper()}: {value:.4f}  # æ•´ä½“ç²¾ç¡®ç‡ - é¢„æµ‹ä¸ºæ¶¨çš„æ ·æœ¬ä¸­å®é™…ä¸Šæ¶¨çš„æ¯”ä¾‹")
                    elif name == 'recall':
                        print(f"  - {name.upper()}: {value:.4f}  # æ•´ä½“å¬å›ç‡ - å®é™…ä¸Šæ¶¨çš„æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹")
                    elif name == 'f1_score':
                        print(f"  - {name.upper()}: {value:.4f}  # æ•´ä½“F1åˆ†æ•° - ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡")
                    elif name == 'avg_accuracy':
                        print(f"  - {name.upper()}: {value:.4f}  # å¹³å‡å‡†ç¡®ç‡ - å„å¸ç§å‡†ç¡®ç‡çš„å¹³å‡å€¼")
                    elif name == 'avg_precision':
                        print(f"  - {name.upper()}: {value:.4f}  # å¹³å‡ç²¾ç¡®ç‡ - å„å¸ç§ç²¾ç¡®ç‡çš„å¹³å‡å€¼")
                    elif name == 'avg_recall':
                        print(f"  - {name.upper()}: {value:.4f}  # å¹³å‡å¬å›ç‡ - å„å¸ç§å¬å›ç‡çš„å¹³å‡å€¼")
                    elif name == 'avg_f1_score':
                        print(f"  - {name.upper()}: {value:.4f}  # å¹³å‡F1åˆ†æ•° - å„å¸ç§F1åˆ†æ•°çš„å¹³å‡å€¼")
                    elif name == 'precision_class_0':
                        print(f"  - {name.upper()}: {value:.4f}  # ä¸‹è·Œç±»ç²¾ç¡®ç‡ - é¢„æµ‹ä¸‹è·Œä¸­å®é™…ä¸‹è·Œçš„æ¯”ä¾‹")
                    elif name == 'precision_class_1':
                        print(f"  - {name.upper()}: {value:.4f}  # ä¸Šæ¶¨ç±»ç²¾ç¡®ç‡ - é¢„æµ‹ä¸Šæ¶¨ä¸­å®é™…ä¸Šæ¶¨çš„æ¯”ä¾‹")
                    elif name == 'recall_class_0':
                        print(f"  - {name.upper()}: {value:.4f}  # ä¸‹è·Œç±»å¬å›ç‡ - å®é™…ä¸‹è·Œä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹")
                    elif name == 'recall_class_1':
                        print(f"  - {name.upper()}: {value:.4f}  # ä¸Šæ¶¨ç±»å¬å›ç‡ - å®é™…ä¸Šæ¶¨ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹")
                    elif name == 'f1_class_0':
                        print(f"  - {name.upper()}: {value:.4f}  # ä¸‹è·Œç±»F1åˆ†æ•° - ä¸‹è·Œç±»ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡")
                    elif name == 'f1_class_1':
                        print(f"  - {name.upper()}: {value:.4f}  # ä¸Šæ¶¨ç±»F1åˆ†æ•° - ä¸Šæ¶¨ç±»ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡")
                    elif name == 'loss':
                        print(f"  - {name.upper()}: {value:.4f}  # éªŒè¯æŸå¤± - æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æŸå¤±å€¼")
                    else:
                        print(f"  - {name.upper()}: {value:.4f}")
                elif isinstance(value, list):
                    if name == 'confusion_matrix':
                        print(f"  - {name.upper()}: {value}  # æ··æ·†çŸ©é˜µ - [[çœŸè´Ÿä¾‹,å‡æ­£ä¾‹],[å‡è´Ÿä¾‹,çœŸæ­£ä¾‹]]")
                    else:
                        print(f"  - {name.upper()}: {value}")
                else:
                    print(f"  - {name.upper()}: {value}")

        # === 7.4: æ—©åœæœºåˆ¶å’Œæœ€ä½³æ¨¡å‹ä¿å­˜ ===
        # æ£€æŸ¥éªŒè¯æŒ‡æ ‡æ˜¯å¦æœ‰æ˜¾è‘—æ”¹å–„
        if TASK_TYPE == 'classification':
            # åˆ†ç±»ä»»åŠ¡ï¼šF1åˆ†æ•°è¶Šé«˜è¶Šå¥½
            if val_metric_for_early_stopping > best_val_metric + MIN_DELTA:
                best_val_metric = val_metric_for_early_stopping
                patience_counter = 0
                torch.save(model.state_dict(), BEST_MODEL_PATH)
                print(f"ğŸš€ ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹åˆ° {BEST_MODEL_PATH} (éªŒè¯F1: {best_val_metric:.4f})")
            else:
                patience_counter += 1
                print(f"â³ è¿ç»­ {patience_counter} ä¸ªepochæ— æ”¹å–„ (æœ€ä½³F1: {best_val_metric:.4f})")
        else:
            # å›å½’ä»»åŠ¡ï¼šæŸå¤±è¶Šä½è¶Šå¥½
            if val_metric_for_early_stopping < best_val_metric - MIN_DELTA:
                best_val_metric = val_metric_for_early_stopping
                patience_counter = 0
                torch.save(model.state_dict(), BEST_MODEL_PATH)
                print(f"ğŸš€ ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹åˆ° {BEST_MODEL_PATH} (éªŒè¯æŸå¤±: {best_val_metric:.4f})")
            else:
                patience_counter += 1
                print(f"â³ è¿ç»­ {patience_counter} ä¸ªepochæ— æ”¹å–„ (æœ€ä½³æŸå¤±: {best_val_metric:.4f})")
            print(f"â³ è¿ç»­ {patience_counter} ä¸ªepochæ— æ”¹å–„")

        # === 7.5: æ—©åœæ£€æŸ¥ ===
        # å¦‚æœè¿ç»­æ— æ”¹å–„çš„epochæ•°è¾¾åˆ°è€å¿ƒå€¼ï¼Œè§¦å‘æ—©åœ
        if patience_counter >= EARLY_STOPPING_PATIENCE:
            print(f"ğŸ›‘ è§¦å‘æ—©åœæœºåˆ¶ï¼è®­ç»ƒåœ¨ç¬¬ {epoch+1} ä¸ªepochåœæ­¢ (è€å¿ƒå€¼: {EARLY_STOPPING_PATIENCE})")
            print(f"ğŸ’¡ åŸå› : è¿ç»­ {EARLY_STOPPING_PATIENCE} ä¸ªepochéªŒè¯æŒ‡æ ‡æ— æ˜¾è‘—æ”¹å–„")
            break  # è·³å‡ºè®­ç»ƒå¾ªç¯

    # === æ­¥éª¤8: æµ‹è¯•é˜¶æ®µ ===
    print("\n" + "="*60)
    print("ğŸ§ª å¼€å§‹æµ‹è¯•é˜¶æ®µ - ä½¿ç”¨æœ€ä½³æ¨¡å‹")
    print("="*60)

    # åŠ è½½è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜çš„æœ€ä½³æ¨¡å‹
    if os.path.exists(BEST_MODEL_PATH):
        print(f"ğŸ“‚ åŠ è½½æœ€ä½³æ¨¡å‹: {BEST_MODEL_PATH}")
        model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE, weights_only=True))
    else:
        print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æ–‡ä»¶ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹çŠ¶æ€è¿›è¡Œæµ‹è¯•")

    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
    print("ğŸ” åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
    test_metrics, test_preds, test_targets = evaluate_model(
        model, test_loader, criterion, edge_index, edge_weights, DEVICE, TASK_TYPE, scaler
    )

    # === æ­¥éª¤9: ä¿å­˜æµ‹è¯•ç»“æœ ===
    print("ğŸ’¾ ä¿å­˜æµ‹è¯•é›†é¢„æµ‹ç»“æœ...")
    if TASK_TYPE == 'regression':
        # å¯¹äºå›å½’ä»»åŠ¡ï¼Œéœ€è¦å°†é¢„æµ‹ç»“æœè½¬æ¢å›åŸå§‹å°ºåº¦
        if PREDICTION_TARGET == 'price' and scaler:
            # å¦‚æœé¢„æµ‹ä»·æ ¼ä¸”ä½¿ç”¨äº†å½’ä¸€åŒ–ï¼Œéœ€è¦åå½’ä¸€åŒ–
            original_test_preds = scaler.inverse_transform(test_preds)      # é¢„æµ‹å€¼åå½’ä¸€åŒ–
            original_test_targets = scaler.inverse_transform(test_targets)  # çœŸå®å€¼åå½’ä¸€åŒ–
        else:
            # å¦‚æœé¢„æµ‹ä»·æ ¼å·®æˆ–æœªä½¿ç”¨å½’ä¸€åŒ–ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å€¼
            original_test_preds = test_preds
            original_test_targets = test_targets

        # ä¿å­˜è¯¦ç»†çš„é¢„æµ‹ç»“æœåˆ°CSVæ–‡ä»¶
        save_test_predictions(original_test_preds, original_test_targets, COIN_NAMES, model_variant_str)

    # === æ­¥éª¤10: æ‰“å°æœ€ç»ˆæµ‹è¯•ç»“æœ ===
    print(f"\n" + "="*60)
    print("ğŸ‰ æœ€ç»ˆæµ‹è¯•ç»“æœ")
    print("="*60)
    print("ğŸ“Š æ•´ä½“æŒ‡æ ‡:")
    for name, value in test_metrics.items():
        if not isinstance(value, dict):  # è·³è¿‡åµŒå¥—å­—å…¸
            if isinstance(value, (int, float)):
                # ä¸ºä¸åŒæŒ‡æ ‡æ·»åŠ ä¸­æ–‡æ³¨é‡Š
                if name == 'accuracy':
                    print(f"    - {name.upper()}: {value:.4f}  # æ•´ä½“å‡†ç¡®ç‡ - é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ¯”ä¾‹")
                elif name == 'precision':
                    print(f"    - {name.upper()}: {value:.4f}  # æ•´ä½“ç²¾ç¡®ç‡ - é¢„æµ‹ä¸ºæ¶¨çš„æ ·æœ¬ä¸­å®é™…ä¸Šæ¶¨çš„æ¯”ä¾‹")
                elif name == 'recall':
                    print(f"    - {name.upper()}: {value:.4f}  # æ•´ä½“å¬å›ç‡ - å®é™…ä¸Šæ¶¨çš„æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹")
                elif name == 'f1_score':
                    print(f"    - {name.upper()}: {value:.4f}  # æ•´ä½“F1åˆ†æ•° - ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡")
                elif name == 'avg_accuracy':
                    print(f"    - {name.upper()}: {value:.4f}  # å¹³å‡å‡†ç¡®ç‡ - å„å¸ç§å‡†ç¡®ç‡çš„å¹³å‡å€¼")
                elif name == 'avg_precision':
                    print(f"    - {name.upper()}: {value:.4f}  # å¹³å‡ç²¾ç¡®ç‡ - å„å¸ç§ç²¾ç¡®ç‡çš„å¹³å‡å€¼")
                elif name == 'avg_recall':
                    print(f"    - {name.upper()}: {value:.4f}  # å¹³å‡å¬å›ç‡ - å„å¸ç§å¬å›ç‡çš„å¹³å‡å€¼")
                elif name == 'avg_f1_score':
                    print(f"    - {name.upper()}: {value:.4f}  # å¹³å‡F1åˆ†æ•° - å„å¸ç§F1åˆ†æ•°çš„å¹³å‡å€¼")
                elif name == 'precision_class_0':
                    print(f"    - {name.upper()}: {value:.4f}  # ä¸‹è·Œç±»ç²¾ç¡®ç‡ - é¢„æµ‹ä¸‹è·Œä¸­å®é™…ä¸‹è·Œçš„æ¯”ä¾‹")
                elif name == 'precision_class_1':
                    print(f"    - {name.upper()}: {value:.4f}  # ä¸Šæ¶¨ç±»ç²¾ç¡®ç‡ - é¢„æµ‹ä¸Šæ¶¨ä¸­å®é™…ä¸Šæ¶¨çš„æ¯”ä¾‹")
                elif name == 'recall_class_0':
                    print(f"    - {name.upper()}: {value:.4f}  # ä¸‹è·Œç±»å¬å›ç‡ - å®é™…ä¸‹è·Œä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹")
                elif name == 'recall_class_1':
                    print(f"    - {name.upper()}: {value:.4f}  # ä¸Šæ¶¨ç±»å¬å›ç‡ - å®é™…ä¸Šæ¶¨ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹")
                elif name == 'f1_class_0':
                    print(f"    - {name.upper()}: {value:.4f}  # ä¸‹è·Œç±»F1åˆ†æ•° - ä¸‹è·Œç±»ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡")
                elif name == 'f1_class_1':
                    print(f"    - {name.upper()}: {value:.4f}  # ä¸Šæ¶¨ç±»F1åˆ†æ•° - ä¸Šæ¶¨ç±»ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡")
                elif name == 'loss':
                    print(f"    - {name.upper()}: {value:.4f}  # æµ‹è¯•æŸå¤± - æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æŸå¤±å€¼")
                # å›å½’æŒ‡æ ‡æ³¨é‡Š
                elif name == 'mae':
                    print(f"    - {name.upper()}: {value:.4f}  # å¹³å‡ç»å¯¹è¯¯å·® - é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å¹³å‡ç»å¯¹å·®")
                elif name == 'new_mae':
                    print(f"    - {name.upper()}: {value:.4f}  # æ–°MAEæŒ‡æ ‡ - çœŸå®å€¼æ€»å’Œ/é¢„æµ‹å€¼æ€»å’Œ")
                elif name == 'mse':
                    print(f"    - {name.upper()}: {value:.4f}  # å‡æ–¹è¯¯å·® - é¢„æµ‹å€¼ä¸çœŸå®å€¼å·®çš„å¹³æ–¹çš„å¹³å‡")
                elif name == 'rmse':
                    print(f"    - {name.upper()}: {value:.4f}  # å‡æ–¹æ ¹è¯¯å·® - MSEçš„å¹³æ–¹æ ¹")
                elif name == 'r2':
                    print(f"    - {name.upper()}: {value:.4f}  # å†³å®šç³»æ•° - æ¨¡å‹è§£é‡Šæ•°æ®å˜å¼‚æ€§çš„æ¯”ä¾‹(è¶Šæ¥è¿‘1è¶Šå¥½)")
                elif name == 'mape':
                    print(f"    - {name.upper()}: {value:.4f}  # å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® - ç›¸å¯¹è¯¯å·®çš„ç™¾åˆ†æ¯”")
                elif name == 'normalized_mae':
                    print(f"    - {name.upper()}: {value:.4f}  # å½’ä¸€åŒ–MAE - æ¶ˆé™¤å¸ç§ä»·æ ¼å°ºåº¦å½±å“çš„MAE")
                elif name == 'normalized_mse':
                    print(f"    - {name.upper()}: {value:.4f}  # å½’ä¸€åŒ–MSE - æ¶ˆé™¤å¸ç§ä»·æ ¼å°ºåº¦å½±å“çš„MSE")
                elif name == 'normalized_rmse':
                    print(f"    - {name.upper()}: {value:.4f}  # å½’ä¸€åŒ–RMSE - æ¶ˆé™¤å¸ç§ä»·æ ¼å°ºåº¦å½±å“çš„RMSE")
                else:
                    print(f"    - {name.upper()}: {value:.4f}")
            elif isinstance(value, list):
                if name == 'confusion_matrix':
                    print(f"    - {name.upper()}: {value}  # æ··æ·†çŸ©é˜µ")
                    # å®‰å…¨åœ°è§£ææ··æ·†çŸ©é˜µ
                    if len(value) >= 2 and len(value[0]) >= 2 and len(value[1]) >= 2:
                        print(f"      è§£é‡Š: çœŸè´Ÿä¾‹={value[0][0]}, å‡æ­£ä¾‹={value[0][1]}, å‡è´Ÿä¾‹={value[1][0]}, çœŸæ­£ä¾‹={value[1][1]}")
                    else:
                        print(f"      æ³¨æ„: æ··æ·†çŸ©é˜µç»´åº¦ä¸å®Œæ•´ï¼Œå¯èƒ½æŸäº›ç±»åˆ«æœªè¢«é¢„æµ‹åˆ°")
                else:
                    print(f"    - {name.upper()}: {value}")
            else:
                print(f"    - {name.upper()}: {value}")

    # å¦‚æœæœ‰æ¯ä¸ªå¸ç§çš„è¯¦ç»†æŒ‡æ ‡ï¼Œä¹Ÿæ‰“å°å‡ºæ¥
    if 'per_coin_metrics' in test_metrics:
        print("\nğŸ“ˆ å„å¸ç§è¯¦ç»†æŒ‡æ ‡:")
        for coin_name, coin_metrics in test_metrics['per_coin_metrics'].items():
            print(f"  ğŸª™ {coin_name}:")
            for metric_name, value in coin_metrics.items():
                if isinstance(value, (int, float)):
                    # ä¸ºæ¯ä¸ªå¸ç§çš„æŒ‡æ ‡æ·»åŠ æ³¨é‡Š
                    if metric_name == 'accuracy':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„é¢„æµ‹å‡†ç¡®ç‡")
                    elif metric_name == 'precision':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„é¢„æµ‹ç²¾ç¡®ç‡")
                    elif metric_name == 'recall':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„é¢„æµ‹å¬å›ç‡")
                    elif metric_name == 'f1_score':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„F1åˆ†æ•°")
                    elif metric_name == 'mae':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„å¹³å‡ç»å¯¹è¯¯å·®")
                    elif metric_name == 'mse':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„å‡æ–¹è¯¯å·®")
                    elif metric_name == 'rmse':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„å‡æ–¹æ ¹è¯¯å·®")
                    elif metric_name == 'r2':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„å†³å®šç³»æ•°")
                    elif metric_name == 'mape':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®")
                    elif metric_name == 'normalized_mae':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„å½’ä¸€åŒ–MAE")
                    elif metric_name == 'normalized_mse':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„å½’ä¸€åŒ–MSE")
                    elif metric_name == 'normalized_rmse':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„å½’ä¸€åŒ–RMSE")
                    else:
                        print(f"    - {metric_name.upper()}: {value:.4f}")
                else:
                    print(f"    - {metric_name.upper()}: {value}")

    print(f"\n" + "="*60)
    print("âœ… è®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæˆï¼")
    print("ğŸ“ æ£€æŸ¥ experiments/cache/ ç›®å½•æŸ¥çœ‹ä¿å­˜çš„æ¨¡å‹å’Œç»“æœ")
    print("="*60)
