import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
import pandas as pd
from tqdm import tqdm
import os
import sys
import numpy as np
import random
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
import csv
from datetime import datetime
import json
from skopt import gp_minimize
from skopt.space import Real, Integer, Categorical
import warnings
warnings.filterwarnings('ignore')

# Add the project root to Python path
current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()
project_root = os.path.join(current_dir, '..', '..')
sys.path.append(project_root)

# --- Model and Dataset Imports ---
from models.MixModel.unified_wpmixer import UnifiedWPMixer
from scripts.analysis.crypto_new_analyzer.unified_dataset import UnifiedCryptoDataset

# --- Configuration ---
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# --- Master Switches ---
PREDICTION_TARGET = 'price'  # åªåšä»·æ ¼å›å½’
TASK_TYPE = 'regression'     # å›ºå®šä¸ºå›å½’ä»»åŠ¡
USE_GCN = False             # ä¸ä½¿ç”¨GCN
USE_NEWS_FEATURES = False   # ä¸ä½¿ç”¨æ–°é—»ç‰¹å¾

# --- Data & Cache Paths ---
PRICE_CSV_PATH = 'scripts/analysis/crypto_analysis/data/processed_data/1H/all_1H.csv'
CACHE_DIR = "experiments/cache/bayesian_optimization"
BEST_MODEL_NAME = "best_bayesian_wpmixer_model.pt"

# --- Dataset Parameters ---
COIN_NAMES = ['BTC', 'ETH', 'BNB', 'XRP', 'LTC', 'DOGE', 'SOL', 'AVAX']
NORM_TYPE = 'standard'
TIME_ENCODING_ENABLED_IN_DATASET = True
TIME_FREQ_IN_DATASET = 'h'

# --- Fixed Training Parameters ---
RANDOM_SEED = 42

# --- Import Configuration ---
try:
    from bayesian_optimization_config import (
        OPTIMIZATION_OBJECTIVE, COMPOSITE_WEIGHTS, NORMALIZATION_PARAMS,
        N_CALLS, N_RANDOM_STARTS, EARLY_STOPPING_PATIENCE, MIN_DELTA,
        VALIDATION_SPLIT_RATIO, TEST_SPLIT_RATIO
    )
    print("âœ… å·²ä»é…ç½®æ–‡ä»¶åŠ è½½ä¼˜åŒ–è®¾ç½®")
except ImportError:
    print("âš ï¸ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
    # --- Bayesian Optimization Configuration ---
    N_CALLS = 50  # è´å¶æ–¯ä¼˜åŒ–çš„è¿­ä»£æ¬¡æ•°
    N_RANDOM_STARTS = 10  # éšæœºåˆå§‹åŒ–çš„æ¬¡æ•°

    # --- Optimization Objective Configuration ---
    OPTIMIZATION_OBJECTIVE = 'composite'

    # ç»¼åˆè¯„åˆ†æƒé‡é…ç½®
    COMPOSITE_WEIGHTS = {
        'mse_weight': 0.4,      # MSEæŸå¤±æƒé‡
        'mae_weight': 0.3,      # MAEæƒé‡
        'r2_weight': 0.2,       # RÂ²æƒé‡ï¼ˆå®é™…ä½¿ç”¨1-RÂ²ä½œä¸ºæƒ©ç½šé¡¹ï¼‰
        'mape_weight': 0.1      # MAPEæƒé‡
    }

    # å½’ä¸€åŒ–å‚æ•°
    NORMALIZATION_PARAMS = {
        'mse_scale': 100.0,     # MSEæŸå¤±é€šå¸¸åœ¨0-100èŒƒå›´
        'mae_scale': 10.0,      # MAEé€šå¸¸åœ¨0-10èŒƒå›´
        'mape_scale': 100.0     # MAPEæ˜¯ç™¾åˆ†æ¯”ï¼Œé€šå¸¸åœ¨0-100èŒƒå›´
    }

    # --- Fixed Training Parameters ---
    VALIDATION_SPLIT_RATIO = 0.15
    TEST_SPLIT_RATIO = 0.15
    EARLY_STOPPING_PATIENCE = 15
    MIN_DELTA = 1e-6

def set_random_seeds(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

def optimize_gpu_performance():
    """GPUæ€§èƒ½ä¼˜åŒ–è®¾ç½®"""
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.cuda.empty_cache()
        print("âœ… GPUæ€§èƒ½ä¼˜åŒ–å·²å¯ç”¨")

# --- Bayesian Optimization Search Space ---
# å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
search_space = [
    # WPMixeræ ¸å¿ƒå‚æ•°
    Integer(32, 256, name='d_model'),                    # æ¨¡å‹ç»´åº¦
    Integer(4, 16, name='patch_len'),                    # è¡¥ä¸é•¿åº¦
    Integer(2, 8, name='patch_stride'),                  # è¡¥ä¸æ­¥é•¿
    Integer(30, 120, name='price_seq_len'),              # ä»·æ ¼åºåˆ—é•¿åº¦
    Categorical(['db1', 'db4', 'db8', 'haar'], name='wavelet_name'),  # å°æ³¢ç±»å‹
    Integer(1, 4, name='wavelet_level'),                 # å°æ³¢åˆ†è§£å±‚æ•°
    Integer(2, 8, name='tfactor'),                       # Tokenæ··åˆå™¨æ‰©å±•å› å­
    Integer(2, 8, name='dfactor'),                       # åµŒå…¥æ··åˆå™¨æ‰©å±•å› å­
    
    # MLPå‚æ•°
    Integer(256, 2048, name='mlp_hidden_dim_1'),         # MLPç¬¬ä¸€éšè—å±‚ç»´åº¦
    Integer(128, 1024, name='mlp_hidden_dim_2'),         # MLPç¬¬äºŒéšè—å±‚ç»´åº¦
    
    # è®­ç»ƒå‚æ•°
    Integer(16, 128, name='batch_size'),                 # æ‰¹æ¬¡å¤§å°
    Real(1e-5, 1e-2, prior='log-uniform', name='learning_rate'),  # å­¦ä¹ ç‡
    Real(1e-6, 1e-2, prior='log-uniform', name='weight_decay'),   # æƒé‡è¡°å‡
    Real(0.0, 0.5, name='dropout'),                      # Dropoutç‡
    Integer(20, 100, name='epochs'),                     # è®­ç»ƒè½®æ•°
]

# æå–å‚æ•°åç§°ç”¨äºåç»­ä½¿ç”¨
param_names = [dim.name for dim in search_space]

def evaluate_model_performance(model, data_loader, criterion, device, scaler=None):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    total_loss, all_preds, all_targets = 0.0, [], []
    
    with torch.no_grad():
        for batch_data in data_loader:
            price_seq = batch_data['price_seq'].to(device)
            target_data = batch_data['target_price'].to(device)
            
            outputs = model(price_data=price_seq)
            outputs = outputs.squeeze(-1)  # [batch, num_coins, 1] -> [batch, num_coins]
            
            loss = criterion(outputs, target_data)
            total_loss += loss.item() * price_seq.size(0)
            all_preds.append(outputs.cpu().numpy())
            all_targets.append(target_data.cpu().numpy())
    
    avg_loss = total_loss / len(data_loader.dataset)
    all_preds = np.concatenate(all_preds, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    
    # åå½’ä¸€åŒ–ç”¨äºè®¡ç®—çœŸå®æŒ‡æ ‡
    if scaler:
        original_preds = scaler.inverse_transform(all_preds)
        original_targets = scaler.inverse_transform(all_targets)
    else:
        original_preds = all_preds
        original_targets = all_targets
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mae = mean_absolute_error(original_targets, original_preds)
    mse = mean_squared_error(original_targets, original_preds)
    rmse = np.sqrt(mse)
    r2 = r2_score(original_targets, original_preds)
    mape = mean_absolute_percentage_error(original_targets, original_preds)
    
    metrics = {
        'loss': avg_loss,
        'mae': mae,
        'mse': mse,
        'rmse': rmse,
        'r2': r2,
        'mape': mape
    }
    
    return metrics, all_preds, all_targets

def calculate_optimization_score(metrics, objective_type='composite'):
    """
    è®¡ç®—ä¼˜åŒ–è¯„åˆ†

    Args:
        metrics: åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        objective_type: ä¼˜åŒ–ç›®æ ‡ç±»å‹

    Returns:
        score: ä¼˜åŒ–è¯„åˆ†ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        details: è¯„åˆ†è¯¦æƒ…å­—å…¸
    """
    val_loss = metrics['loss']      # MSEæŸå¤±
    val_mae = metrics['mae']        # å¹³å‡ç»å¯¹è¯¯å·®
    val_r2 = metrics['r2']          # å†³å®šç³»æ•°
    val_mape = metrics['mape']      # å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®

    if objective_type == 'mse_only':
        # ä»…ä¼˜åŒ–MSEæŸå¤±
        score = val_loss
        details = {
            'score_type': 'MSE Loss Only',
            'mse_loss': val_loss,
            'final_score': score
        }

    elif objective_type == 'mae_focused':
        # ä¸»è¦ä¼˜åŒ–MAEï¼Œè¾…åŠ©è€ƒè™‘RÂ²
        r2_penalty = max(0, 1 - val_r2)
        score = 0.8 * val_mae + 0.2 * r2_penalty
        details = {
            'score_type': 'MAE Focused',
            'mae': val_mae,
            'r2_penalty': r2_penalty,
            'final_score': score
        }

    elif objective_type == 'r2_focused':
        # ä¸»è¦ä¼˜åŒ–RÂ²ï¼Œè¾…åŠ©è€ƒè™‘MSE
        r2_penalty = max(0, 1 - val_r2)
        normalized_mse = val_loss / NORMALIZATION_PARAMS['mse_scale']
        score = 0.7 * r2_penalty + 0.3 * normalized_mse
        details = {
            'score_type': 'RÂ² Focused',
            'r2_penalty': r2_penalty,
            'normalized_mse': normalized_mse,
            'final_score': score
        }

    else:  # 'composite'
        # ç»¼åˆä¼˜åŒ–å¤šä¸ªæŒ‡æ ‡
        normalized_loss = val_loss / NORMALIZATION_PARAMS['mse_scale']
        normalized_mae = val_mae / NORMALIZATION_PARAMS['mae_scale']
        normalized_r2_penalty = max(0, 1 - val_r2)
        normalized_mape = min(val_mape / NORMALIZATION_PARAMS['mape_scale'], 1.0)

        score = (
            COMPOSITE_WEIGHTS['mse_weight'] * normalized_loss +
            COMPOSITE_WEIGHTS['mae_weight'] * normalized_mae +
            COMPOSITE_WEIGHTS['r2_weight'] * normalized_r2_penalty +
            COMPOSITE_WEIGHTS['mape_weight'] * normalized_mape
        )

        details = {
            'score_type': 'Composite Score',
            'normalized_mse': normalized_loss,
            'normalized_mae': normalized_mae,
            'r2_penalty': normalized_r2_penalty,
            'normalized_mape': normalized_mape,
            'weights': COMPOSITE_WEIGHTS,
            'final_score': score
        }

    return score, details

class WPMixerConfigs:
    """WPMixeré…ç½®ç±»"""
    def __init__(self, **kwargs):
        # ä»kwargsä¸­è®¾ç½®å±æ€§
        for key, value in kwargs.items():
            setattr(self, key, value)
        
        # è®¾ç½®å›ºå®šå±æ€§
        self.pred_length = 1
        self.no_decomposition = False
        self.use_amp = False
        self.task_type = TASK_TYPE
        self.device = DEVICE

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨æ•°æ®é›†
global_dataset = None
global_train_loader = None
global_val_loader = None
global_test_loader = None
global_scaler = None

def prepare_data():
    """å‡†å¤‡æ•°æ®é›†ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰"""
    global global_dataset, global_train_loader, global_val_loader, global_test_loader, global_scaler

    if global_dataset is not None:
        return  # æ•°æ®å·²ç»å‡†å¤‡å¥½äº†

    print("ğŸ“Š å‡†å¤‡æ•°æ®é›†...")

    # åŠ è½½ä»·æ ¼æ•°æ®
    price_df_raw = pd.read_csv(PRICE_CSV_PATH, index_col=0, parse_dates=True)
    rename_map = {f"{coin}-USDT": coin for coin in COIN_NAMES}
    price_df_full = price_df_raw.rename(columns=rename_map)[COIN_NAMES]

    # ç¡®ä¿æ—¶é—´ç´¢å¼•æ˜¯å‡åºæ’åˆ—
    if not price_df_full.index.is_monotonic_increasing:
        price_df_full = price_df_full.sort_index()

    # åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨é»˜è®¤åºåˆ—é•¿åº¦ï¼‰
    dataset = UnifiedCryptoDataset(
        price_data_df=price_df_full,
        news_data_dict=None,
        seq_len=60,  # é»˜è®¤å€¼ï¼Œä¼šåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´
        processed_news_features_path=None,
        force_recompute_news=False,
        time_encoding_enabled=TIME_ENCODING_ENABLED_IN_DATASET,
        time_freq=TIME_FREQ_IN_DATASET,
    )

    # æ•°æ®é›†åˆ’åˆ†
    total_size = len(dataset)
    test_size = int(TEST_SPLIT_RATIO * total_size)
    val_size = int(VALIDATION_SPLIT_RATIO * total_size)
    train_size = total_size - test_size - val_size

    train_indices = list(range(0, train_size))
    val_indices = list(range(train_size, train_size + val_size))
    test_indices = list(range(train_size + val_size, total_size))

    # æ•°æ®æ ‡å‡†åŒ–
    if NORM_TYPE != 'none':
        if NORM_TYPE == 'standard':
            scaler = StandardScaler()
        elif NORM_TYPE == 'minmax':
            scaler = MinMaxScaler()

        # åªä½¿ç”¨è®­ç»ƒé›†æ•°æ®æ¥æ‹Ÿåˆæ ‡å‡†åŒ–å™¨
        train_data_for_scaling = []
        for idx in train_indices:
            sample = dataset[idx]
            price_seq = sample['price_seq'].numpy()
            target_price = sample['target_price'].numpy()
            train_data_for_scaling.append(price_seq)
            train_data_for_scaling.append(target_price.reshape(1, -1))

        train_data_array = np.vstack(train_data_for_scaling)
        scaler.fit(train_data_array)

        # æ›´æ–°æ•°æ®é›†
        original_price_df = dataset.price_data_df.copy()
        scaled_values = scaler.transform(original_price_df.values)
        dataset.price_data_df = pd.DataFrame(
            scaled_values,
            columns=original_price_df.columns,
            index=original_price_df.index
        )
        global_scaler = scaler
    else:
        global_scaler = None

    # åˆ›å»ºæ•°æ®å­é›†
    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)
    test_dataset = Subset(dataset, test_indices)

    # å­˜å‚¨å…¨å±€å˜é‡ - ä¿®å¤ï¼šæ­£ç¡®è®¾ç½®å…¨å±€å˜é‡
    global_dataset = dataset
    global_train_loader = train_dataset
    global_val_loader = val_dataset
    global_test_loader = test_dataset

    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: è®­ç»ƒé›†={len(train_dataset)}, éªŒè¯é›†={len(val_dataset)}, æµ‹è¯•é›†={len(test_dataset)}")

    # è¿”å›æ•°æ®é›†ä»¥ä¾¿å¤–éƒ¨è®¿é—®
    return dataset, train_dataset, val_dataset, test_dataset, scaler

def objective(params_list):
    """è´å¶æ–¯ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°"""
    global global_dataset, global_train_loader, global_val_loader, global_scaler

    # å°†å‚æ•°åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸ï¼Œå¹¶ç¡®ä¿ç±»å‹æ­£ç¡®
    params = {}
    for i, name in enumerate(param_names):
        value = params_list[i]
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        if hasattr(value, 'item'):
            value = value.item()
        params[name] = value

    try:
        # è®¾ç½®éšæœºç§å­
        set_random_seeds(RANDOM_SEED)

        print(f"\nğŸ” è¯„ä¼°å‚æ•°ç»„åˆ: {params}")

        # ç¡®ä¿æ•°æ®å·²å‡†å¤‡
        if global_dataset is None:
            prepare_data()

        # æ›´æ–°æ•°æ®é›†çš„åºåˆ—é•¿åº¦
        if global_dataset.seq_len != params['price_seq_len']:
            global_dataset.seq_len = params['price_seq_len']
            print(f"ğŸ“ æ›´æ–°åºåˆ—é•¿åº¦ä¸º: {params['price_seq_len']}")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œç¡®ä¿batch_sizeæ˜¯æ•´æ•°
        batch_size = int(params['batch_size'])
        train_loader = DataLoader(global_train_loader, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(global_val_loader, batch_size=batch_size, shuffle=False)

        # åˆ›å»ºæ¨¡å‹é…ç½®
        configs = WPMixerConfigs(
            input_length=params['price_seq_len'],
            num_coins=global_dataset.num_coins,
            d_model=params['d_model'],
            patch_len=params['patch_len'],
            patch_stride=params['patch_stride'],
            wavelet_name=params['wavelet_name'],
            level=params['wavelet_level'],
            tfactor=params['tfactor'],
            dfactor=params['dfactor'],
            dropout=params['dropout']
        )

        # åˆ›å»ºæ¨¡å‹
        model = UnifiedWPMixer(
            configs=configs,
            use_gcn=USE_GCN,
            gcn_config='improved_light',
            news_feature_dim=None,
            gcn_hidden_dim=256,
            gcn_output_dim=128,
            news_processed_dim=64,
            mlp_hidden_dim_1=params['mlp_hidden_dim_1'],
            mlp_hidden_dim_2=params['mlp_hidden_dim_2'],
            num_classes=1
        ).to(DEVICE)

        # è®¾ç½®è®­ç»ƒç»„ä»¶
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.8, min_lr=1e-7)

        # è®­ç»ƒå¾ªç¯
        best_val_loss = float('inf')
        patience_counter = 0

        for epoch in range(params['epochs']):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            epoch_loss = 0.0

            for batch_data in train_loader:
                price_seq = batch_data['price_seq'].to(DEVICE)
                target_data = batch_data['target_price'].to(DEVICE)

                optimizer.zero_grad()
                outputs = model(price_data=price_seq)
                outputs = outputs.squeeze(-1)

                loss = criterion(outputs, target_data)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

                epoch_loss += loss.item() * price_seq.size(0)

            # éªŒè¯é˜¶æ®µ
            val_metrics, _, _ = evaluate_model_performance(model, val_loader, criterion, DEVICE, global_scaler)
            val_loss = val_metrics['loss']

            scheduler.step(val_loss)

            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss - MIN_DELTA:
                best_val_loss = val_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if patience_counter >= EARLY_STOPPING_PATIENCE:
                print(f"â¹ï¸ æ—©åœåœ¨ç¬¬ {epoch+1} è½®")
                break

        # è·å–æœ€ç»ˆéªŒè¯æŒ‡æ ‡è¿›è¡Œç»¼åˆè¯„ä¼°
        final_val_metrics, _, _ = evaluate_model_performance(model, val_loader, criterion, DEVICE, global_scaler)

        # è®¡ç®—ä¼˜åŒ–è¯„åˆ†
        optimization_score, score_details = calculate_optimization_score(final_val_metrics, OPTIMIZATION_OBJECTIVE)

        # æ‰“å°è¯¦ç»†çš„è¯„ä¼°ç»“æœ
        print(f"ğŸ“Š æœ€ç»ˆéªŒè¯æŒ‡æ ‡:")
        print(f"   MSE Loss: {final_val_metrics['loss']:.6f}")
        print(f"   MAE: {final_val_metrics['mae']:.6f}")
        print(f"   RÂ²: {final_val_metrics['r2']:.6f}")
        print(f"   MAPE: {final_val_metrics['mape']:.6f}")

        print(f"ğŸ“Š ä¼˜åŒ–è¯„åˆ†è¯¦æƒ… ({score_details['score_type']}):")
        if OPTIMIZATION_OBJECTIVE == 'composite':
            print(f"   å½’ä¸€åŒ–MSE: {score_details['normalized_mse']:.6f} (æƒé‡{COMPOSITE_WEIGHTS['mse_weight']:.1%})")
            print(f"   å½’ä¸€åŒ–MAE: {score_details['normalized_mae']:.6f} (æƒé‡{COMPOSITE_WEIGHTS['mae_weight']:.1%})")
            print(f"   RÂ²æƒ©ç½šé¡¹: {score_details['r2_penalty']:.6f} (æƒé‡{COMPOSITE_WEIGHTS['r2_weight']:.1%})")
            print(f"   å½’ä¸€åŒ–MAPE: {score_details['normalized_mape']:.6f} (æƒé‡{COMPOSITE_WEIGHTS['mape_weight']:.1%})")
        elif OPTIMIZATION_OBJECTIVE == 'mae_focused':
            print(f"   MAE: {score_details['mae']:.6f} (æƒé‡80%)")
            print(f"   RÂ²æƒ©ç½šé¡¹: {score_details['r2_penalty']:.6f} (æƒé‡20%)")
        elif OPTIMIZATION_OBJECTIVE == 'r2_focused':
            print(f"   RÂ²æƒ©ç½šé¡¹: {score_details['r2_penalty']:.6f} (æƒé‡70%)")
            print(f"   å½’ä¸€åŒ–MSE: {score_details['normalized_mse']:.6f} (æƒé‡30%)")
        else:  # mse_only
            print(f"   MSEæŸå¤±: {score_details['mse_loss']:.6f}")

        print(f"   æœ€ç»ˆä¼˜åŒ–è¯„åˆ†: {optimization_score:.6f}")

        # ç¡®ä¿è¿”å›å€¼æ˜¯æœ‰é™çš„
        if np.isnan(optimization_score) or np.isinf(optimization_score):
            print(f"âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆä¼˜åŒ–è¯„åˆ†ï¼Œè¿”å›å¤§æ•°å€¼")
            return 1e6

        return float(optimization_score)

    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1e6  # è¿”å›å¤§æ•°å€¼è€Œä¸æ˜¯æ— ç©·å¤§

def save_optimization_results(result, best_params, best_score, test_metrics=None):
    """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
    os.makedirs(CACHE_DIR, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = os.path.join(CACHE_DIR, f"bayesian_optimization_results_{timestamp}.json")

    results_data = {
        'best_params': best_params,
        'best_score': best_score,
        'test_metrics': test_metrics,
        'optimization_objective': OPTIMIZATION_OBJECTIVE,
        'composite_weights': COMPOSITE_WEIGHTS if OPTIMIZATION_OBJECTIVE == 'composite' else None,
        'optimization_history': {
            'func_vals': result.func_vals.tolist(),
            'x_iters': [dict(zip(param_names, x)) for x in result.x_iters],
        },
        'search_space': {dim.name: str(dim) for dim in search_space},
        'n_calls': N_CALLS,
        'n_random_starts': N_RANDOM_STARTS,
        'timestamp': datetime.now().isoformat()
    }

    with open(results_file, 'w') as f:
        json.dump(results_data, f, indent=2)

    print(f"ğŸ’¾ ä¼˜åŒ–ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    # ä¿å­˜æœ€ä½³å‚æ•°çš„ç®€åŒ–ç‰ˆæœ¬
    best_params_file = os.path.join(CACHE_DIR, "best_params.json")
    with open(best_params_file, 'w') as f:
        json.dump(best_params, f, indent=2)

    # ä¿å­˜æœ€ä½³å‚æ•°çš„Pythoné…ç½®æ–‡ä»¶æ ¼å¼
    best_params_py_file = os.path.join(CACHE_DIR, f"best_params_{timestamp}.py")
    save_best_params_as_python_config(best_params, best_score, test_metrics, best_params_py_file)

    # ä¿å­˜æœ€ä½³å‚æ•°çš„YAMLæ ¼å¼ï¼ˆä¾¿äºé˜…è¯»ï¼‰
    best_params_yaml_file = os.path.join(CACHE_DIR, f"best_params_{timestamp}.yaml")
    save_best_params_as_yaml(best_params, best_score, test_metrics, best_params_yaml_file)

    return results_file, best_params_file, best_params_py_file, best_params_yaml_file

def save_best_params_as_python_config(best_params, best_score, test_metrics, filepath):
    """å°†æœ€ä½³å‚æ•°ä¿å­˜ä¸ºPythoné…ç½®æ–‡ä»¶æ ¼å¼"""

    config_content = f'''"""
æœ€ä½³è´å¶æ–¯ä¼˜åŒ–å‚æ•°é…ç½®
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ä¼˜åŒ–ç›®æ ‡: {OPTIMIZATION_OBJECTIVE}
æœ€ä½³è¯„åˆ†: {best_score:.6f}
"""

# =============================================================================
# æœ€ä½³è¶…å‚æ•°é…ç½®
# =============================================================================

# WPMixeræ ¸å¿ƒå‚æ•°
D_MODEL = {best_params['d_model']}
PATCH_LEN = {best_params['patch_len']}
PATCH_STRIDE = {best_params['patch_stride']}
PRICE_SEQ_LEN = {best_params['price_seq_len']}
WAVELET_NAME = '{best_params['wavelet_name']}'
WAVELET_LEVEL = {best_params['wavelet_level']}
TFACTOR = {best_params['tfactor']}
DFACTOR = {best_params['dfactor']}

# MLPæ¶æ„å‚æ•°
MLP_HIDDEN_DIM_1 = {best_params['mlp_hidden_dim_1']}
MLP_HIDDEN_DIM_2 = {best_params['mlp_hidden_dim_2']}

# è®­ç»ƒå‚æ•°
BATCH_SIZE = {best_params['batch_size']}
LEARNING_RATE = {best_params['learning_rate']:.8f}
WEIGHT_DECAY = {best_params['weight_decay']:.8f}
DROPOUT = {best_params['dropout']:.6f}
EPOCHS = {best_params['epochs']}

# =============================================================================
# ä¼˜åŒ–ç»“æœ
# =============================================================================

OPTIMIZATION_SCORE = {best_score:.6f}
OPTIMIZATION_OBJECTIVE = '{OPTIMIZATION_OBJECTIVE}'
'''

    if OPTIMIZATION_OBJECTIVE == 'composite':
        config_content += f'''
COMPOSITE_WEIGHTS = {{
    'mse_weight': {COMPOSITE_WEIGHTS['mse_weight']},
    'mae_weight': {COMPOSITE_WEIGHTS['mae_weight']},
    'r2_weight': {COMPOSITE_WEIGHTS['r2_weight']},
    'mape_weight': {COMPOSITE_WEIGHTS['mape_weight']}
}}
'''

    if test_metrics:
        config_content += f'''
# æµ‹è¯•é›†æ€§èƒ½æŒ‡æ ‡
TEST_METRICS = {{
    'loss': {test_metrics.get('loss', 0):.6f},
    'mae': {test_metrics.get('mae', 0):.6f},
    'mse': {test_metrics.get('mse', 0):.6f},
    'rmse': {test_metrics.get('rmse', 0):.6f},
    'r2': {test_metrics.get('r2', 0):.6f},
    'mape': {test_metrics.get('mape', 0):.6f}
}}
'''

    config_content += '''
# =============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# =============================================================================

def get_wpmixer_config():
    """è·å–WPMixeré…ç½®å¯¹è±¡"""
    class WPMixerConfigs:
        def __init__(self):
            self.input_length = PRICE_SEQ_LEN
            self.pred_length = 1
            self.num_coins = 8
            self.d_model = D_MODEL
            self.patch_len = PATCH_LEN
            self.patch_stride = PATCH_STRIDE
            self.wavelet_name = WAVELET_NAME
            self.level = WAVELET_LEVEL
            self.tfactor = TFACTOR
            self.dfactor = DFACTOR
            self.no_decomposition = False
            self.use_amp = False
            self.dropout = DROPOUT
            self.task_type = 'regression'
            self.device = 'cuda'

    return WPMixerConfigs()

def get_training_config():
    """è·å–è®­ç»ƒé…ç½®"""
    return {
        'batch_size': BATCH_SIZE,
        'learning_rate': LEARNING_RATE,
        'weight_decay': WEIGHT_DECAY,
        'epochs': EPOCHS,
        'dropout': DROPOUT
    }

if __name__ == '__main__':
    print("ğŸ¯ æœ€ä½³è´å¶æ–¯ä¼˜åŒ–å‚æ•°")
    print(f"ä¼˜åŒ–è¯„åˆ†: {OPTIMIZATION_SCORE:.6f}")
    print(f"ä¼˜åŒ–ç›®æ ‡: {OPTIMIZATION_OBJECTIVE}")

    config = get_wpmixer_config()
    training_config = get_training_config()

    print("\\nğŸ“‹ WPMixeré…ç½®:")
    for attr in dir(config):
        if not attr.startswith('_'):
            print(f"  {attr}: {getattr(config, attr)}")

    print("\\nğŸƒ è®­ç»ƒé…ç½®:")
    for key, value in training_config.items():
        print(f"  {key}: {value}")
'''

    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(config_content)

    print(f"ğŸ“„ Pythoné…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {filepath}")

def save_best_params_as_yaml(best_params, best_score, test_metrics, filepath):
    """å°†æœ€ä½³å‚æ•°ä¿å­˜ä¸ºYAMLæ ¼å¼"""

    yaml_content = f'''# æœ€ä½³è´å¶æ–¯ä¼˜åŒ–å‚æ•°é…ç½®
# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# ä¼˜åŒ–ç›®æ ‡: {OPTIMIZATION_OBJECTIVE}
# æœ€ä½³è¯„åˆ†: {best_score:.6f}

optimization_info:
  score: {best_score:.6f}
  objective: "{OPTIMIZATION_OBJECTIVE}"
  timestamp: "{datetime.now().isoformat()}"
'''

    if OPTIMIZATION_OBJECTIVE == 'composite':
        yaml_content += f'''  composite_weights:
    mse_weight: {COMPOSITE_WEIGHTS['mse_weight']}
    mae_weight: {COMPOSITE_WEIGHTS['mae_weight']}
    r2_weight: {COMPOSITE_WEIGHTS['r2_weight']}
    mape_weight: {COMPOSITE_WEIGHTS['mape_weight']}
'''

    yaml_content += f'''
# WPMixeræ ¸å¿ƒå‚æ•°
wpmixer:
  d_model: {best_params['d_model']}
  patch_len: {best_params['patch_len']}
  patch_stride: {best_params['patch_stride']}
  price_seq_len: {best_params['price_seq_len']}
  wavelet_name: "{best_params['wavelet_name']}"
  wavelet_level: {best_params['wavelet_level']}
  tfactor: {best_params['tfactor']}
  dfactor: {best_params['dfactor']}

# MLPæ¶æ„å‚æ•°
mlp:
  hidden_dim_1: {best_params['mlp_hidden_dim_1']}
  hidden_dim_2: {best_params['mlp_hidden_dim_2']}

# è®­ç»ƒå‚æ•°
training:
  batch_size: {best_params['batch_size']}
  learning_rate: {best_params['learning_rate']:.8f}
  weight_decay: {best_params['weight_decay']:.8f}
  dropout: {best_params['dropout']:.6f}
  epochs: {best_params['epochs']}
'''

    if test_metrics:
        yaml_content += f'''
# æµ‹è¯•é›†æ€§èƒ½æŒ‡æ ‡
test_metrics:
  loss: {test_metrics.get('loss', 0):.6f}
  mae: {test_metrics.get('mae', 0):.6f}
  mse: {test_metrics.get('mse', 0):.6f}
  rmse: {test_metrics.get('rmse', 0):.6f}
  r2: {test_metrics.get('r2', 0):.6f}
  mape: {test_metrics.get('mape', 0):.6f}
'''

    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(yaml_content)

    print(f"ğŸ“„ YAMLé…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {filepath}")

def train_final_model(best_params):
    """ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
    global global_dataset, global_train_loader, global_val_loader, global_test_loader, global_scaler

    print(f"\nğŸš€ ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    print(f"ğŸ“‹ æœ€ä½³å‚æ•°: {best_params}")

    # è®¾ç½®éšæœºç§å­
    set_random_seeds(RANDOM_SEED)

    # æ›´æ–°æ•°æ®é›†åºåˆ—é•¿åº¦
    global_dataset.seq_len = best_params['price_seq_len']

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(global_train_loader, batch_size=best_params['batch_size'], shuffle=True)
    val_loader = DataLoader(global_val_loader, batch_size=best_params['batch_size'], shuffle=False)
    test_loader = DataLoader(global_test_loader, batch_size=best_params['batch_size'], shuffle=False)

    # åˆ›å»ºæœ€ç»ˆæ¨¡å‹
    configs = WPMixerConfigs(
        input_length=best_params['price_seq_len'],
        num_coins=global_dataset.num_coins,
        d_model=best_params['d_model'],
        patch_len=best_params['patch_len'],
        patch_stride=best_params['patch_stride'],
        wavelet_name=best_params['wavelet_name'],
        level=best_params['wavelet_level'],
        tfactor=best_params['tfactor'],
        dfactor=best_params['dfactor'],
        dropout=best_params['dropout']
    )

    model = UnifiedWPMixer(
        configs=configs,
        use_gcn=USE_GCN,
        gcn_config='improved_light',
        news_feature_dim=None,
        gcn_hidden_dim=256,
        gcn_output_dim=128,
        news_processed_dim=64,
        mlp_hidden_dim_1=best_params['mlp_hidden_dim_1'],
        mlp_hidden_dim_2=best_params['mlp_hidden_dim_2'],
        num_classes=1
    ).to(DEVICE)

    # è®­ç»ƒè®¾ç½®
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.8, min_lr=1e-7)

    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_path = os.path.join(CACHE_DIR, BEST_MODEL_NAME)

    print(f"ğŸƒ å¼€å§‹è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼Œæœ€å¤§è½®æ•°: {best_params['epochs']}")

    for epoch in range(best_params['epochs']):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        epoch_loss = 0.0

        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{best_params['epochs']}")
        for batch_data in train_pbar:
            price_seq = batch_data['price_seq'].to(DEVICE)
            target_data = batch_data['target_price'].to(DEVICE)

            optimizer.zero_grad()
            outputs = model(price_data=price_seq)
            outputs = outputs.squeeze(-1)

            loss = criterion(outputs, target_data)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            epoch_loss += loss.item() * price_seq.size(0)
            train_pbar.set_postfix({'loss': loss.item()})

        # éªŒè¯é˜¶æ®µ
        val_metrics, _, _ = evaluate_model_performance(model, val_loader, criterion, DEVICE, global_scaler)
        val_loss = val_metrics['loss']

        scheduler.step(val_loss)

        print(f"Epoch {epoch+1}: è®­ç»ƒæŸå¤±={epoch_loss/len(global_train_loader):.6f}, éªŒè¯æŸå¤±={val_loss:.6f}, R2={val_metrics['r2']:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss - MIN_DELTA:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), best_model_path)
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
        else:
            patience_counter += 1

        if patience_counter >= EARLY_STOPPING_PATIENCE:
            print(f"â¹ï¸ æ—©åœåœ¨ç¬¬ {epoch+1} è½®")
            break

    # åŠ è½½æœ€ä½³æ¨¡å‹å¹¶æµ‹è¯•
    if os.path.exists(best_model_path):
        model.load_state_dict(torch.load(best_model_path, map_location=DEVICE, weights_only=True))

    # æµ‹è¯•é˜¶æ®µ
    print(f"\nğŸ§ª æµ‹è¯•æœ€ç»ˆæ¨¡å‹...")
    test_metrics, _, _ = evaluate_model_performance(model, test_loader, criterion, DEVICE, global_scaler)

    print(f"\nğŸ‰ æœ€ç»ˆæµ‹è¯•ç»“æœ:")
    for name, value in test_metrics.items():
        print(f"  {name.upper()}: {value:.6f}")

    return model, test_metrics

if __name__ == '__main__':
    """
    è´å¶æ–¯ä¼˜åŒ–WPMixerä¸»æµç¨‹

    æµç¨‹ï¼š
    1. åˆå§‹åŒ–è®¾ç½®å’Œæ•°æ®å‡†å¤‡
    2. è¿è¡Œè´å¶æ–¯ä¼˜åŒ–å¯»æ‰¾æœ€ä½³è¶…å‚æ•°
    3. ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    4. è¯„ä¼°å’Œä¿å­˜ç»“æœ
    """

    print("ğŸ¯ å¼€å§‹è´å¶æ–¯ä¼˜åŒ–WPMixer")
    print("="*60)

    # åˆå§‹åŒ–è®¾ç½®
    set_random_seeds(RANDOM_SEED)
    print(f"ğŸ² è®¾ç½®éšæœºç§å­: {RANDOM_SEED}")
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {DEVICE}")

    # GPUæ€§èƒ½ä¼˜åŒ–
    optimize_gpu_performance()

    # åˆ›å»ºç¼“å­˜ç›®å½•
    os.makedirs(CACHE_DIR, exist_ok=True)
    print(f"ğŸ“ ç¼“å­˜ç›®å½•: {CACHE_DIR}")

    # å‡†å¤‡æ•°æ®
    prepare_data()

    # æ‰“å°æœç´¢ç©ºé—´ä¿¡æ¯
    print(f"\nğŸ” è´å¶æ–¯ä¼˜åŒ–é…ç½®:")
    print(f"  è¿­ä»£æ¬¡æ•°: {N_CALLS}")
    print(f"  éšæœºåˆå§‹åŒ–: {N_RANDOM_STARTS}")
    print(f"  æœç´¢ç©ºé—´ç»´åº¦: {len(search_space)}")
    print(f"  ä¼˜åŒ–ç›®æ ‡: {OPTIMIZATION_OBJECTIVE}")

    if OPTIMIZATION_OBJECTIVE == 'composite':
        print(f"  ç»¼åˆè¯„åˆ†æƒé‡:")
        print(f"    MSEæŸå¤±: {COMPOSITE_WEIGHTS['mse_weight']:.1%}")
        print(f"    MAE: {COMPOSITE_WEIGHTS['mae_weight']:.1%}")
        print(f"    RÂ²: {COMPOSITE_WEIGHTS['r2_weight']:.1%}")
        print(f"    MAPE: {COMPOSITE_WEIGHTS['mape_weight']:.1%}")

    print(f"\nğŸ“Š æœç´¢ç©ºé—´:")
    for dim in search_space:
        print(f"  {dim.name}: {dim}")

    # è¿è¡Œè´å¶æ–¯ä¼˜åŒ–
    print(f"\nğŸš€ å¼€å§‹è´å¶æ–¯ä¼˜åŒ–...")
    start_time = datetime.now()

    try:
        result = gp_minimize(
            func=objective,
            dimensions=search_space,
            n_calls=N_CALLS,
            n_random_starts=N_RANDOM_STARTS,
            acq_func='EI',  # Expected Improvement
            random_state=RANDOM_SEED,
            verbose=True
        )

        end_time = datetime.now()
        optimization_time = end_time - start_time

        print(f"\nâœ… è´å¶æ–¯ä¼˜åŒ–å®Œæˆ!")
        print(f"â±ï¸ ä¼˜åŒ–è€—æ—¶: {optimization_time}")
        print(f"ğŸ¯ æœ€ä½³éªŒè¯æŸå¤±: {result.fun:.6f}")

        # æå–æœ€ä½³å‚æ•°
        best_params = dict(zip(param_names, result.x))

        # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ˜¯PythonåŸç”Ÿç±»å‹ï¼Œä»¥é¿å…åç»­å‡ºç°ç±»å‹é”™è¯¯
        for name, value in best_params.items():
            if hasattr(value, 'item'):
                best_params[name] = value.item()
        print(f"\nğŸ† æœ€ä½³å‚æ•°ç»„åˆ:")
        for name, value in best_params.items():
            print(f"  {name}: {value}")

        # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        final_model, test_metrics = train_final_model(best_params)

        # ä¿å­˜ä¼˜åŒ–ç»“æœï¼ˆåŒ…å«æµ‹è¯•æŒ‡æ ‡ï¼‰
        results_file, best_params_file, best_params_py_file, best_params_yaml_file = save_optimization_results(
            result, best_params, result.fun, test_metrics
        )

        # ä¿å­˜æœ€ç»ˆç»“æœæ‘˜è¦
        summary_file = os.path.join(CACHE_DIR, "optimization_summary.json")
        summary_data = {
            'optimization_completed': True,
            'best_validation_loss': result.fun,
            'best_params': best_params,
            'test_metrics': test_metrics,
            'optimization_time_seconds': optimization_time.total_seconds(),
            'n_calls': N_CALLS,
            'timestamp': datetime.now().isoformat()
        }

        with open(summary_file, 'w') as f:
            json.dump(summary_data, f, indent=2)

        print(f"\nğŸ“‹ ä¼˜åŒ–æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
        print(f"ğŸ“Š è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        print(f"ğŸ¯ æœ€ä½³å‚æ•°æ–‡ä»¶:")
        print(f"  JSONæ ¼å¼: {best_params_file}")
        print(f"  Pythoné…ç½®: {best_params_py_file}")
        print(f"  YAMLæ ¼å¼: {best_params_yaml_file}")

        # æ‰“å°ä¼˜åŒ–å†å²çš„ç®€è¦ç»Ÿè®¡
        print(f"\nğŸ“ˆ ä¼˜åŒ–å†å²ç»Ÿè®¡:")
        print(f"  æœ€ä½³æŸå¤±: {min(result.func_vals):.6f}")
        print(f"  æœ€å·®æŸå¤±: {max(result.func_vals):.6f}")
        print(f"  å¹³å‡æŸå¤±: {np.mean(result.func_vals):.6f}")
        print(f"  æŸå¤±æ ‡å‡†å·®: {np.std(result.func_vals):.6f}")

        # æ‰¾å‡ºå‰5ä¸ªæœ€ä½³é…ç½®
        sorted_indices = np.argsort(result.func_vals)
        print(f"\nğŸ… å‰5ä¸ªæœ€ä½³é…ç½®:")
        for i, idx in enumerate(sorted_indices[:5]):
            params_dict = dict(zip(param_names, result.x_iters[idx]))
            print(f"  #{i+1}: æŸå¤±={result.func_vals[idx]:.6f}")
            print(f"       å‚æ•°: {params_dict}")

        print(f"\nğŸ‰ è´å¶æ–¯ä¼˜åŒ–å®Œæˆ! æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {os.path.join(CACHE_DIR, BEST_MODEL_NAME)}")

    except Exception as e:
        print(f"âŒ è´å¶æ–¯ä¼˜åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

        # ä¿å­˜å¤±è´¥ä¿¡æ¯
        error_file = os.path.join(CACHE_DIR, f"optimization_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
        with open(error_file, 'w') as f:
            f.write(f"Optimization failed at: {datetime.now()}\n")
            f.write(f"Error: {str(e)}\n")
            f.write(f"Traceback:\n{traceback.format_exc()}")

        print(f"ğŸ’¾ é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°: {error_file}")
        raise
