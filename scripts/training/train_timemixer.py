"""
TimeMixer-GCNç»Ÿä¸€æ¨¡å‹è®­ç»ƒè„šæœ¬

åŠŸèƒ½æ¦‚è¿°ï¼š
    æœ¬è„šæœ¬ç”¨äºè®­ç»ƒç»“åˆTimeMixerï¼ˆå¤šå°ºåº¦æ—¶é—´æ··åˆå™¨ï¼‰å’ŒGCNï¼ˆå›¾å·ç§¯ç½‘ç»œï¼‰çš„ç»Ÿä¸€æ¨¡å‹ï¼Œ
    ç”¨äºåŠ å¯†è´§å¸ä»·æ ¼é¢„æµ‹ä»»åŠ¡ã€‚

ä¸»è¦ç‰¹æ€§ï¼š
    1. TimeMixerå¤šå°ºåº¦å»ºæ¨¡ï¼šä½¿ç”¨å¤šå°ºåº¦æ—¶é—´æ··åˆå™¨æ•è·ä¸åŒæ—¶é—´å°ºåº¦çš„æ¨¡å¼
    2. GCNå›¾å»ºæ¨¡ï¼šå»ºæ¨¡åŠ å¯†è´§å¸ä¹‹é—´çš„å¤æ‚å…³ç³»
    3. å¤šæ¨¡æ€èåˆï¼šç»“åˆä»·æ ¼æ•°æ®å’Œæ–°é—»ç‰¹å¾
    4. çµæ´»é…ç½®ï¼šæ”¯æŒå¤šç§å›¾æ„å»ºæ–¹æ³•å’ŒGCNæ¶æ„
    5. å®Œæ•´è¯„ä¼°ï¼šåŒ…å«NEW_MAEç­‰å¤šç§è¯„ä¼°æŒ‡æ ‡

ä½œè€…ï¼šAI Assistant
åˆ›å»ºæ—¶é—´ï¼š2024
æœ€åæ›´æ–°ï¼š2024
"""

# === å¯¼å…¥å¿…è¦çš„åº“ ===
import torch                    # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import torch.nn as nn          # ç¥ç»ç½‘ç»œæ¨¡å—
import torch.optim as optim    # ä¼˜åŒ–å™¨æ¨¡å—
from torch.utils.data import DataLoader, Subset  # æ•°æ®åŠ è½½å’Œåˆ’åˆ†å·¥å…·
import pandas as pd            # æ•°æ®å¤„ç†åº“
from tqdm import tqdm         # è¿›åº¦æ¡æ˜¾ç¤º
import os                     # æ“ä½œç³»ç»Ÿæ¥å£
import sys                    # ç³»ç»Ÿç›¸å…³å‚æ•°å’Œå‡½æ•°
import numpy as np            # æ•°å€¼è®¡ç®—åº“
import random                 # éšæœºæ•°ç”Ÿæˆ
from sklearn.preprocessing import StandardScaler, MinMaxScaler  # æ•°æ®é¢„å¤„ç†
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error  # è¯„ä¼°æŒ‡æ ‡
import csv                    # CSVæ–‡ä»¶å¤„ç†
from datetime import datetime # æ—¥æœŸæ—¶é—´å¤„ç†

# === é¡¹ç›®è·¯å¾„é…ç½® ===
# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„ä¸­ï¼Œç¡®ä¿èƒ½å¤Ÿå¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()
project_root = os.path.join(current_dir, '..', '..')
sys.path.append(project_root)

# === æ¨¡å‹å’Œæ•°æ®é›†å¯¼å…¥ ===
# å¯¼å…¥TimeMixer-GCNç»Ÿä¸€æ¨¡å‹ï¼šç»“åˆå¤šå°ºåº¦æ—¶é—´æ··åˆå™¨å’Œå›¾å·ç§¯ç½‘ç»œ
from models.MixModel.unified_multiscale_timemixer_gcn import UnifiedMultiScaleTimeMixer
# å¯¼å…¥æ•°æ®é›†å¤„ç†ç±»ï¼šå¤„ç†åŠ å¯†è´§å¸ä»·æ ¼å’Œæ–°é—»æ•°æ®
from scripts.analysis.crypto_new_analyzer.unified_dataset import UnifiedCryptoDataset, load_news_data
# å¯¼å…¥å›¾æ„å»ºå·¥å…·ï¼šç”¨äºæ„å»ºåŠ å¯†è´§å¸ä¹‹é—´çš„å…³ç³»å›¾
from dataloader.gnn_loader import generate_edge_index, generate_advanced_edge_index, analyze_graph_properties


# --- TimeMixer Base Configuration Class (Referenced from train_test_timemixer_gcn.py) ---
class TimeMixerBaseConfigs:
    """Holds the detailed configuration for the TimeMixer base model, including multi-scale parameters."""
    def __init__(self, num_nodes, price_seq_len, num_time_features,
                 task_type='regression',
                 d_model=64, pred_len=1, dropout=0.1, n_heads=4, d_ff=128, e_layers=2,
                 freq='h', embed_type='timeF', use_norm: bool = False, decomp_method='moving_avg',
                 down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg',
                 channel_independence=False, moving_avg=25):
        # Essential dimensions
        self.enc_in = num_nodes
        self.seq_len = price_seq_len
        self.d_model = d_model
        self.num_time_features = num_time_features
        self.task_type = task_type
        # Multi-scale parameters
        self.down_sampling_layers = down_sampling_layers
        self.down_sampling_window = down_sampling_window
        self.down_sampling_method = down_sampling_method
        # TimeMixer-specific parameters
        self.e_layers = e_layers
        self.n_heads = n_heads
        self.d_ff = d_ff
        self.dropout = dropout
        self.channel_independence = channel_independence
        # Other parameters from reference
        self.pred_len = pred_len
        self.freq = freq
        self.embed = embed_type
        self.use_norm = use_norm
        self.decomp_method = decomp_method
        self.moving_avg = moving_avg
        self.dec_in = num_nodes
        self.c_out = num_nodes
        self.label_len = int(price_seq_len * 0.5)
        self.output_attention = False
        self.factor = 5
        self.activation = 'gelu'

# --- Main Configuration ---
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# --- Master Switches ---
# é¢„æµ‹ç›®æ ‡é…ç½®ï¼š
# - 'price': é¢„æµ‹ç»å¯¹ä»·æ ¼ (ä»…å›å½’)
# - 'diff': é¢„æµ‹ä»·æ ¼å·®åˆ† (ä»…åˆ†ç±»)
# - 'return': é¢„æµ‹ä»·æ ¼å˜åŒ–ç‡ (ä»…åˆ†ç±»)
PREDICTION_TARGET = 'price'

# ä»»åŠ¡ç±»å‹è‡ªåŠ¨ç¡®å®š
TASK_TYPE = 'regression' if PREDICTION_TARGET == 'price' else 'classification'
USE_GCN = False  # é‡æ–°å¯ç”¨GCNæµ‹è¯•æ”¹è¿›åçš„æ¨¡å‹è¯•æ–°çš„å›¾æ„å»ºæ–¹æ³•
USE_NEWS_FEATURES = True

# --- Graph Construction Configuration ---
# åŸºäºå¿«é€Ÿæ¯”è¾ƒç»“æœï¼ŒåŸå§‹æ–¹æ³•è¡¨ç°æœ€ä½³ï¼
GRAPH_METHOD = 'dynamic'  # 'original', 'multi_layer', 'dynamic', 'domain_knowledge', 'attention_based'
GRAPH_PARAMS = {
    'original': {'threshold': 0.6},
    'multi_layer': {'correlation_threshold': 0.3, 'volatility_threshold': 0.5, 'trend_threshold': 0.4},
    'dynamic': {'window_size': 168, 'overlap': 24},
    'domain_knowledge': {'coin_names': ['BTC', 'ETH', 'BNB', 'XRP', 'LTC', 'DOGE', 'SOL', 'AVAX']},
    'attention_based': {'top_k': 3, 'use_returns': True}
}

# --- GCN Configuration ---
# æµ‹è¯•æ”¹è¿›çš„GCNé…ç½®
GCN_CONFIG = 'improved_light'  # 'basic', 'improved_light', 'improved_gelu', 'gat_attention', 'adaptive'

# --- Data & Cache Paths ---
PRICE_CSV_PATH = 'scripts/analysis/crypto_analysis/data/processed_data/1H/all_1H.csv'
NEWS_FEATURES_FOLDER = 'scripts/analysis/crypto_new_analyzer/features'
CACHE_DIR = "experiments/cache"
BEST_MODEL_NAME = "best_unified_multiscale.pt"

# --- Dataset Parameters ---
COIN_NAMES = ['BTC', 'ETH', 'BNB', 'XRP', 'LTC', 'DOGE', 'SOL', 'AVAX']
PRICE_SEQ_LEN = 90 # As per train_test file
THRESHOLD = 0.6
NORM_TYPE = 'standard'
TIME_ENCODING_ENABLED_IN_DATASET = True
TIME_FREQ_IN_DATASET = 'h'

# --- Unified Model Parameters ---
NEWS_PROCESSED_DIM = 32
GCN_HIDDEN_DIM = 256
GCN_OUTPUT_DIM = 128
PREDICTION_HEAD_DIM = 64
MLP_HIDDEN_DIM = 256
NUM_CLASSES = 1 if TASK_TYPE == 'regression' else 2

# --- TimeMixer Base Model Configs (from train_test file) ---
D_MODEL = 64
E_LAYERS = 2
N_HEADS = 4
D_FF = 128
DROPOUT_TIMEMIXER = 0.1
DOWN_SAMPLING_LAYERS = 2
DOWN_SAMPLING_WINDOW = 2
DOWN_SAMPLING_METHOD = 'avg'
CHANNEL_INDEPENDENCE = True
USE_NORM = False

# --- Training Parameters ---
BATCH_SIZE = 32
EPOCHS = 50  # å¢åŠ è®­ç»ƒè½®æ•°
LEARNING_RATE = 0.001
WEIGHT_DECAY = 1e-5
VALIDATION_SPLIT_RATIO = 0.15
TEST_SPLIT_RATIO = 0.15
FORCE_RECOMPUTE_NEWS = False
RANDOM_SEED = 42
# æ—©åœå‚æ•°
EARLY_STOPPING_PATIENCE = 20
MIN_DELTA = 1e-6

def set_random_seeds(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

# --- Dynamic File Path ---
# æ ¹æ®ä»»åŠ¡ç±»å‹åˆ›å»ºå­ç›®å½•
task_dir = "classification" if TASK_TYPE == "classification" else "regression"
model_save_dir = os.path.join(CACHE_DIR, task_dir)

# ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
os.makedirs(model_save_dir, exist_ok=True)

model_variant = ['TimeMixer', TASK_TYPE]
model_variant.append("with_gcn" if USE_GCN else "no_gcn")
model_variant.append("with_news" if USE_NEWS_FEATURES else "no_news")
model_variant_str = "_".join(model_variant)
BEST_MODEL_PATH = os.path.join(model_save_dir, f"{model_variant_str}_{BEST_MODEL_NAME}")
print(f"--- Configuration: {model_variant_str} ---")
print(f"Best model will be saved to: {BEST_MODEL_PATH}")

def save_classification_results(all_preds, all_targets, coin_names, model_name, test_metrics=None):
    """ä¿å­˜åˆ†ç±»ä»»åŠ¡çš„æµ‹è¯•ç»“æœ"""
    import csv
    import os
    from datetime import datetime

    base_save_dir = "experiments/cache/test_predictions"
    model_save_dir = os.path.join(base_save_dir, model_name)
    os.makedirs(model_save_dir, exist_ok=True)

    # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
    predictions_file = os.path.join(model_save_dir, "test_predictions.csv")
    with open(predictions_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['sample_idx', 'coin', 'true_label', 'predicted_label', 'is_correct'])

        for sample_idx in range(len(all_preds)):
            for coin_idx, coin_name in enumerate(coin_names):
                true_val = all_targets[sample_idx, coin_idx]
                pred_val = all_preds[sample_idx, coin_idx]
                is_correct = 1 if true_val == pred_val else 0
                true_label = "ä¸Šæ¶¨" if true_val == 1 else "ä¸‹è·Œ"
                pred_label = "ä¸Šæ¶¨" if pred_val == 1 else "ä¸‹è·Œ"
                writer.writerow([sample_idx, coin_name, true_label, pred_label, is_correct])

    # ä¿å­˜æµ‹è¯•ç»“æœæ‘˜è¦
    if test_metrics:
        results_file = os.path.join(model_save_dir, "test_results.txt")
        with open(results_file, 'w', encoding='utf-8') as f:
            f.write("ğŸ‰ æœ€ç»ˆæµ‹è¯•ç»“æœ\n")
            f.write("="*60 + "\n")
            f.write("ğŸ“Š æ•´ä½“æŒ‡æ ‡:\n")

            for name, value in test_metrics.items():
                if not isinstance(value, dict) and isinstance(value, (int, float)):
                    if name == 'loss':
                        comment = "# æµ‹è¯•æŸå¤±"
                    elif name == 'accuracy':
                        comment = "# æ•´ä½“å‡†ç¡®ç‡"
                    elif name == 'precision':
                        comment = "# æ•´ä½“ç²¾ç¡®ç‡"
                    elif name == 'recall':
                        comment = "# æ•´ä½“å¬å›ç‡"
                    elif name == 'f1' or name == 'f1_score':
                        comment = "# æ•´ä½“F1åˆ†æ•°"
                    else:
                        comment = f"# {name}"
                    f.write(f"    - {name.upper()}: {value:.4f}  {comment}\n")

            f.write("\nğŸ“ˆ å„å¸ç§è¯¦ç»†æŒ‡æ ‡:\n")
            if 'per_coin_metrics' in test_metrics:
                for coin_name, coin_metrics in test_metrics['per_coin_metrics'].items():
                    f.write(f"  ğŸª™ {coin_name}:\n")
                    for metric_name, metric_value in coin_metrics.items():
                        if isinstance(metric_value, (int, float)):
                            comment = f"# {coin_name}çš„{metric_name}"
                            f.write(f"    - {metric_name.upper()}: {metric_value:.4f}  {comment}\n")

            f.write(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    print(f"âœ… åˆ†ç±»ä»»åŠ¡æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {model_save_dir}")

def save_test_predictions(all_preds, all_targets, coin_names, timestamp=None, test_metrics=None):
    """
    ä¿å­˜æµ‹è¯•é›†çš„é¢„æµ‹å€¼å’ŒçœŸå®å€¼åˆ°CSVæ–‡ä»¶

    Args:
        all_preds: é¢„æµ‹å€¼æ•°ç»„ [num_samples, num_coins]
        all_targets: çœŸå®å€¼æ•°ç»„ [num_samples, num_coins]
        coin_names: å¸ç§åç§°åˆ—è¡¨
        timestamp: æ—¶é—´æˆ³å­—ç¬¦ä¸²ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
    """
    if timestamp is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # åˆ›å»ºæŒ‰æ¨¡å‹åç§°åˆ†ç±»çš„ä¿å­˜ç›®å½•
    base_save_dir = "experiments/cache/test_predictions"
    model_save_dir = os.path.join(base_save_dir, f"TimeMixer_{timestamp}")
    os.makedirs(model_save_dir, exist_ok=True)

    # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
    predictions_file = os.path.join(model_save_dir, "test_predictions.csv")
    with open(predictions_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['timestamp', 'coin', 'true_value', 'predicted_value', 'absolute_error', 'percentage_error'])

        for sample_idx in range(len(all_preds)):
            for coin_idx, coin_name in enumerate(coin_names):
                true_val = all_targets[sample_idx, coin_idx]
                pred_val = all_preds[sample_idx, coin_idx]
                abs_error = abs(true_val - pred_val)
                pct_error = (abs_error / abs(true_val)) * 100 if abs(true_val) > 1e-8 else float('inf')

                writer.writerow([sample_idx, coin_name, true_val, pred_val, abs_error, pct_error])

    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    statistics_file = os.path.join(model_save_dir, "test_statistics.csv")
    with open(statistics_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['coin', 'mean_true', 'mean_pred', 'std_true', 'std_pred',
                        'min_true', 'min_pred', 'max_true', 'max_pred', 'mae', 'mape'])

        for coin_idx, coin_name in enumerate(coin_names):
            true_vals = all_targets[:, coin_idx]
            pred_vals = all_preds[:, coin_idx]

            mae = mean_absolute_error(true_vals, pred_vals)
            mape = np.mean(np.abs((true_vals - pred_vals) / np.where(np.abs(true_vals) > 1e-8, true_vals, 1e-8))) * 100

            writer.writerow([
                coin_name,
                np.mean(true_vals), np.mean(pred_vals),
                np.std(true_vals), np.std(pred_vals),
                np.min(true_vals), np.min(pred_vals),
                np.max(true_vals), np.max(pred_vals),
                mae, mape
            ])

    # === ä¿å­˜æ ¼å¼åŒ–çš„æµ‹è¯•ç»“æœ (TXTæ ¼å¼) ===
    if test_metrics:
        results_txt_file = os.path.join(model_save_dir, "test_results.txt")

        with open(results_txt_file, 'w', encoding='utf-8') as f:
            f.write("ğŸ‰ æœ€ç»ˆæµ‹è¯•ç»“æœ\n")
            f.write("="*60 + "\n")
            f.write("ğŸ“Š æ•´ä½“æŒ‡æ ‡:\n")

            # å†™å…¥æ•´ä½“æŒ‡æ ‡
            for name, value in test_metrics.items():
                if not isinstance(value, dict):  # è·³è¿‡åµŒå¥—å­—å…¸
                    if isinstance(value, (int, float)):
                        # ä¸ºä¸åŒæŒ‡æ ‡æ·»åŠ ä¸­æ–‡æ³¨é‡Š
                        if name.upper() == 'LOSS':
                            comment = "# æµ‹è¯•æŸå¤± - æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æŸå¤±å€¼"
                        elif name.upper() == 'MAE':
                            comment = "# å¹³å‡ç»å¯¹è¯¯å·® - é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å¹³å‡ç»å¯¹å·®"
                        elif name.upper() == 'RD':
                            comment = "# ç›¸å¯¹åå·® - |1-é¢„æµ‹å€¼æ€»å’Œ/çœŸå®å€¼æ€»å’Œ|"
                        elif name.upper() == 'MSE':
                            comment = "# å‡æ–¹è¯¯å·® - é¢„æµ‹å€¼ä¸çœŸå®å€¼å·®çš„å¹³æ–¹çš„å¹³å‡"
                        elif name.upper() == 'RMSE':
                            comment = "# å‡æ–¹æ ¹è¯¯å·® - MSEçš„å¹³æ–¹æ ¹"
                        elif name.upper() == 'R2':
                            comment = "# å†³å®šç³»æ•° - æ¨¡å‹è§£é‡Šæ•°æ®å˜å¼‚æ€§çš„æ¯”ä¾‹(è¶Šæ¥è¿‘1è¶Šå¥½)"
                        elif name.upper() == 'MAPE':
                            comment = "# å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® - ç›¸å¯¹è¯¯å·®çš„ç™¾åˆ†æ¯”"
                        elif 'NORMALIZED' in name.upper():
                            comment = f"# å½’ä¸€åŒ–{name.split('_')[-1]} - æ¶ˆé™¤å¸ç§ä»·æ ¼å°ºåº¦å½±å“çš„{name.split('_')[-1]}"
                        else:
                            comment = ""

                        if name.upper() == 'RD':
                            f.write(f"    - RD: {value:.4f}  {comment}\n")
                        else:
                            f.write(f"    - {name.upper()}: {value:.4f}  {comment}\n")
                    else:
                        f.write(f"    - {name.upper()}: {value}\n")

            f.write("\nğŸ“ˆ å„å¸ç§è¯¦ç»†æŒ‡æ ‡:\n")

            # å†™å…¥å„å¸ç§è¯¦ç»†æŒ‡æ ‡
            if 'per_coin_metrics' in test_metrics:
                for coin_name, coin_metrics in test_metrics['per_coin_metrics'].items():
                    f.write(f"  ğŸª™ {coin_name}:\n")
                    for metric_name, metric_value in coin_metrics.items():
                        if isinstance(metric_value, (int, float)):
                            # ä¸ºä¸åŒæŒ‡æ ‡æ·»åŠ ä¸­æ–‡æ³¨é‡Š
                            if metric_name.lower() == 'normalized_mae':
                                comment = f"# {coin_name}çš„å½’ä¸€åŒ–MAE"
                            elif metric_name.lower() == 'normalized_mse':
                                comment = f"# {coin_name}çš„å½’ä¸€åŒ–MSE"
                            elif metric_name.lower() == 'normalized_rmse':
                                comment = f"# {coin_name}çš„å½’ä¸€åŒ–RMSE"
                            elif metric_name.lower() == 'mape':
                                comment = f"# {coin_name}çš„å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®"
                            elif metric_name.lower() == 'r2':
                                comment = f"# {coin_name}çš„å†³å®šç³»æ•°"
                            elif metric_name.lower() == 'mae':
                                comment = f"# {coin_name}çš„å¹³å‡ç»å¯¹è¯¯å·®"
                            elif metric_name.lower() == 'mse':
                                comment = f"# {coin_name}çš„å‡æ–¹è¯¯å·®"
                            elif metric_name.lower() == 'rmse':
                                comment = f"# {coin_name}çš„å‡æ–¹æ ¹è¯¯å·®"
                            else:
                                comment = f"# {coin_name}çš„{metric_name}"
                            f.write(f"    - {metric_name.upper()}: {metric_value:.4f}  {comment}\n")
                        else:
                            f.write(f"    - {metric_name.upper()}: {metric_value}\n")

            f.write(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    print(f"æµ‹è¯•é›†é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° {model_save_dir}:")
    print(f"  è¯¦ç»†ç»“æœ: test_predictions.csv")
    print(f"  ç»Ÿè®¡ä¿¡æ¯: test_statistics.csv")
    if test_metrics:
        print(f"  æ ¼å¼åŒ–ç»“æœ: test_results.txt")

    return predictions_file, statistics_file

def evaluate_model(model, data_loader, criterion, edge_index, device, task_type, scaler=None):
    model.eval()
    total_loss, all_preds, all_targets = 0.0, [], []

    with torch.no_grad():
        for batch_data in tqdm(data_loader, desc="Evaluating"):
            price_seq = batch_data['price_seq'].to(device)
            target_data = batch_data['target_price'].to(device)
            x_mark_enc = batch_data.get('price_seq_mark')
            if x_mark_enc is not None: x_mark_enc = x_mark_enc.to(device)
            news_features = batch_data.get('news_features')
            if news_features is not None: news_features = news_features.to(device)
            outputs = model(price_seq, x_mark_enc, edge_index=edge_index, edge_weight=edge_weights, news_features=news_features)
            if task_type == 'classification':
                # === åˆ†ç±»ä»»åŠ¡ï¼šé¢„æµ‹ä»·æ ¼æ¶¨è·Œæ–¹å‘ ===
                if PREDICTION_TARGET in ('diff', 'return'):
                    # diff/returnï¼štarget å·²æ˜¯å·®åˆ†/æ”¶ç›Šç‡ï¼Œç›´æ¥åˆ¤æ–­æ­£è´Ÿ
                    targets = (target_data > 0).long()
                else:
                    raise ValueError(f"PREDICTION_TARGET '{PREDICTION_TARGET}' not supported for classification")

                # ç¡®ä¿è¾“å‡ºå’Œç›®æ ‡çš„å½¢çŠ¶åŒ¹é…
                if len(outputs.shape) == 3:  # [batch_size, num_nodes, num_classes]
                    outputs_flat = outputs.view(-1, NUM_CLASSES)
                    targets_flat = targets.view(-1)
                elif len(outputs.shape) == 2:  # [batch_size * num_nodes, num_classes]
                    outputs_flat = outputs
                    targets_flat = targets.view(-1)
                else:
                    raise ValueError(f"Unexpected output shape: {outputs.shape}")

                loss = criterion(outputs_flat, targets_flat)
                preds = torch.argmax(outputs, dim=-1)
            else: # regression
                if PREDICTION_TARGET == 'price':
                    targets = target_data
                else:
                    raise ValueError(f"PREDICTION_TARGET '{PREDICTION_TARGET}' not supported for regression")
                loss = criterion(outputs, targets)
                preds = outputs
            total_loss += loss.item() * price_seq.size(0)
            all_preds.append(preds.cpu().numpy())
            all_targets.append(targets.cpu().numpy())
    avg_loss = total_loss / len(data_loader.dataset)
    all_preds = np.concatenate(all_preds, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    metrics = {'loss': avg_loss}

    # === è®¡ç®—æŒ‡æ ‡ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹è®¡ç®—ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡ ===
    if task_type == 'classification':
        # === åˆ†ç±»ä»»åŠ¡æŒ‡æ ‡è®¡ç®— ===
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

        # æ•´ä½“åˆ†ç±»æŒ‡æ ‡
        overall_accuracy = accuracy_score(all_targets.flatten(), all_preds.flatten())
        overall_precision = precision_score(all_targets.flatten(), all_preds.flatten(), average='weighted', zero_division=0)
        overall_recall = recall_score(all_targets.flatten(), all_preds.flatten(), average='weighted', zero_division=0)
        overall_f1 = f1_score(all_targets.flatten(), all_preds.flatten(), average='weighted', zero_division=0)

        # æŒ‰ç±»åˆ«åˆ†ç±»æŒ‡æ ‡
        precision_per_class = precision_score(all_targets.flatten(), all_preds.flatten(), average=None, zero_division=0)
        recall_per_class = recall_score(all_targets.flatten(), all_preds.flatten(), average=None, zero_division=0)
        f1_per_class = f1_score(all_targets.flatten(), all_preds.flatten(), average=None, zero_division=0)

        # æ¯ä¸ªå¸ç§çš„åˆ†ç±»æŒ‡æ ‡
        per_coin_metrics = {}
        coin_accuracies = []
        coin_precisions = []
        coin_recalls = []
        coin_f1s = []
        
        for i, coin_name in enumerate(COIN_NAMES):
            coin_targets = all_targets[:, i]
            coin_preds = all_preds[:, i]

            coin_accuracy = accuracy_score(coin_targets, coin_preds)
            coin_precision = precision_score(coin_targets, coin_preds, average='weighted', zero_division=0)
            coin_recall = recall_score(coin_targets, coin_preds, average='weighted', zero_division=0)
            coin_f1 = f1_score(coin_targets, coin_preds, average='weighted', zero_division=0)

            coin_accuracies.append(coin_accuracy)
            coin_precisions.append(coin_precision)
            coin_recalls.append(coin_recall)
            coin_f1s.append(coin_f1)

            per_coin_metrics[coin_name] = {
                'accuracy': coin_accuracy,
                'precision': coin_precision,
                'recall': coin_recall,
                'f1_score': coin_f1
            }

        # è®¡ç®—æ··æ·†çŸ©é˜µï¼Œç¡®ä¿æ˜¯2x2
        conf_matrix = confusion_matrix(all_targets.flatten(), all_preds.flatten(), labels=[0, 1])

        # æ›´æ–°æŒ‡æ ‡å­—å…¸
        metrics.update({
            'accuracy': overall_accuracy,
            'precision': overall_precision,
            'recall': overall_recall,
            'f1_score': overall_f1,
            'avg_accuracy': np.mean(coin_accuracies),
            'avg_precision': np.mean(coin_precisions),
            'avg_recall': np.mean(coin_recalls),
            'avg_f1_score': np.mean(coin_f1s),
            'precision_class_0': precision_per_class[0] if len(precision_per_class) > 0 else 0,
            'precision_class_1': precision_per_class[1] if len(precision_per_class) > 1 else 0,
            'recall_class_0': recall_per_class[0] if len(recall_per_class) > 0 else 0,
            'recall_class_1': recall_per_class[1] if len(recall_per_class) > 1 else 0,
            'f1_class_0': f1_per_class[0] if len(f1_per_class) > 0 else 0,
            'f1_class_1': f1_per_class[1] if len(f1_per_class) > 1 else 0,
            'confusion_matrix': conf_matrix.tolist()
        })

        metrics['per_coin_metrics'] = per_coin_metrics
    else: # regression
        # Denormalize for metrics calculation (only for price regression)
        if PREDICTION_TARGET == 'price':
            if scaler:
                num_coins = all_preds.shape[1]
                original_preds = scaler.inverse_transform(all_preds.reshape(-1, num_coins))
                original_targets = scaler.inverse_transform(all_targets.reshape(-1, num_coins))
            else:
                original_preds = all_preds
                original_targets = all_targets
        else:
            # diff/return ä¸åº”è¯¥è¿›å…¥å›å½’åˆ†æ”¯
            raise ValueError(f"PREDICTION_TARGET '{PREDICTION_TARGET}' not supported for regression")

        # Calculate per-coin means for normalization
        coin_means = np.mean(original_targets, axis=0)
        
        # Initialize lists to store per-coin metrics
        coin_maes = []
        coin_mses = []
        coin_mapes = []
        coin_r2s = []
        
        # Per-coin regression metrics
        per_coin_metrics = {}
        for i, coin_name in enumerate(COIN_NAMES):
            coin_targets = original_targets[:, i]
            coin_preds = original_preds[:, i]
            coin_mean = coin_means[i]
            
            # Calculate normalized errors (divided by mean price)
            norm_targets = coin_targets / coin_mean
            norm_preds = coin_preds / coin_mean
            
            # Calculate metrics
            mae = mean_absolute_error(norm_targets, norm_preds)  # Normalized MAE
            mse = mean_squared_error(norm_targets, norm_preds)   # Normalized MSE
            rmse = np.sqrt(mse)                                  # Normalized RMSE
            mape = mean_absolute_percentage_error(coin_targets, coin_preds)  # MAPE (already relative)
            r2 = r2_score(coin_targets, coin_preds)             # RÂ² (already scale-invariant)
            
            # Store metrics for averaging
            coin_maes.append(mae)
            coin_mses.append(mse)
            coin_mapes.append(mape)
            coin_r2s.append(r2)
            
            # Store per-coin metrics
            per_coin_metrics[coin_name] = {
                'normalized_mae': mae,
                'normalized_mse': mse,
                'normalized_rmse': rmse,
                'mape': mape,
                'r2': r2,
                # Also store original metrics for reference
                'mae': mean_absolute_error(coin_targets, coin_preds),
                'mse': mean_squared_error(coin_targets, coin_preds),
                'rmse': np.sqrt(mean_squared_error(coin_targets, coin_preds))
            }
        
        # Calculate RD: |1 - sum of all predicted values / sum of all true values|
        total_true_sum = np.sum(original_targets)
        total_pred_sum = np.sum(original_preds)
        rd = abs(1 - total_pred_sum / total_true_sum) if total_pred_sum != 0 else float('inf')

        # Calculate both original and normalized overall metrics
        metrics.update({
            # Original metrics (calculated directly on all data)
            'mae': mean_absolute_error(original_targets, original_preds),  # åŸæ¥çš„MAEè®¡ç®—æ–¹å¼
            'rd': rd,  # RDæŒ‡æ ‡ï¼š|1-é¢„æµ‹å€¼æ€»å’Œ/çœŸå®å€¼æ€»å’Œ|
            'mse': mean_squared_error(original_targets, original_preds),
            'rmse': np.sqrt(mean_squared_error(original_targets, original_preds)),
            'r2': r2_score(original_targets, original_preds),
            'mape': mean_absolute_percentage_error(original_targets, original_preds),

            # Normalized metrics (average of per-coin metrics)
            'normalized_mae': np.mean(coin_maes),      # Average of normalized MAEs
            'normalized_mse': np.mean(coin_mses),      # Average of normalized MSEs
            'normalized_rmse': np.sqrt(np.mean(coin_mses)),  # RMSE of normalized errors
            'avg_mape': np.mean(coin_mapes),          # Average MAPE
            'avg_r2': np.mean(coin_r2s),             # Average RÂ²
            'median_mape': np.median(coin_mapes),     # Median MAPE
            'worst_mape': np.max(coin_mapes),         # Worst MAPE
            'best_mape': np.min(coin_mapes)           # Best MAPE
        })
        
        metrics['per_coin_metrics'] = per_coin_metrics

    return metrics, all_preds, all_targets

if __name__ == '__main__':
    # è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
    set_random_seeds(RANDOM_SEED)
    print(f"ğŸ¯ è®¾ç½®éšæœºç§å­: {RANDOM_SEED}")
    print(f"Using device: {DEVICE}")
    os.makedirs(CACHE_DIR, exist_ok=True)

    # 1. Load Data
    print("ğŸ“Š åŠ è½½ä»·æ ¼æ•°æ®...")
    price_df_raw = pd.read_csv(PRICE_CSV_PATH, index_col=0, parse_dates=True)
    rename_map = {f"{coin}-USDT": coin for coin in COIN_NAMES}
    price_df_full = price_df_raw.rename(columns=rename_map)[COIN_NAMES]

    # ç¡®ä¿æ—¶é—´ç´¢å¼•æ˜¯å‡åºæ’åˆ—ï¼ˆä»æ—©åˆ°æ™šï¼‰
    if not price_df_full.index.is_monotonic_increasing:
        print(f"âš ï¸ ä»·æ ¼æ•°æ®æ—¶é—´ç´¢å¼•ä¸æ˜¯å‡åºï¼Œæ­£åœ¨æ’åº...")
        price_df_full = price_df_full.sort_index()
        print(f"âœ… ä»·æ ¼æ•°æ®å·²æŒ‰æ—¶é—´å‡åºæ’åˆ—")
    
    # 2. Generate Edge Index with Advanced Methods
    if USE_GCN:
        print(f"ğŸ”— æ„å»ºå›¾ç»“æ„ï¼Œä½¿ç”¨æ–¹æ³•: {GRAPH_METHOD}")

        if GRAPH_METHOD == 'original':
            # åŸå§‹æ–¹æ³•ï¼šåŸºäºç›¸å…³æ€§æ„å»ºå›¾ï¼Œç°åœ¨ä¹Ÿæ”¯æŒè¾¹æƒé‡
            edge_index, edge_weights = generate_edge_index(
                price_df_full,
                return_weights=True,  # è¯·æ±‚è¿”å›è¾¹æƒé‡
                **GRAPH_PARAMS[GRAPH_METHOD]
            )
            # å°†å›¾æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
            edge_index = edge_index.to(DEVICE)
            edge_weights = edge_weights.to(DEVICE) if edge_weights is not None else None
        else:
            # é«˜çº§æ–¹æ³•ï¼šä½¿ç”¨æ›´å¤æ‚çš„å›¾æ„å»ºç®—æ³•
            edge_index, edge_weights = generate_advanced_edge_index(
                price_df_full,                    # ä»·æ ¼æ•°æ®
                method=GRAPH_METHOD,              # å›¾æ„å»ºæ–¹æ³•
                **GRAPH_PARAMS[GRAPH_METHOD]      # æ–¹æ³•ç‰¹å®šå‚æ•°
            )
            # å°†å›¾æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
            edge_index = edge_index.to(DEVICE)
            edge_weights = edge_weights.to(DEVICE) if edge_weights is not None else None

        # åˆ†æå¹¶æ‰“å°å›¾çš„å±æ€§ï¼ˆè¿æ¥æ•°ã€å¯†åº¦ç­‰ï¼‰
        graph_properties = analyze_graph_properties(edge_index, edge_weights, len(COIN_NAMES))
        print(f"ğŸ“ˆ å›¾å±æ€§åˆ†æ:")
        for key, value in graph_properties.items():
            # æ ¼å¼åŒ–è¾“å‡ºï¼šæµ®ç‚¹æ•°ä¿ç•™4ä½å°æ•°ï¼Œå…¶ä»–ç›´æ¥è¾“å‡º
            print(f"  {key}: {value:.4f}" if isinstance(value, float) else f"  {key}: {value}")
    else:
        # ä¸ä½¿ç”¨GCNæ—¶ï¼Œå›¾ç›¸å…³å˜é‡è®¾ä¸ºNone
        print("ğŸš« æœªå¯ç”¨GCNï¼Œè·³è¿‡å›¾æ„å»º")
        edge_index = None
        edge_weights = None

    # 3. Data preprocessing and normalization
    print(f"ğŸ”„ æ•°æ®é¢„å¤„ç†: target={PREDICTION_TARGET}, norm={NORM_TYPE}")

    if PREDICTION_TARGET == 'diff':
        df_to_scale = price_df_full.diff().dropna()
    elif PREDICTION_TARGET == 'return':
        df_to_scale = price_df_full.pct_change().dropna()
    else:  # 'price'
        df_to_scale = price_df_full

    # é€‰æ‹©å½’ä¸€åŒ–å™¨ï¼ˆå·®åˆ†/å˜åŒ–ç‡æ•°æ®éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
    if NORM_TYPE == 'none':
        scaler = None
    elif PREDICTION_TARGET in ('diff', 'return'):
        # å¯¹äºå·®åˆ†/å˜åŒ–ç‡ï¼Œå½’ä¸€åŒ–å¯èƒ½ç ´åæ­£è´Ÿåˆ†å¸ƒ
        print(f"âš ï¸  å·®åˆ†/å˜åŒ–ç‡æ•°æ®ä½¿ç”¨å½’ä¸€åŒ–ï¼Œè¯·æ³¨æ„æ­£è´Ÿæ ·æœ¬åˆ†å¸ƒ")
        if NORM_TYPE == 'standard':
            scaler = StandardScaler()
        elif NORM_TYPE == 'minmax':
            print(f"âš ï¸  MinMaxå½’ä¸€åŒ–å¯èƒ½ä¸é€‚åˆå·®åˆ†æ•°æ®ï¼Œå»ºè®®ä½¿ç”¨ 'standard' æˆ– 'none'")
            scaler = MinMaxScaler()
        else:
            scaler = None
    else:
        # ä»·æ ¼æ•°æ®æ­£å¸¸å½’ä¸€åŒ–
        scaler = StandardScaler() if NORM_TYPE == 'standard' else MinMaxScaler() if NORM_TYPE == 'minmax' else None

    # æ‰§è¡Œå½’ä¸€åŒ–ï¼ˆå¦‚é€‰æ‹©äº†å½’ä¸€åŒ–å™¨ï¼‰
    if scaler:
        price_df_values = scaler.fit_transform(df_to_scale)
        price_df = pd.DataFrame(price_df_values, columns=df_to_scale.columns, index=df_to_scale.index)
        print(f"âœ… æ•°æ®å½’ä¸€åŒ–å®Œæˆï¼Œæ–¹æ³•: {NORM_TYPE}")

        # æ£€æŸ¥å·®åˆ†æ•°æ®å½’ä¸€åŒ–åçš„åˆ†å¸ƒ
        if PREDICTION_TARGET in ('diff', 'return'):
            pos_ratio = (price_df > 0).sum().sum() / price_df.size
            print(f"ğŸ” å½’ä¸€åŒ–åæ­£å€¼æ¯”ä¾‹: {pos_ratio:.1%}")
            if pos_ratio < 0.1 or pos_ratio > 0.9:
                print(f"âš ï¸  æ­£è´Ÿæ ·æœ¬ä¸¥é‡ä¸å¹³è¡¡ï¼å»ºè®®ä½¿ç”¨ NORM_TYPE='none'")
    else:
        price_df = df_to_scale
        print(f"âœ… è·³è¿‡å½’ä¸€åŒ–ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")

    # 4. Create Dataset and DataLoaders
    print("ğŸ“° åŠ è½½æ–°é—»æ•°æ®..." if USE_NEWS_FEATURES else "ğŸš« è·³è¿‡æ–°é—»æ•°æ®åŠ è½½")
    news_data_dict = load_news_data(NEWS_FEATURES_FOLDER, COIN_NAMES) if USE_NEWS_FEATURES else None

    # è°ƒè¯•ï¼šæ£€æŸ¥æ–°é—»æ•°æ®åŠ è½½æƒ…å†µ
    if USE_NEWS_FEATURES:
        if news_data_dict:
            print(f"ğŸ” æ–°é—»æ•°æ®åŠ è½½æ£€æŸ¥:")
            for coin, data in news_data_dict.items():
                if data and 'news' in data:
                    print(f"  {coin}: {len(data['news'])} æ¡æ–°é—»")
                else:
                    print(f"  {coin}: æ— æ–°é—»æ•°æ®")
        else:
            print(f"âŒ æ–°é—»æ•°æ®å­—å…¸ä¸ºç©ºï¼")
            print(f"ğŸ“ æ£€æŸ¥æ–°é—»æ–‡ä»¶å¤¹: {NEWS_FEATURES_FOLDER}")
            if os.path.exists(NEWS_FEATURES_FOLDER):
                files = os.listdir(NEWS_FEATURES_FOLDER)
                print(f"  æ–‡ä»¶å¤¹å†…å®¹: {files}")
            else:
                print(f"  æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼")
    
    if USE_NEWS_FEATURES:
        processed_news_path = os.path.join(CACHE_DIR, "news_features", "all_processed_news_feature_new10days.pt")
    else:
        processed_news_path = None
        FORCE_RECOMPUTE_NEWS = False

    dataset = UnifiedCryptoDataset(
        price_data_df=price_df,
        news_data_dict=news_data_dict,
        seq_len=PRICE_SEQ_LEN,
        processed_news_features_path=processed_news_path,
        force_recompute_news=FORCE_RECOMPUTE_NEWS,
        time_encoding_enabled=TIME_ENCODING_ENABLED_IN_DATASET,
        time_freq=TIME_FREQ_IN_DATASET,
    )

    # ä½¿ç”¨æ—¶é—´åºåˆ—æ­£ç¡®çš„åˆ’åˆ†æ–¹å¼ï¼Œé¿å…æ•°æ®æ³„éœ²
    total_size = len(dataset)
    test_size = int(TEST_SPLIT_RATIO * total_size)
    val_size = int(VALIDATION_SPLIT_RATIO * total_size)
    train_size = total_size - test_size - val_size

    print(f"ğŸ“Š æ—¶é—´åºåˆ—æ•°æ®åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: 0 åˆ° {train_size-1} ({train_size} æ ·æœ¬)")
    print(f"  éªŒè¯é›†: {train_size} åˆ° {train_size+val_size-1} ({val_size} æ ·æœ¬)")
    print(f"  æµ‹è¯•é›†: {train_size+val_size} åˆ° {total_size-1} ({test_size} æ ·æœ¬)")

    # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†ï¼Œé¿å…éšæœºåˆ’åˆ†å¯¼è‡´çš„æ•°æ®æ³„éœ²
    train_indices = list(range(0, train_size))
    val_indices = list(range(train_size, train_size + val_size))
    test_indices = list(range(train_size + val_size, total_size))

    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)
    test_dataset = Subset(dataset, test_indices)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # 5. Initialize Model
    timemixer_configs = TimeMixerBaseConfigs(
        num_nodes=dataset.num_coins,
        price_seq_len=PRICE_SEQ_LEN,
        num_time_features=dataset.num_actual_time_features,
        task_type=TASK_TYPE,
        d_model=D_MODEL,
        e_layers=E_LAYERS,
        n_heads=N_HEADS,
        d_ff=D_FF,
        dropout=DROPOUT_TIMEMIXER,
        freq=TIME_FREQ_IN_DATASET,
        use_norm=USE_NORM,
        down_sampling_layers=DOWN_SAMPLING_LAYERS,
        down_sampling_window=DOWN_SAMPLING_WINDOW,
        down_sampling_method=DOWN_SAMPLING_METHOD,
        channel_independence=CHANNEL_INDEPENDENCE
    )

    print(f"ğŸš€ åˆå§‹åŒ–TimeMixeræ¨¡å‹ï¼ŒGCNé…ç½®: {GCN_CONFIG}")
    model = UnifiedMultiScaleTimeMixer(
        configs=timemixer_configs,
        use_gcn=USE_GCN,
        gcn_config=GCN_CONFIG,  # æ–°å¢ï¼šä¼ é€’GCNé…ç½®
        news_feature_dim=dataset.news_feature_dim if USE_NEWS_FEATURES else None,
        gcn_hidden_dim=GCN_HIDDEN_DIM,
        gcn_output_dim=GCN_OUTPUT_DIM,
        news_processed_dim=NEWS_PROCESSED_DIM,
        prediction_head_dim=PREDICTION_HEAD_DIM,
        mlp_hidden_dim=MLP_HIDDEN_DIM,
        num_classes=NUM_CLASSES
    ).to(DEVICE)
    print(model)
    print(f"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # 6. Setup Loss, Optimizer
    criterion = nn.CrossEntropyLoss() if TASK_TYPE == 'classification' else nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 'min', patience=10, factor=0.8, min_lr=1e-7
    )
    print(f"ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®: patience=10, factor=0.8, min_lr=1e-7")
    
    # 7. Training Loop
    if TASK_TYPE == 'classification':
        best_val_metric = float('-inf')   # åˆ†ç±»ä»»åŠ¡ï¼šF1åˆ†æ•°è¶Šå¤§è¶Šå¥½ï¼ˆä½¿ç”¨è´Ÿå€¼ï¼Œåˆå§‹åŒ–ä¸ºè´Ÿæ— ç©·å¤§ï¼‰
    else:
        best_val_metric = float('inf')    # å›å½’ä»»åŠ¡ï¼šæŸå¤±è¶Šå°è¶Šå¥½ï¼ˆåˆå§‹åŒ–ä¸ºæ­£æ— ç©·å¤§ï¼‰
    patience_counter = 0
    for epoch in range(EPOCHS):
        model.train()
        epoch_loss = 0.0
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} Training")
        for batch_data in train_pbar:
            price_seq = batch_data['price_seq'].to(DEVICE)
            target_data = batch_data['target_price'].to(DEVICE)
            x_mark_enc = batch_data.get('price_seq_mark')
            if x_mark_enc is not None: x_mark_enc = x_mark_enc.to(DEVICE)
            news_features = batch_data.get('news_features')
            if news_features is not None: news_features = news_features.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(price_seq, x_mark_enc, edge_index=edge_index, edge_weight=edge_weights, news_features=news_features)
            
            if TASK_TYPE == 'classification':
                # === åˆ†ç±»ä»»åŠ¡ï¼šé¢„æµ‹ä»·æ ¼æ¶¨è·Œæ–¹å‘ ===
                if PREDICTION_TARGET in ('diff', 'return'):
                    # diff/returnï¼štarget å·²æ˜¯å·®åˆ†/æ”¶ç›Šç‡ï¼Œç›´æ¥åˆ¤æ–­æ­£è´Ÿ
                    targets = (target_data > 0).long()
                else:
                    raise ValueError(f"PREDICTION_TARGET '{PREDICTION_TARGET}' not supported for classification")

                # ç¡®ä¿è¾“å‡ºå’Œç›®æ ‡çš„å½¢çŠ¶åŒ¹é…
                if len(outputs.shape) == 3:  # [batch_size, num_nodes, num_classes]
                    outputs_flat = outputs.view(-1, NUM_CLASSES)
                    targets_flat = targets.view(-1)
                elif len(outputs.shape) == 2:  # [batch_size * num_nodes, num_classes]
                    outputs_flat = outputs
                    targets_flat = targets.view(-1)
                else:
                    raise ValueError(f"Unexpected output shape: {outputs.shape}")

                loss = criterion(outputs_flat, targets_flat)
            else:
                if PREDICTION_TARGET == 'price':
                    targets = target_data
                else:
                    raise ValueError(f"PREDICTION_TARGET '{PREDICTION_TARGET}' not supported for regression")
                loss = criterion(outputs, targets)
            
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            epoch_loss += loss.item() * price_seq.size(0)

        avg_train_loss = epoch_loss / len(train_dataset)
        val_metrics, _, _ = evaluate_model(model, val_loader, criterion, edge_index, DEVICE, TASK_TYPE, scaler)
        
        # è·å–ç”¨äºå­¦ä¹ ç‡è°ƒåº¦çš„éªŒè¯æŒ‡æ ‡
        if TASK_TYPE == 'classification':
            # åˆ†ç±»ä»»åŠ¡ä½¿ç”¨F1åˆ†æ•°ï¼ˆè¶Šå¤§è¶Šå¥½ï¼Œéœ€è¦å–è´Ÿå€¼ç”¨äºæ—©åœï¼‰
            val_metric_for_scheduler = -val_metrics.get('f1', 0)  # å–è´Ÿå€¼ï¼Œå› ä¸ºæ—©åœæœºåˆ¶æ˜¯åŸºäº"è¶Šå°è¶Šå¥½"
        else:
            # å›å½’ä»»åŠ¡ä½¿ç”¨æŸå¤±ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            val_metric_for_scheduler = val_metrics['loss']
        
        # æ ¹æ®éªŒè¯æŒ‡æ ‡è°ƒæ•´å­¦ä¹ ç‡
        if TASK_TYPE == 'classification':
            # åˆ†ç±»ä»»åŠ¡ï¼šä½¿ç”¨F1åˆ†æ•°ï¼ˆä¼ å…¥æ­£å€¼ç»™è°ƒåº¦å™¨ï¼‰
            scheduler.step(-val_metric_for_scheduler)
        else:
            # å›å½’ä»»åŠ¡ï¼šä½¿ç”¨æŸå¤±
            scheduler.step(val_metric_for_scheduler)

        current_lr = optimizer.param_groups[0]['lr']
        print(f"\nEpoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | LR: {current_lr:.6f}")

        # æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡å°
        if current_lr < 1e-6:
            print(f"âš ï¸ å­¦ä¹ ç‡è¿‡å° ({current_lr:.2e})ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ")
        print("--- Validation Metrics (Overall) ---")
        for name, value in val_metrics.items():
            if not isinstance(value, dict):
                if isinstance(value, (int, float)):
                    # ä¸ºä¸åŒæŒ‡æ ‡æ·»åŠ ä¸­æ–‡æ³¨é‡Š
                    if name == 'accuracy':
                        print(f"  - {name.upper()}: {value:.4f}  # æ•´ä½“å‡†ç¡®ç‡ - é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ¯”ä¾‹")
                    elif name == 'precision':
                        print(f"  - {name.upper()}: {value:.4f}  # æ•´ä½“ç²¾ç¡®ç‡ - é¢„æµ‹ä¸ºæ¶¨çš„æ ·æœ¬ä¸­å®é™…ä¸Šæ¶¨çš„æ¯”ä¾‹")
                    elif name == 'recall':
                        print(f"  - {name.upper()}: {value:.4f}  # æ•´ä½“å¬å›ç‡ - å®é™…ä¸Šæ¶¨çš„æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹")
                    elif name == 'f1_score':
                        print(f"  - {name.upper()}: {value:.4f}  # æ•´ä½“F1åˆ†æ•° - ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡")
                    elif name == 'loss':
                        print(f"  - {name.upper()}: {value:.4f}  # éªŒè¯æŸå¤± - æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æŸå¤±å€¼")
                    else:
                        print(f"  - {name.upper()}: {value:.4f}")
                elif isinstance(value, list):
                    if name == 'confusion_matrix':
                        print(f"  - {name.upper()}: {value}  # æ··æ·†çŸ©é˜µ - [[çœŸè´Ÿä¾‹,å‡æ­£ä¾‹],[å‡è´Ÿä¾‹,çœŸæ­£ä¾‹]]")
                    else:
                        print(f"  - {name.upper()}: {value}")
                else:
                    print(f"  - {name.upper()}: {value}")

        # æ—©åœå’Œæ¨¡å‹ä¿å­˜é€»è¾‘
        if val_metric_for_scheduler < best_val_metric - MIN_DELTA:
            best_val_metric = val_metric_for_scheduler
            patience_counter = 0
            torch.save(model.state_dict(), BEST_MODEL_PATH)
            if TASK_TYPE == 'classification':
                print(f"ğŸš€ ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹åˆ° {BEST_MODEL_PATH} (éªŒè¯F1: {-best_val_metric:.4f})")
            else:
                print(f"ğŸš€ ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹åˆ° {BEST_MODEL_PATH} (éªŒè¯æŸå¤±: {best_val_metric:.4f})")
        else:
            patience_counter += 1
            if TASK_TYPE == 'classification':
                print(f"â³ è¿ç»­ {patience_counter} ä¸ªepochæ— æ”¹å–„ (æœ€ä½³F1: {-best_val_metric:.4f})")
            else:
                print(f"â³ è¿ç»­ {patience_counter} ä¸ªepochæ— æ”¹å–„ (æœ€ä½³æŸå¤±: {best_val_metric:.4f})")

        # æ—©åœæ£€æŸ¥
        if patience_counter >= EARLY_STOPPING_PATIENCE:
            print(f"ğŸ›‘ Early stopping triggered after {epoch+1} epochs (patience: {EARLY_STOPPING_PATIENCE})")
            break

    # 8. Testing
    print("\n--- Starting Testing with Best Model ---")
    if os.path.exists(BEST_MODEL_PATH):
        model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE, weights_only=True))
    else:
        print("Warning: Best model not found. Testing with the last state.")

    test_metrics, test_preds, test_targets = evaluate_model(model, test_loader, criterion, edge_index, DEVICE, TASK_TYPE, scaler)

    # ä¿å­˜æµ‹è¯•é›†é¢„æµ‹ç»“æœåˆ°æ–°æ–‡ä»¶å¤¹
    if TASK_TYPE == 'regression':
        # å¯¹äºå›å½’ä»»åŠ¡ï¼Œéœ€è¦åå½’ä¸€åŒ–é¢„æµ‹å€¼å’ŒçœŸå®å€¼ç”¨äºä¿å­˜
        if PREDICTION_TARGET == 'price' and scaler:
            original_test_preds = scaler.inverse_transform(test_preds)
            original_test_targets = scaler.inverse_transform(test_targets)
        else:
            original_test_preds = test_preds
            original_test_targets = test_targets

        save_test_predictions(original_test_preds, original_test_targets, COIN_NAMES, test_metrics=test_metrics)

    elif TASK_TYPE == 'classification':
        # å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨ä¸“ç”¨çš„ä¿å­˜å‡½æ•°
        save_classification_results(test_preds, test_targets, COIN_NAMES, model_variant_str, test_metrics)

    print(f"\nâœ… Test Results:")
    print("  Overall:")
    for name, value in test_metrics.items():
        if not isinstance(value, dict):
            if isinstance(value, (int, float)):
                # ä¸ºä¸åŒæŒ‡æ ‡æ·»åŠ ä¸­æ–‡æ³¨é‡Š
                if name == 'accuracy':
                    print(f"    - {name.upper()}: {value:.4f}  # æ•´ä½“å‡†ç¡®ç‡ - é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ¯”ä¾‹")
                elif name == 'precision':
                    print(f"    - {name.upper()}: {value:.4f}  # æ•´ä½“ç²¾ç¡®ç‡ - é¢„æµ‹ä¸ºæ¶¨çš„æ ·æœ¬ä¸­å®é™…ä¸Šæ¶¨çš„æ¯”ä¾‹")
                elif name == 'recall':
                    print(f"    - {name.upper()}: {value:.4f}  # æ•´ä½“å¬å›ç‡ - å®é™…ä¸Šæ¶¨çš„æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹")
                elif name == 'f1_score':
                    print(f"    - {name.upper()}: {value:.4f}  # æ•´ä½“F1åˆ†æ•° - ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡")
                elif name == 'loss':
                    print(f"    - {name.upper()}: {value:.4f}  # æµ‹è¯•æŸå¤± - æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æŸå¤±å€¼")
                elif name == 'mae':
                    print(f"    - {name.upper()}: {value:.4f}  # å¹³å‡ç»å¯¹è¯¯å·® - é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å¹³å‡ç»å¯¹å·®")
                elif name == 'mse':
                    print(f"    - {name.upper()}: {value:.4f}  # å‡æ–¹è¯¯å·® - é¢„æµ‹å€¼ä¸çœŸå®å€¼å·®çš„å¹³æ–¹çš„å¹³å‡")
                elif name == 'rmse':
                    print(f"    - {name.upper()}: {value:.4f}  # å‡æ–¹æ ¹è¯¯å·® - MSEçš„å¹³æ–¹æ ¹")
                elif name == 'r2':
                    print(f"    - {name.upper()}: {value:.4f}  # å†³å®šç³»æ•° - æ¨¡å‹è§£é‡Šæ•°æ®å˜å¼‚æ€§çš„æ¯”ä¾‹")
                else:
                    print(f"    - {name.upper()}: {value:.4f}")
            elif isinstance(value, list):
                if name == 'confusion_matrix':
                    print(f"    - {name.upper()}: {value}  # æ··æ·†çŸ©é˜µ - [[çœŸè´Ÿä¾‹,å‡æ­£ä¾‹],[å‡è´Ÿä¾‹,çœŸæ­£ä¾‹]]")
                else:
                    print(f"    - {name.upper()}: {value}")
            else:
                print(f"    - {name.upper()}: {value}")

    if 'per_coin_metrics' in test_metrics:
        print(f"\n  --- å„å¸ç§è¯¦ç»†æŒ‡æ ‡ (æµ‹è¯•é›†) ---")
        for coin_name, coin_metrics in test_metrics['per_coin_metrics'].items():
            print(f"  ğŸª™ {coin_name}:")
            for metric_name, value in coin_metrics.items():
                if isinstance(value, (int, float)):
                    # ä¸ºæ¯ä¸ªå¸ç§çš„æŒ‡æ ‡æ·»åŠ æ³¨é‡Š
                    if metric_name == 'accuracy':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„é¢„æµ‹å‡†ç¡®ç‡")
                    elif metric_name == 'precision':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„é¢„æµ‹ç²¾ç¡®ç‡")
                    elif metric_name == 'recall':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„é¢„æµ‹å¬å›ç‡")
                    elif metric_name == 'f1_score':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„F1åˆ†æ•°")
                    elif metric_name == 'mae':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„å¹³å‡ç»å¯¹è¯¯å·®")
                    elif metric_name == 'mse':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„å‡æ–¹è¯¯å·®")
                    elif metric_name == 'rmse':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„å‡æ–¹æ ¹è¯¯å·®")
                    elif metric_name == 'r2':
                        print(f"    - {metric_name.upper()}: {value:.4f}  # {coin_name}çš„å†³å®šç³»æ•°")
                    else:
                        print(f"    - {metric_name.upper()}: {value:.4f}")
                else:
                    print(f"    - {metric_name.upper()}: {value}")
    print("\n--- Script Finished ---") 