from bisect import bisect_left
import torch
import pandas as pd
from torch.utils.data import Dataset
from datetime import datetime, timedelta
import json
import os
import numpy as np

def time_features(dates, freq='h'):
    """
    Extract time features from a pd.DatetimeIndex.
    `dates` should be a pd.DatetimeIndex.
    `freq` indicates the frequency of the data.
    Returns a numpy array of shape (len(dates), num_features).
    """
    if isinstance(dates, np.ndarray) and np.issubdtype(dates.dtype, np.datetime64):
        dates = pd.to_datetime(dates) 
    elif not isinstance(dates, pd.DatetimeIndex):
        if isinstance(dates, pd.Timestamp):
            dates = pd.DatetimeIndex([dates])
        else:
            raise TypeError("Input `dates` must be a pd.DatetimeIndex, pd.Timestamp, or numpy array of datetime64.")

    features = []
    if freq == 'h':
        features.append(dates.month / 12.0 - 0.5)
        features.append(dates.day / 31.0 - 0.5)
        features.append(dates.dayofweek / 6.0 - 0.5)
        features.append(dates.dayofyear / 365.0 - 0.5)
        features.append(dates.hour / 23.0 - 0.5)
        features.append(dates.isocalendar().week.values.astype(float) / 52.0 - 0.5)
    elif freq == 'd':
        features.append(dates.month / 12.0 - 0.5)
        features.append(dates.day / 31.0 - 0.5)
        features.append(dates.dayofweek / 6.0 - 0.5)
        features.append(dates.dayofyear / 365.0 - 0.5)
        features.append(dates.isocalendar().week.values.astype(float) / 52.0 - 0.5)
    elif freq == 'w':
        features.append(dates.month / 12.0 - 0.5)
        features.append(dates.isocalendar().week.values.astype(float) / 52.0 - 0.5)
    elif freq == 'm':
        features.append(dates.month / 12.0 - 0.5)
    else:
        raise ValueError(f"Unsupported freq: {freq}")
    return np.stack(features, axis=1).astype(np.float32)

class UnifiedCryptoDataset(Dataset):
    def __init__(self, price_data_df, news_data_dict=None, seq_len=24, pred_len=1,
                 processed_news_features_path="processed_news_features.pt", force_recompute_news=False,
                 time_encoding_enabled: bool = True,
                 time_freq: str = 'h',
                 predict_mode: bool = False):
        """
        Args:
            price_data_df: DataFrame, contains price data for all coins, indexed by timestamp, with columns as coin names.
            news_data_dict: Dict[str, List[Dict]] or None. List of news for each coin. If None, news features are not used.
            seq_len: Sequence length, the number of time steps for model input.
            processed_news_features_path: Path to save/load preprocessed news features (used only if news_data_dict is not None).
            force_recompute_news: Whether to force re-computation of news features (used only if news_data_dict is not None).
            time_encoding_enabled: bool, whether to enable time feature encoding.
            time_freq: str, time frequency (e.g., 'h' for hourly), for the time_features function.
            predict_mode: bool. If True, the dataset includes the last possible sequence, which has a dummy target. Useful for inference.
        """
        super().__init__()
        print(f"[UnifiedCryptoDataset] Reading price data: shape={price_data_df.shape}, columns={list(price_data_df.columns)}")
        print(f"[UnifiedCryptoDataset] News data: {'enabled' if news_data_dict is not None else 'disabled'}")
        self.price_data_df = price_data_df
        self.news_data_dict = news_data_dict
        self.seq_len = seq_len
        self.pred_len = pred_len # Store pred_len
        self.time_index = pd.to_datetime(self.price_data_df.index)
        self.time_encoding_enabled = time_encoding_enabled
        self.time_freq = time_freq
        self.predict_mode = predict_mode
        self.has_news = self.news_data_dict is not None

        # æ£€æŸ¥æ—¶é—´ç´¢å¼•é¡ºåº
        if len(self.time_index) > 1:
            time_ascending = self.time_index[0] < self.time_index[-1]
            print(f"[UnifiedCryptoDataset] æ—¶é—´ç´¢å¼•é¡ºåº: {'å‡åº(æ—©â†’æ™š)' if time_ascending else 'é™åº(æ™šâ†’æ—©)'}")
            print(f"[UnifiedCryptoDataset] æ—¶é—´èŒƒå›´: {self.time_index[0]} åˆ° {self.time_index[-1]}")

            if not time_ascending:
                print(f"âš ï¸ [UnifiedCryptoDataset] æ£€æµ‹åˆ°é™åºæ—¶é—´ç´¢å¼•ï¼Œæ–°é—»ç‰¹å¾å°†æŒ‰æ­¤é¡ºåºå¯¹é½")

        self.coin_names = list(price_data_df.columns)
        self.coin_to_idx = {name: i for i, name in enumerate(self.coin_names)}
        self.num_coins = len(self.coin_names)
        
        if self.has_news:
            # --- æ–°é—»ç‰¹å¾å¤„ç† ---
            # å®šä¹‰æ–°é—»ç‰¹å¾çš„æ€»ç»´åº¦ (3ä¸ªåµŒå…¥å‘é‡ + 3ä¸ªç›¸ä¼¼åº¦åˆ†æ•° + 6ä¸ªç»Ÿè®¡ç‰¹å¾ + 2ä¸ªæƒ…ç»ª/çŠ¶æ€å€¼)
            self.news_feature_dim = 768 * 3 + 3 + 6 + 2
            self.processed_news_features_path = processed_news_features_path
            self.force_recompute_news = force_recompute_news
            self.processed_news_features = None  # åˆå§‹åŒ–é¢„å¤„ç†æ–°é—»ç‰¹å¾ä¸ºç©º

            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é¢„å¤„ç†å¥½çš„æ–°é—»ç‰¹å¾æ–‡ä»¶ï¼Œå¹¶ä¸”ä¸å¼ºåˆ¶é‡æ–°è®¡ç®—
            if not self.force_recompute_news and self.processed_news_features_path and os.path.exists(self.processed_news_features_path):
                try:
                    # å°è¯•ä»Žç¼“å­˜æ–‡ä»¶åŠ è½½é¢„å¤„ç†çš„æ–°é—»ç‰¹å¾
                    self.processed_news_features = torch.load(self.processed_news_features_path, weights_only=True)
                    print(f"ðŸ“° åŠ è½½ç¼“å­˜æ–°é—»ç‰¹å¾: {self.processed_news_features.shape}")

                    # åˆ†æžæ¯ä¸ªå¸ç§çš„ç‰¹å¾è¦†ç›–çŽ‡
                    print(f"ðŸ” å„å¸ç§æ–°é—»ç‰¹å¾è¦†ç›–çŽ‡åˆ†æž:")
                    for i, coin_name in enumerate(self.coin_names):
                        if i < self.processed_news_features.shape[1]:
                            coin_features = self.processed_news_features[:, i, :]  # [time, features]

                            # è®¡ç®—è¦†ç›–çŽ‡ç»Ÿè®¡
                            total_timepoints = coin_features.shape[0]
                            total_features = coin_features.shape[1]

                            # æ—¶é—´ç»´åº¦è¦†ç›–çŽ‡ï¼šæœ‰å¤šå°‘æ—¶é—´ç‚¹æœ‰éžé›¶ç‰¹å¾
                            time_norms = torch.norm(coin_features, dim=1)  # æ¯ä¸ªæ—¶é—´ç‚¹çš„ç‰¹å¾èŒƒæ•°
                            active_timepoints = (time_norms > 0).sum().item()
                            time_coverage = active_timepoints / total_timepoints

                            # ç‰¹å¾ç»´åº¦è¦†ç›–çŽ‡ï¼šæœ‰å¤šå°‘ç‰¹å¾ç»´åº¦è¢«ä½¿ç”¨
                            feature_activity = (coin_features != 0).any(dim=0)  # æ¯ä¸ªç‰¹å¾ç»´åº¦æ˜¯å¦è¢«ä½¿ç”¨
                            active_features = feature_activity.sum().item()
                            feature_coverage = active_features / total_features

                            # æ•´ä½“éžé›¶æ¯”ä¾‹
                            nonzero_ratio = (coin_features != 0).float().mean().item()

                            # ç‰¹å¾å¼ºåº¦ç»Ÿè®¡
                            feature_mean = coin_features.mean().item()
                            feature_std = coin_features.std().item()
                            feature_norm = torch.norm(coin_features).item()

                            print(f"  {coin_name}:")
                            print(f"    æ—¶é—´è¦†ç›–çŽ‡: {time_coverage:.1%} ({active_timepoints}/{total_timepoints})")
                            print(f"    ç‰¹å¾è¦†ç›–çŽ‡: {feature_coverage:.1%} ({active_features}/{total_features})")
                            print(f"    éžé›¶æ¯”ä¾‹: {nonzero_ratio:.1%}")
                            print(f"    ç‰¹å¾å¼ºåº¦: å‡å€¼={feature_mean:.6f}, æ ‡å‡†å·®={feature_std:.6f}, èŒƒæ•°={feature_norm:.4f}")
                        else:
                            print(f"  {coin_name}: âŒ ç´¢å¼•è¶…å‡ºèŒƒå›´")

                    # ä¸åœ¨è¿™é‡ŒéªŒè¯å½¢çŠ¶ï¼Œè€Œæ˜¯åœ¨åŽé¢å°è¯•è‡ªåŠ¨å¯¹é½
                except Exception as e:
                    # å¦‚æžœåŠ è½½å¤±è´¥ï¼Œä¹Ÿç½®ä¸ºç©ºï¼ŒåŽç»­å°†é‡æ–°è®¡ç®—
                    print(f"âŒ åŠ è½½æ–°é—»ç‰¹å¾å¤±è´¥: {e}")
                    self.processed_news_features = None
            
            # å¦‚æžœé¢„å¤„ç†çš„æ–°é—»ç‰¹å¾ä»ç„¶ä¸ºç©ºï¼ˆå› ä¸ºæ–‡ä»¶ä¸å­˜åœ¨ã€åŠ è½½å¤±è´¥æˆ–å¼ºåˆ¶é‡æ–°è®¡ç®—ï¼‰
            if self.processed_news_features is None:
                print("ðŸ”„ é‡æ–°è®¡ç®—æ–°é—»ç‰¹å¾...")
                # è°ƒç”¨å†…éƒ¨æ–¹æ³•æ¥å‡†å¤‡/è®¡ç®—æ‰€æœ‰æ–°é—»ç‰¹å¾
                self.processed_news_features = self._prepare_all_news_features()
                # å¦‚æžœæŒ‡å®šäº†ä¿å­˜è·¯å¾„ï¼Œåˆ™å°†æ–°è®¡ç®—çš„ç‰¹å¾ä¿å­˜åˆ°æ–‡ä»¶ä»¥å¤‡åŽç”¨
                if self.processed_news_features_path:
                    save_dir = os.path.dirname(self.processed_news_features_path)
                    if save_dir and not os.path.exists(save_dir):
                        os.makedirs(save_dir)  # ç¡®ä¿ç›®å½•å­˜åœ¨
                    torch.save(self.processed_news_features, self.processed_news_features_path)
            else:
                # æ£€æŸ¥æ–°é—»ç‰¹å¾ä¸Žä»·æ ¼æ•°æ®çš„æ—¶é—´ç´¢å¼•æ˜¯å¦åŒ¹é…
                if self.processed_news_features.shape[0] != len(self.time_index):
                    print(f"âš ï¸  æ–°é—»ç‰¹å¾æ—¶é—´ç»´åº¦ ({self.processed_news_features.shape[0]}) ä¸Žä»·æ ¼æ•°æ® ({len(self.time_index)}) ä¸åŒ¹é…")
                    alignment_success = False

                    if self.processed_news_features.shape[0] == len(self.time_index) + 1:
                        # æ–°é—»ç‰¹å¾æ¯”ä»·æ ¼æ•°æ®å¤šä¸€ä¸ªæ—¶é—´ç‚¹ï¼ˆé€šå¸¸æ˜¯diff/pct_changeå¯¼è‡´çš„ï¼‰
                        print("ðŸ”§ è‡ªåŠ¨å¯¹é½ï¼šåˆ é™¤æ–°é—»ç‰¹å¾çš„ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹")
                        self.processed_news_features = self.processed_news_features[1:]
                        print(f"âœ… å¯¹é½æˆåŠŸï¼šæ–°é—»ç‰¹å¾å½¢çŠ¶ {self.processed_news_features.shape}")
                        alignment_success = True
                    elif self.processed_news_features.shape[0] == len(self.time_index) - 1:
                        # ä»·æ ¼æ•°æ®æ¯”æ–°é—»ç‰¹å¾å¤šä¸€ä¸ªæ—¶é—´ç‚¹
                        print("ðŸ”§ è‡ªåŠ¨å¯¹é½ï¼šåœ¨æ–°é—»ç‰¹å¾å¼€å¤´è¡¥é›¶")
                        padding = torch.zeros(1, self.num_coins, self.news_feature_dim, dtype=torch.float32)
                        self.processed_news_features = torch.cat([padding, self.processed_news_features], dim=0)
                        print(f"âœ… å¯¹é½æˆåŠŸï¼šæ–°é—»ç‰¹å¾å½¢çŠ¶ {self.processed_news_features.shape}")
                        alignment_success = True

                    if not alignment_success:
                        print(f"âŒ æ— æ³•è‡ªåŠ¨å¯¹é½ï¼Œæ—¶é—´ç»´åº¦å·®å¼‚è¿‡å¤§: {self.processed_news_features.shape[0]} vs {len(self.time_index)}")
                        print(f"ï¿½ å°è¯•é‡æ–°è®¡ç®—æ–°é—»ç‰¹å¾...")
                        # è‡ªåŠ¨å¯¹é½å¤±è´¥ï¼Œé‡æ–°è®¡ç®—æ–°é—»ç‰¹å¾
                        self.processed_news_features = self._prepare_all_news_features()
                        # ä¿å­˜é‡æ–°è®¡ç®—çš„ç‰¹å¾
                        if self.processed_news_features_path:
                            save_dir = os.path.dirname(self.processed_news_features_path)
                            if save_dir and not os.path.exists(save_dir):
                                os.makedirs(save_dir)
                            torch.save(self.processed_news_features, self.processed_news_features_path)
                            print(f"ðŸ’¾ å·²ä¿å­˜é‡æ–°è®¡ç®—çš„æ–°é—»ç‰¹å¾åˆ°: {self.processed_news_features_path}")
                else:
                    print(f"âœ… æ–°é—»ç‰¹å¾ä¸Žä»·æ ¼æ•°æ®æ—¶é—´ç»´åº¦åŒ¹é…: {self.processed_news_features.shape[0]}")
        else:
            # å¦‚æžœä¸ä½¿ç”¨æ–°é—»æ•°æ®
            self.news_feature_dim = 0  # å°†æ–°é—»ç‰¹å¾ç»´åº¦è®¾ä¸º0
            # åˆ›å»ºä¸€ä¸ªå½¢çŠ¶æ­£ç¡®ä½†ç¬¬ä¸‰ç»´ä¸º0çš„ç©ºå¼ é‡ï¼Œä»¥ä¿æŒæ•°æ®ç»“æž„å’Œç±»åž‹çš„ä¸€è‡´æ€§
            self.processed_news_features = torch.zeros(len(self.time_index), self.num_coins, 0, dtype=torch.float32)

        # --- æ—¶é—´ç‰¹å¾ç¼–ç  ---
        if self.time_encoding_enabled:
            # å¦‚æžœå¯ç”¨æ—¶é—´ç¼–ç ï¼Œåˆ™ä¸ºæ‰€æœ‰æ—¶é—´ç‚¹ç”Ÿæˆæ—¶é—´ç‰¹å¾
            encoded_stamps_np = time_features(self.time_index, freq=self.time_freq)
            self.all_time_stamps_encoded = torch.tensor(encoded_stamps_np, dtype=torch.float32)
        else:
            # å¦‚æžœä¸å¯ç”¨æ—¶é—´ç¼–ç ï¼Œåˆ›å»ºä¸€ä¸ªå…¨é›¶çš„å¼ é‡ä½œä¸ºå ä½ç¬¦
            # å…ˆç”¨ä¸€ä¸ªè™šæ‹Ÿæ—¥æœŸè®¡ç®—å‡ºæ—¶é—´ç‰¹å¾åº”æœ‰çš„ç»´åº¦
            _dummy_dates = pd.DatetimeIndex([pd.Timestamp('2000-01-01')])
            num_time_features_dim = time_features(_dummy_dates, freq=self.time_freq).shape[1]
            # åˆ›å»ºä¸€ä¸ªå½¢çŠ¶ä¸º (æ—¶é—´ç‚¹æ•°é‡, ç‰¹å¾ç»´åº¦) çš„å…¨é›¶å¼ é‡
            self.all_time_stamps_encoded = torch.zeros(len(self.time_index), num_time_features_dim, dtype=torch.float32)
        # è®°å½•æ¯ä¸ªæ—¶é—´ç‚¹å®žé™…æ‹¥æœ‰çš„æ—¶é—´ç‰¹å¾æ•°é‡ï¼ˆæ— è®ºæ˜¯çœŸå®žè®¡ç®—çš„è¿˜æ˜¯0ï¼‰
        self.num_actual_time_features = self.all_time_stamps_encoded.shape[1]

    def _extract_single_news_feature(self, news_item):
        try:
            title_emb = torch.tensor(news_item['title_embedding'][0], dtype=torch.float32)
            subtitle_emb = torch.tensor(news_item['subtitle_embedding'][0], dtype=torch.float32)
            body_emb = torch.tensor(news_item['body_embedding'][0], dtype=torch.float32)

            similarities = torch.tensor([
                news_item.get('title_subtitle_similarity', 0.0),
                news_item.get('title_body_similarity', 0.0), 
                news_item.get('subtitle_body_similarity', 0.0)
            ], dtype=torch.float32)

            stats = torch.tensor([
                news_item.get('title_length', 0), news_item.get('title_words', 0),
                news_item.get('subtitle_length', 0), news_item.get('subtitle_words', 0),
                news_item.get('body_length', 0), news_item.get('body_words', 0)
            ], dtype=torch.float32)
            
            sentiment_status = torch.tensor([
                news_item.get('sentiment_value', 0.0),
                news_item.get('status_value', 0.0)
            ], dtype=torch.float32)

            return torch.cat([title_emb, subtitle_emb, body_emb, similarities, stats, sentiment_status])
        except Exception:
            return torch.zeros(self.news_feature_dim, dtype=torch.float32)

    def _process_coin_news(self):
        coin_news_indices = {}
        for coin_name, news_list in self.news_data_dict.items():
            if coin_name not in self.coin_to_idx:
                continue

            valid_news_items = [item for item in news_list if pd.to_datetime(item.get('published_ts'), errors='coerce') is not pd.NaT]
            
            sorted_news = sorted(valid_news_items, key=lambda x: pd.to_datetime(x['published_ts']))
            times = [pd.to_datetime(n['published_ts']) for n in sorted_news]
            
            coin_news_indices[coin_name] = {'news': sorted_news, 'times': times, 'count': len(sorted_news)}
        return coin_news_indices

    def _prepare_all_news_features(self):
        T = len(self.time_index)
        all_features = torch.zeros(T, self.num_coins, self.news_feature_dim, dtype=torch.float32)
        coin_news_data = self._process_coin_news()
        
        default_params = {'validity_days': 10, 'decay_factor': 1.0}

        for t, timestamp in enumerate(self.time_index):
            for coin_name in self.coin_names:
                coin_idx = self.coin_to_idx[coin_name]
                
                if coin_name not in coin_news_data or not coin_news_data[coin_name]['times']:
                    continue

                current_coin_data = coin_news_data[coin_name]
                params = default_params
                validity_days = params['validity_days']

                search_start_time_cutoff = timestamp - pd.Timedelta(days=validity_days + 1)
                start_index = bisect_left(current_coin_data['times'], search_start_time_cutoff)

                valid_news_info = []
                for i in range(start_index, len(current_coin_data['times'])):
                    news_time = current_coin_data['times'][i]
                    if news_time > timestamp: break
                    time_diff_days = (timestamp - news_time).days
                    if time_diff_days <= validity_days:
                        weight = 1.0 - (time_diff_days / float(validity_days))
                        valid_news_info.append((current_coin_data['news'][i], weight))
                
                if valid_news_info:
                    features_tensor_list = [self._extract_single_news_feature(item) for item, _ in valid_news_info]
                    weights_list = [weight for _, weight in valid_news_info]
                    
                    features_stack = torch.stack(features_tensor_list)
                    weights_tensor = torch.tensor(weights_list, dtype=torch.float32)
                    
                    if weights_tensor.sum() > 0:
                        weights_tensor = weights_tensor / weights_tensor.sum()
                        all_features[t, coin_idx] = (features_stack * weights_tensor.unsqueeze(1)).sum(dim=0)
                    elif features_stack.numel() > 0:
                        all_features[t, coin_idx] = features_stack.mean(dim=0)
        return all_features

    def __len__(self):
        # The number of possible start indices for a complete sample (input sequence + target sequence)
        if self.predict_mode:
            # In predict mode, we only need a valid input sequence
            return len(self.time_index) - self.seq_len + 1
        else:
            # In training/validation mode, we need a valid input and target sequence
            return len(self.time_index) - self.seq_len - self.pred_len + 1

    def __getitem__(self, idx):
        seq_end_idx = idx + self.seq_len
        target_end_idx = seq_end_idx + self.pred_len

        price_seq_df = self.price_data_df[self.coin_names].iloc[idx:seq_end_idx]
        price_seq_tensor = torch.tensor(price_seq_df.values, dtype=torch.float32)

        if target_end_idx <= len(self.price_data_df):
            # We have a valid target sequence
            target_price_df = self.price_data_df[self.coin_names].iloc[target_end_idx]
            target_price_tensor = torch.tensor(target_price_df.values, dtype=torch.float32)
        else:
            # This handles cases at the end of the dataset, including predict_mode where a dummy target is needed.
            target_price_tensor = torch.zeros(self.num_coins, dtype=torch.float32)

        price_seq_mark = self.all_time_stamps_encoded[idx:seq_end_idx]

        return_dict = {
            'price_seq': price_seq_tensor,
            'price_seq_mark': price_seq_mark,
            'target_price': target_price_tensor
        }

        if self.has_news:
            news_features_tensor = self.processed_news_features[seq_end_idx-1]
            return_dict['news_features'] = news_features_tensor
        
        return return_dict

def load_news_data(features_dir, coin_names):
    news_data_dict = {}
    for coin in coin_names:
        file_path = os.path.join(features_dir, f"{coin.replace(' ', '')}_features.json")
        try:
            with open(file_path, 'r') as f:
                news_data_dict[coin] = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            news_data_dict[coin] = []
    return news_data_dict

if __name__ == '__main__':
    COIN_NAMES = ['BTC', 'ETH', 'BNB']
    SEQ_LEN = 180
    
    date_rng = pd.date_range(start='2023-01-01', end='2024-01-01', freq='H')
    simulated_price_data = pd.DataFrame(torch.randn(len(date_rng), len(COIN_NAMES)).numpy(), index=date_rng, columns=COIN_NAMES)
    
    features_folder = 'features' # Assume a local features folder for testing
    if not os.path.exists(features_folder): os.makedirs(features_folder)
    news_data = load_news_data(features_folder, COIN_NAMES)

    print("\n--- Test Case 1: With News Data ---")
    dataset_with_news = UnifiedCryptoDataset(simulated_price_data, news_data, seq_len=SEQ_LEN)
    print(f"Dataset with news size: {len(dataset_with_news)}")
    if len(dataset_with_news) > 0:
        sample = dataset_with_news[0]
        assert 'news_features' in sample

    print("\n--- Test Case 2: Without News Data ---")
    dataset_no_news = UnifiedCryptoDataset(simulated_price_data, news_data_dict=None, seq_len=SEQ_LEN)
    print(f"Dataset without news size: {len(dataset_no_news)}")
    if len(dataset_no_news) > 0:
        sample = dataset_no_news[0]
        assert 'news_features' not in sample

    print("\n--- Test Case 3: Predict Mode ---")
    dataset_predict = UnifiedCryptoDataset(simulated_price_data, news_data_dict=None, seq_len=SEQ_LEN, predict_mode=True)
    expected_len = len(simulated_price_data) - SEQ_LEN + 1
    print(f"Dataset in predict mode size: {len(dataset_predict)} (Expected: {expected_len})")
    assert len(dataset_predict) == expected_len
    if len(dataset_predict) > 0:
        last_sample = dataset_predict[len(dataset_predict)-1]
        assert torch.all(last_sample['target_price'] == 0)
    
    print("\n--- All tests passed! ---") 