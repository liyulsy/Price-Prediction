import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
from models.MixModel.timemixer_gcn_no_news import TimeMixerGNN
from dataloader.gnn_loader import load_gnn_data, create_gnn_dataloaders
import argparse

def get_args():
    parser = argparse.ArgumentParser()
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--file_path', type=str, default='Project1/datafiles/1H.csv', help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--test_size', type=float, default=0.2, help='æµ‹è¯•é›†æ¯”ä¾‹')
    parser.add_argument('--threshold', type=float, default=0.6, help='ç›¸å…³ç³»æ•°é˜ˆå€¼')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--freq', type=str, default='h')
    parser.add_argument('--seq_len', type=int, default=180, help='è¾“å…¥åºåˆ—é•¿åº¦')
    parser.add_argument('--pred_len', type=int, default=1, help='é¢„æµ‹é•¿åº¦')
    parser.add_argument('--enc_in', type=int, default=8, help='encoder input size')
    parser.add_argument('--c_out', type=int, default=8, help='output size')
    parser.add_argument('--d_model', type=int, default=512, help='æ¨¡å‹ç»´åº¦')
    parser.add_argument('--decomp_method', type=str, default='moving_avg',
                        help='method of series decompsition, only support moving_avg or dft_decomp')
    parser.add_argument('--down_sampling_layers', type=int, default=2, help='ä¸‹é‡‡æ ·å±‚æ•°')
    parser.add_argument('--down_sampling_window', type=int, default=2, help='ä¸‹é‡‡æ ·çª—å£å¤§å°')
    parser.add_argument('--down_sampling_method', type=str, default='avg', help='ä¸‹é‡‡æ ·æ–¹æ³•')
    parser.add_argument('--use_norm', type=int, default=0, help='æ˜¯å¦ä½¿ç”¨å½’ä¸€åŒ–')
    parser.add_argument('--dropout', type=float, default=0.1, help='dropoutç‡')
    parser.add_argument('--channel_independence', type=bool, default=False, help='é€šé“ç‹¬ç«‹æ€§')
    parser.add_argument('--e_layers', type=int, default=3, help='num of encoder layers')
    parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')
    parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn,FeedForward ç½‘ç»œçš„éšè—å±‚ç»´åº¦')
    parser.add_argument('--embed', type=str, default='timeF',
                        help='time features encoding, options:[timeF, fixed, learned]')

    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=10, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--patience', type=int, default=10, help='æ—©åœè€å¿ƒå€¼')
    
    args = parser.parse_args()
    args.task_name = 'classification'  # è®¾ç½®ä»»åŠ¡ç±»å‹ä¸ºåˆ†ç±»
    return args

def train_epoch(args, model, train_loader, criterion, optimizer, scheduler, edge_index, device):
    for epoch in range(args.epochs):
        model.train()
        epoch_loss = 0.0

        # tqdm å¤–å±‚æ˜¾ç¤º Epoch
        batch_iterator = zip(*train_loader)
        num_batches = min(len(loader) for loader in train_loader)  # æœ€å°çš„ batch æ•°é‡ä½œä¸ºè¿­ä»£æ¬¡æ•°
        pbar = tqdm(batch_iterator, total=num_batches, desc=f"Epoch {epoch+1}/{args.epochs}")

        for batches in pbar:
            coin_data_list = []
            target_list = []

            for data, target in batches:
                coin_data_list.append(data.to(device))
                target_list.append(target.to(device))

            input_data = torch.stack(coin_data_list, dim=1)
            target = torch.stack(target_list, dim=1)

            optimizer.zero_grad()
            output = model(x_enc=input_data, x_mark_enc=None, edge_index=edge_index.to(device))
            #output = model(input_data, None)
            output_flat = output.view(-1, 2)
            target_flat = target.long().view(-1)

            loss = criterion(output_flat, target_flat)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            pbar.set_postfix(loss=loss.item())  # æ¯ä¸ª batch æ›´æ–°ä¸€æ¬¡ loss

        avg_loss = epoch_loss / num_batches
        # æ·»åŠ scheduleræ­¥è¿›
        scheduler.step(avg_loss)
        
        # æ‰“å°å½“å‰å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']
        print(f"âœ… Epoch {epoch+1}/{args.epochs} completed. Avg Loss: {avg_loss:.4f}, Total Loss: {epoch_loss:.4f}ï¼ŒLR: {current_lr:.6f}")

def evaluate(model, test_loader, criterion, edge_index, device):
    total_correct = 0
    total_samples = 0
    total_loss = 0.0

    # åˆå§‹åŒ–æ€»è®¡æ•°å™¨
    total_target_zeros = 0
    total_target_ones = 0
    total_pred_zeros = 0
    total_pred_ones = 0

    # tqdm å¤–å±‚æ˜¾ç¤º Epoch
    batch_iterator = zip(*test_loader)
    num_batches = min(len(loader) for loader in test_loader)  # æœ€å°çš„ batch æ•°é‡ä½œä¸ºè¿­ä»£æ¬¡æ•°
    pbar = tqdm(batch_iterator, total=num_batches, desc="Testing")

    with torch.no_grad():  # æµ‹è¯•æ—¶ä¸è®¡ç®—æ¢¯åº¦
        for batches in pbar:
            coin_data_list = []
            target_list = []

            for data, target in batches:
                coin_data_list.append(data.to(device))
                target_list.append(target.to(device))

            # å †å æˆ [batch_size, num_coins, input_dim]
            input_data = torch.stack(coin_data_list, dim=1)
            target = torch.stack(target_list, dim=1)

            # å‰å‘ä¼ æ’­
            output = model(x_enc=input_data, x_mark_enc=None, edge_index=edge_index.to(device))  # è¾“å‡ºå½¢çŠ¶ [batch_size, num_coins, num_classes]
            #output = model(input_data, None)
            output_flat = output.view(-1, 2)
            target_flat = target.long().view(-1)

            # è®¡ç®—æŸå¤±
            loss = criterion(output_flat, target_flat)
            total_loss += loss.item()

            # å‡†ç¡®ç‡è®¡ç®—
            preds = torch.argmax(output_flat, dim=1)
            correct = (preds == target_flat).sum().item()

            total_correct += correct
            total_samples += target_flat.size(0)

            # æœ¬ batch ä¸­ 0ã€1 çš„æ•°é‡
            target_zeros = (target_flat == 0).sum().item()
            target_ones = (target_flat == 1).sum().item()
            pred_zeros = (preds == 0).sum().item()
            pred_ones = (preds == 1).sum().item()

            # ç´¯åŠ æ€»æ•°
            total_target_zeros += target_zeros
            total_target_ones += target_ones
            total_pred_zeros += pred_zeros
            total_pred_ones += pred_ones

            # æ›´æ–°è¿›åº¦æ¡çš„åç¼€ï¼Œæ˜¾ç¤ºæ‰¹æ¬¡çš„æŸå¤±ã€ç›®æ ‡å’Œé¢„æµ‹ä¸­ 0 å’Œ 1 çš„æ•°é‡
            pbar.set_postfix(loss=loss.item())

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / num_batches

    # è¾“å‡ºæ€»æµ‹è¯•é›†ç»“æœ
    accuracy = total_correct / total_samples
    print(f"\nâœ… Test completed.")
    print(f"ğŸ“Š Avg Loss: {avg_loss:.4f}, Total Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}")
    print(f"ğŸ”¢ Total Target - 0s: {total_target_zeros}, 1s: {total_target_ones}")
    print(f"ğŸ”¢ Total Predicted - 0s: {total_pred_zeros}, 1s: {total_pred_ones}")

def main():
    # è·å–å‚æ•°
    args = get_args()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    coin_features_list, coin_labels_list, edge_index = load_gnn_data(
        file_path=args.file_path,
        input_dim=args.seq_len,
        threshold=args.threshold,
        task=args.task_name,
        norm_type='standard'
    )
    coin_train_loaders, coin_test_loaders = create_gnn_dataloaders(
        coin_features_list,
        coin_labels_list,
        batch_size=args.batch_size,
        test_size=args.test_size,
        task=args.task_name
    )
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = TimeMixerGNN(args).to(device)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=3, factor=0.5, verbose=True
    )
    
    # æ—©åœè®¾ç½®
    best_loss = float('inf')
    patience_counter = 0
    best_model = None

    # è®­ç»ƒ
    train_epoch(args, model, coin_train_loaders, criterion, optimizer, scheduler, edge_index, device)
    
    # éªŒè¯
    evaluate(model, coin_test_loaders, criterion, edge_index, device)
        


if __name__ == '__main__':
    main()