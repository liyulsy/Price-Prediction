import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import pandas as pd
from tqdm import tqdm
import os
import sys
import numpy as np # For accuracy calculation and other metrics if needed
from sklearn.preprocessing import StandardScaler, MinMaxScaler # å¯¼å…¥å½’ä¸€åŒ–å·¥å…·

# å‡è®¾æ¨¡å‹å’Œæ•°æ®é›†ä»£ç åœ¨ä»¥ä¸‹è·¯å¾„ (è¯·æ ¹æ®æ‚¨çš„é¡¹ç›®ç»“æ„è°ƒæ•´)
# é‡è¦: CnnGnn æ¨¡å‹å’Œ CryptoDataset ç±»å¯èƒ½éœ€è¦ä¿®æ”¹æ‰èƒ½å®Œå…¨æ”¯æŒæ— æ–°é—»æ¨¡å¼
from models.MixModel.cnn_gnn_no_news import CnnGnnNoNews
from scripts.analysis.crypto_new_analyzer.dataset_no_news import CryptoDatasetNoNews # load_news_data ä¸å†éœ€è¦
from dataloader.gnn_loader import generate_edge_index

# Add the project root to Python path
current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()
project_root = os.path.join(current_dir, '..', '..')
sys.path.append(project_root)


# --- é…ç½®å’Œè¶…å‚æ•° ---
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# æ•°æ®è·¯å¾„
PRICE_CSV_PATH = 'datafiles/price_data/1H.csv' # è¯·ç¡®ä¿è¿™æ˜¯æ­£ç¡®çš„ä»·æ ¼æ•°æ®æ–‡ä»¶è·¯å¾„
# NEWS_FEATURES_FOLDER å·²ç§»é™¤
# PROCESSED_NEWS_CACHE_PATH å·²ç§»é™¤

# æ•°æ®é›†å‚æ•°
COIN_NAMES = ['BTC', 'ETH', 'BNB', 'XRP', 'LTC', 'DOGE', 'SOL', 'AVAX'] # æ‚¨çš„å¸ç§åˆ—è¡¨
PRICE_SEQ_LEN = 180 # CNNçš„è¾“å…¥åºåˆ—é•¿åº¦ (ä¾‹å¦‚ï¼Œè¿‡å»180ä¸ªæ—¶é—´ç‚¹)
THRESHOLD = 0.6 # ç”¨äºç”Ÿæˆé‚»æ¥çŸ©é˜µçš„é˜ˆå€¼
NORM_TYPE = 'standard' # ä»·æ ¼æ•°æ®å½’ä¸€åŒ–ç±»å‹: 'standard', 'minmax', or 'none'
# NUM_NODES å°†ä» COIN_NAMES é•¿åº¦åŠ¨æ€è·å–
# NEWS_FEATURE_DIM å°†ç”± CryptoDataset åœ¨æ— æ–°é—»æ—¶æä¾› (åº”ä¸º0)

# CnnGnn æ¨¡å‹å‚æ•°
NEWS_FEATURE_DIM_CONFIG = 0    # æ˜ç¡®è®¾ä¸º0ï¼Œè¡¨ç¤ºæ— æ–°é—»ç‰¹å¾ç»´åº¦
NEWS_PROCESSED_DIM_CONFIG = 0  # æ˜ç¡®è®¾ä¸º0ï¼Œè¡¨ç¤ºæ— å¤„ç†åçš„æ–°é—»ç‰¹å¾ç»´åº¦
GCN_HIDDEN_DIM = 128          # GCNéšè—å±‚ç»´åº¦
GCN_OUTPUT_DIM = 64           # GCNè¾“å‡ºå±‚ç»´åº¦ (ä¹Ÿæ˜¯æœ€ç»ˆMLPçš„è¾“å…¥éƒ¨åˆ†)
CNN_OUTPUT_CHANNELS = 32      # CNNè¾“å‡ºç»´åº¦
FINAL_MLP_HIDDEN_DIM = 128    # æœ€ç»ˆMLPçš„éšè—å±‚ç»´åº¦
NUM_CLASSES = 2               # åˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«æ•°é‡ (ä¾‹å¦‚ï¼Œ2è¡¨ç¤ºæ¶¨/è·Œ)

# è®­ç»ƒå‚æ•°
BATCH_SIZE = 32               # æ‰¹é‡å¤§å°
EPOCHS = 20                   # è®­ç»ƒå‘¨æœŸæ•°
LEARNING_RATE = 0.0005        # å­¦ä¹ ç‡
WEIGHT_DECAY = 1e-5           # ä¼˜åŒ–å™¨çš„æƒé‡è¡°å‡
VALIDATION_SPLIT_RATIO = 0.15 # ä»æ€»æ•°æ®é›†ä¸­åˆ†å‡ºä½œä¸ºéªŒè¯é›†çš„æ¯”ä¾‹
TEST_SPLIT_RATIO = 0.15       # ä»æ€»æ•°æ®é›†ä¸­åˆ†å‡ºä½œä¸ºæµ‹è¯•é›†çš„æ¯”ä¾‹
# FORCE_RECOMPUTE_NEWS å·²ç§»é™¤

# ç”¨äºä¿å­˜æœ€ä½³æ¨¡å‹çš„è·¯å¾„
BEST_MODEL_PATH = "cache/best_cnn_gnn_no_news_model.pt" # ä¿®æ”¹æ¨¡å‹ä¿å­˜è·¯å¾„

def evaluate_model(model, data_loader, criterion, edge_index, num_nodes, device):
    model.eval()
    total_loss = 0.0
    total_correct = 0
    total_samples = 0
    
    # ä¸ºæ¯ä¸ªå¸ç§åˆ›å»ºç»Ÿè®¡å˜é‡
    coin_correct = {coin: 0 for coin in COIN_NAMES}
    coin_total = {coin: 0 for coin in COIN_NAMES}
    
    all_targets_flat = []
    all_preds_flat = []

    with torch.no_grad():
        for batch_data in tqdm(data_loader, desc="Evaluating"):
            price_seq = batch_data['price_seq'].to(device)
            target_prices = batch_data['target_price'].to(device)

            target_labels = (target_prices > 0).long()
            
            outputs = model(price_seq, edge_index)
            
            loss = criterion(outputs.view(-1, NUM_CLASSES), target_labels.view(-1))
            total_loss += loss.item() * price_seq.size(0)
            
            preds = torch.argmax(outputs, dim=-1)
            total_correct += (preds == target_labels).sum().item()
            total_samples += target_labels.numel()

            # è®¡ç®—æ¯ä¸ªå¸ç§çš„æ­£ç¡®æ•°å’Œæ ·æœ¬æ•°
            for i, coin in enumerate(COIN_NAMES):
                coin_correct[coin] += (preds[:, i] == target_labels[:, i]).sum().item()
                coin_total[coin] += preds[:, i].numel()

            all_targets_flat.extend(target_labels.view(-1).cpu().numpy())
            all_preds_flat.extend(preds.view(-1).cpu().numpy())

    avg_loss = total_loss / len(data_loader.dataset)
    accuracy = total_correct / total_samples if total_samples > 0 else 0
    
    # è®¡ç®—æ¯ä¸ªå¸ç§çš„å‡†ç¡®ç‡
    coin_accuracies = {coin: coin_correct[coin] / coin_total[coin] if coin_total[coin] > 0 else 0 
                      for coin in COIN_NAMES}
    
    return avg_loss, accuracy, coin_accuracies

if __name__ == '__main__':
    print(f"Using device: {DEVICE}")

    # --- 1. åŠ è½½å’Œé¢„å¤„ç†ä»·æ ¼æ•°æ® ---
    print("\n--- 1. Loading and Preprocessing Price Data ---")
    if not os.path.exists(PRICE_CSV_PATH):
        print(f"é”™è¯¯: ä»·æ ¼æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°äº {PRICE_CSV_PATH}")
        exit()
    # æ–°é—»æ–‡ä»¶å¤¹æ£€æŸ¥å·²ç§»é™¤
        
    price_df_original_load = pd.read_csv(PRICE_CSV_PATH, index_col=0, parse_dates=True)
    
    print(f"åŸå§‹CSVåŠ è½½çš„åˆ—å: {price_df_original_load.columns.tolist()}") 
    expected_csv_columns = [f"{coin}-USDT" for coin in COIN_NAMES]
    missing_cols = [col for col in expected_csv_columns if col not in price_df_original_load.columns]
    if missing_cols:
        print(f"é”™è¯¯: ä»·æ ¼æ•°æ®CSVæ–‡ä»¶ '{PRICE_CSV_PATH}' ä¸­ç¼ºå°‘ä»¥ä¸‹é¢„æœŸçš„åˆ—: {missing_cols}")
        print(f"åŸºäºCOIN_NAMES = {COIN_NAMES}, è„šæœ¬æœŸæœ›æ‰¾åˆ°åˆ—: {expected_csv_columns}")
        exit()
    rename_map = {f"{coin}-USDT": coin for coin in COIN_NAMES}
    price_df_processed_cols = price_df_original_load.rename(columns=rename_map)
    price_df_final_cols = price_df_processed_cols[COIN_NAMES]
    print(f"åˆ—åå¤„ç†å’Œæ’åºåçš„DataFrameåˆ—: {price_df_final_cols.columns.tolist()}")

    # --- 2. å®šä¹‰ edge_index ---
    print("\n--- 2. Defining Edge Index (using original scale data) ---")
    edge_index = generate_edge_index(price_df_final_cols, THRESHOLD).to(DEVICE)
    print(f"Edge index created (shape: {edge_index.shape})")

    # --- 3. ä»·æ ¼æ•°æ®å½’ä¸€åŒ–/æ ‡å‡†åŒ– ---
    print(f"\n--- 3. Applying Price Data Normalization (Type: {NORM_TYPE}) ---")
    num_total_samples = len(price_df_final_cols)
    fit_train_size = int(num_total_samples * (1 - VALIDATION_SPLIT_RATIO - TEST_SPLIT_RATIO))
    
    if fit_train_size <= 0:
        print(f"é”™è¯¯: æ•°æ®é›†å¤ªå° (æˆ–åˆ’åˆ†æ¯”ä¾‹ä¸å½“)ï¼Œæ— æ³•åˆ’å®šæœ‰æ•ˆçš„è®­ç»ƒéƒ¨åˆ†æ¥æ‹Ÿåˆå½’ä¸€åŒ–scalerã€‚è®¡ç®—å¾—åˆ°çš„æ‹Ÿåˆç”¨æ ·æœ¬æ•°: {fit_train_size}")
        exit()
        
    price_df_for_scaler_fit = price_df_final_cols.iloc[:fit_train_size]
    price_df_to_normalize = price_df_final_cols.copy()

    if NORM_TYPE == 'standard':
        scaler = StandardScaler()
        scaler.fit(price_df_for_scaler_fit)
        price_df_values = scaler.transform(price_df_to_normalize)
        price_df = pd.DataFrame(price_df_values, columns=price_df_to_normalize.columns, index=price_df_to_normalize.index)
        print("ä»·æ ¼æ•°æ®å·²è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚")
    elif NORM_TYPE == 'minmax':
        scaler = MinMaxScaler()
        scaler.fit(price_df_for_scaler_fit)
        price_df_values = scaler.transform(price_df_to_normalize)
        price_df = pd.DataFrame(price_df_values, columns=price_df_to_normalize.columns, index=price_df_to_normalize.index)
        print("ä»·æ ¼æ•°æ®å·²è¿›è¡ŒMin-Maxå½’ä¸€åŒ–å¤„ç†ã€‚")
    elif NORM_TYPE == 'none':
        price_df = price_df_to_normalize
        print("æœªå¯¹ä»·æ ¼æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚")
    else:
        price_df = price_df_to_normalize
        print(f"è­¦å‘Š: æœªçŸ¥çš„NORM_TYPE '{NORM_TYPE}'ã€‚æœªå¯¹ä»·æ ¼æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚")
    
    # --- 4. åˆ›å»º CryptoDataset å’Œ DataLoader ---
    print("\n--- 4. Creating Datasets and DataLoaders ---")
    # news_data = load_news_data(NEWS_FEATURES_FOLDER, COIN_NAMES) # å·²ç§»é™¤
    
    # å®ä¾‹åŒ– CryptoDataset æ—¶ä¸ä¼ å…¥æ–°é—»ç›¸å…³å‚æ•°
    # å‡è®¾ CryptoDataset å·²ä¿®æ”¹æˆ–èƒ½å¤Ÿå¤„ç† news_data_dict ä¸ºç©º/None çš„æƒ…å†µ
    # å¹¶ä¸”èƒ½å¤Ÿåœ¨è¿™ç§æƒ…å†µä¸‹æ­£ç¡®è®¾ç½® self.news_feature_dim = 0
    # ç§»é™¤ processed_news_features_path, force_recompute_news
    # ç§»é™¤ news_norm_type, news_scaler_fit_upto_row (å¦‚æœä¹‹å‰ä½œä¸º CryptoDataset å‚æ•°æ·»åŠ äº†)
    dataset = CryptoDatasetNoNews(
        price_data_df=price_df, 
        seq_len=PRICE_SEQ_LEN
    )
    NUM_NODES = dataset.num_coins
    # NEWS_FEATURE_DIM åº”è¯¥ä» dataset.news_feature_dim è·å–ï¼Œæ­¤æ—¶é¢„æœŸä¸º 0
    # å¦‚æœ CryptoDataset æœªä¿®æ”¹ï¼Œdataset.news_feature_dim å¯èƒ½ä»æ˜¯å…¶é»˜è®¤å€¼ï¼Œå¯¼è‡´é—®é¢˜
    # ä¸ºç¡®ä¿æ¨¡å‹æ­£ç¡®é…ç½®ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨é…ç½®çš„ NEWS_FEATURE_DIM_CONFIG
    # ACTUAL_NEWS_FEATURE_DIM_FROM_DATASET = dataset.news_feature_dim 
    
    print(f"Dataset created. Number of nodes: {NUM_NODES}, Total samples: {len(dataset)}")
    # print(f"  (Info) News feature dim from dataset: {ACTUAL_NEWS_FEATURE_DIM_FROM_DATASET}")
    print(f"  (Info) Model configured with news_feature_dim: {NEWS_FEATURE_DIM_CONFIG}")

    if len(dataset) == 0:
        print("é”™è¯¯: æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶å’Œé¢„å¤„ç†æ­¥éª¤ã€‚")
        exit()

    total_size = len(dataset)
    test_size = int(TEST_SPLIT_RATIO * total_size)
    val_size = int(VALIDATION_SPLIT_RATIO * total_size)
    train_size = total_size - test_size - val_size
    if train_size <= 0 or val_size <= 0 or test_size <= 0:
        print(f"é”™è¯¯: æ•°æ®é›†å¤ªå°æ— æ³•æŒ‰æŒ‡å®šæ¯”ä¾‹åˆ’åˆ†ã€‚Train: {train_size}, Val: {val_size}, Test: {test_size}")
        exit()
    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])
    print(f"Train dataset size: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}")
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

    # --- 5. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ ---
    print("\n--- 5. Initializing Model, Loss, Optimizer ---")
    # é‡è¦: CnnGnn æ¨¡å‹éœ€è¦èƒ½å¤Ÿå¤„ç† news_feature_dim=0 å’Œ news_processed_dim=0
    # è¿™å¯èƒ½æ„å‘³ç€åœ¨å…¶ __init__ ä¸­è·³è¿‡ news_processor çš„åˆ›å»ºï¼Œå¹¶è°ƒæ•´èåˆé€»è¾‘
    # å…¶ forward æ–¹æ³•ä¹Ÿéœ€è¦ä¿®æ”¹ä¸º forward(self, price_data_x, edge_index, news_features=None)
    model = CnnGnnNoNews(
        price_seq_len=PRICE_SEQ_LEN,
        num_nodes=NUM_NODES,
        cnn_output_channels=CNN_OUTPUT_CHANNELS,
        gcn_hidden_dim=GCN_HIDDEN_DIM,
        gcn_output_dim=GCN_OUTPUT_DIM,
        final_mlp_hidden_dim=FINAL_MLP_HIDDEN_DIM,
        num_classes=NUM_CLASSES
    ).to(DEVICE)
    print(model)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)

    # --- 6. è®­ç»ƒå¾ªç¯ ---
    print("\n--- 6. Starting Training ---")
    best_val_loss = float('inf')
    epochs_no_improve = 0
    patience_early_stopping = 10

    for epoch in range(EPOCHS):
        model.train()
        epoch_train_loss = 0.0
        epoch_train_correct = 0
        epoch_train_samples = 0
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} Training", leave=False)
        for batch_data in train_pbar:
            price_seq = batch_data['price_seq'].to(DEVICE)
            target_prices = batch_data['target_price'].to(DEVICE)
            target_labels = (target_prices > 0).long() 
            
            optimizer.zero_grad()
            outputs = model(price_seq, edge_index)
            
            loss = criterion(outputs.view(-1, NUM_CLASSES), target_labels.view(-1))
            loss.backward()
            optimizer.step()
            epoch_train_loss += loss.item() * price_seq.size(0)
            preds = torch.argmax(outputs, dim=-1)
            epoch_train_correct += (preds == target_labels).sum().item()
            epoch_train_samples += target_labels.numel()
            train_pbar.set_postfix(loss=loss.item())

        avg_train_loss = epoch_train_loss / len(train_dataset)
        train_accuracy = epoch_train_correct / epoch_train_samples if epoch_train_samples > 0 else 0
        
        val_loss, val_accuracy, val_coin_accuracies = evaluate_model(model, val_loader, criterion, edge_index, NUM_NODES, DEVICE)
        
        print(f"\nEpoch {epoch+1}/{EPOCHS}")
        print(f"Train - Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.4f}")
        print(f"Val   - Loss: {val_loss:.4f}, Acc: {val_accuracy:.4f}")
        print("Val Coin Accuracies:")
        for coin, acc in val_coin_accuracies.items():
            print(f"  {coin}: {acc:.4f}")
        print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        scheduler.step(val_loss)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            if BEST_MODEL_PATH:
                save_dir = os.path.dirname(BEST_MODEL_PATH)
                if save_dir and not os.path.exists(save_dir):
                    os.makedirs(save_dir)
            torch.save(model.state_dict(), BEST_MODEL_PATH)
            print(f"ğŸš€ New best model saved to {BEST_MODEL_PATH} (Val Loss: {best_val_loss:.4f})")
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience_early_stopping:
                print(f"â³ Early stopping triggered after {patience_early_stopping} epochs with no improvement on validation loss.")
                break
        current_lr = optimizer.param_groups[0]['lr']
        print(f"Current LR: {current_lr:.6f}")

    # --- 7. æµ‹è¯•æ­¥éª¤ ---
    print("\n--- 7. Starting Testing with Best Model ---")
    if os.path.exists(BEST_MODEL_PATH):
        model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE))
        print(f"Loaded best model from {BEST_MODEL_PATH} for testing.")
    else:
        print(f"Warning: Best model file {BEST_MODEL_PATH} not found. Testing with the last state of the model.")

    test_loss, test_accuracy, test_coin_accuracies = evaluate_model(model, test_loader, criterion, edge_index, NUM_NODES, DEVICE)
    print(f"\nâœ… Test Results:")
    print(f"Overall - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}")
    print("Coin-wise Accuracies:")
    for coin, acc in test_coin_accuracies.items():
        print(f"  {coin}: {acc:.4f}")

    print("\n--- Script Finished ---") 