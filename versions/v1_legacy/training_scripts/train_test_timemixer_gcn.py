import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import pandas as pd
from tqdm import tqdm
import os
import sys
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Add the project root to Python path
current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()
project_root = os.path.join(current_dir, '..', '..')
sys.path.append(project_root)

# Model and Dataset Imports
from models.MixModel.timemixer_gcn import TimeMixerGCN
from scripts.analysis.crypto_new_analyzer.dataset import CryptoDataset, load_news_data
from dataloader.gnn_loader import generate_edge_index

# --- TimeMixer Configuration Class ---
class TimeMixerConfigs:
    # TimeMixeræ¨¡å‹é…ç½®å‚æ•°
    def __init__(self, num_nodes, price_seq_len, num_time_features, # èŠ‚ç‚¹æ•°ï¼Œä»·æ ¼åºåˆ—è¾“å…¥é•¿åº¦ï¼Œæ—¶é—´ç‰¹å¾æ•°é‡
                 d_model=64, pred_len=1, label_len_ratio=0.5, # æ¨¡å‹ä¸»è¦ç‰¹å¾ç»´åº¦ï¼Œé¢„æµ‹é•¿åº¦ï¼Œæ ‡ç­¾é•¿åº¦æ¯”ä¾‹
                 dropout=0.1, n_heads=4, d_ff=128, e_layers=2, factor=5, # Dropoutæ¯”ä¾‹ï¼ŒAttentionå¤´æ•°é‡ï¼Œå‰é¦ˆç½‘ç»œç»´åº¦ï¼Œç¼–ç å™¨å±‚æ•°ï¼ŒAttentionç›¸å…³å‚æ•°
                 freq='h', output_attention=False, embed_type='timeF', # æ—¶é—´åºåˆ—é¢‘ç‡ï¼Œæ˜¯å¦è¾“å‡ºæ³¨æ„åŠ›æƒé‡ï¼Œæ—¶é—´ç‰¹å¾ç¼–ç ç±»å‹
                 use_norm: bool = False,
                 decomp_method = 'moving_avg',
                 down_sampling_layers=2,
                 down_sampling_window=2,
                 down_sampling_method='avg',
                 channel_independence=False,
                 moving_avg=25):
        self.enc_in = num_nodes # ç¼–ç å™¨è¾“å…¥ç‰¹å¾ç»´åº¦ (å³èŠ‚ç‚¹æ•°)
        self.dec_in = num_nodes # è§£ç å™¨è¾“å…¥ç‰¹å¾ç»´åº¦ (åœ¨æœ¬æ¨¡å‹ä¸­ä¸ç¼–ç å™¨ç›¸åŒ)
        self.c_out = num_nodes # è¾“å‡ºç‰¹å¾ç»´åº¦ (å³èŠ‚ç‚¹æ•°)
        self.d_model = d_model # æ¨¡å‹çš„ä¸»è¦ç‰¹å¾ç»´åº¦ (patch embeddingåçš„ç»´åº¦)
        self.seq_len = price_seq_len # è¾“å…¥åºåˆ—é•¿åº¦
        self.pred_len = pred_len # é¢„æµ‹é•¿åº¦ (æœ¬æ¨¡å‹é¢„æµ‹ä¸‹ä¸€æ—¶é—´æ­¥)
        self.label_len = int(price_seq_len * label_len_ratio) # æ ‡ç­¾é•¿åº¦ï¼Œç”¨äºTimeXerçš„Decoder (æœ¬æ¨¡å‹æœªä½¿ç”¨Decoder)
        self.output_attention = output_attention # æ˜¯å¦è¾“å‡ºæ³¨æ„åŠ›æƒé‡ (æœ¬æ¨¡å‹æœªä½¿ç”¨Decoder)
        self.embed = embed_type # æ—¶é—´ç‰¹å¾ç¼–ç ç±»å‹ (å¦‚ 'timeF', 'fixed', 'learned')
        self.freq = freq # æ—¶é—´åºåˆ—é¢‘ç‡ (å¦‚ 'h' å°æ—¶)
        self.dropout = dropout # Dropoutæ¯”ä¾‹
        self.n_heads = n_heads # Attentionå¤´çš„æ•°é‡
        self.d_ff = d_ff # å‰é¦ˆç½‘ç»œçš„éšè—å±‚ç»´åº¦
        self.factor = factor # Attentionç›¸å…³å‚æ•° (é€šå¸¸ç”¨äºè‡ªæ³¨æ„åŠ›)
        self.activation = 'gelu' # æ¿€æ´»å‡½æ•°
        self.e_layers = e_layers # ç¼–ç å™¨çš„å±‚æ•°
        self.use_norm = use_norm # æ˜¯å¦ä½¿ç”¨å½’ä¸€åŒ–å±‚
        self.num_time_features = num_time_features # æ—¶é—´ç‰¹å¾çš„æ•°é‡
        self.decomp_method = decomp_method
        self.down_sampling_layers = down_sampling_layers
        self.down_sampling_window = down_sampling_window
        self.down_sampling_method = down_sampling_method
        self.channel_independence = channel_independence
        self.moving_avg = moving_avg

# --- Main Configuration and Hyperparameters ---
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # ä½¿ç”¨çš„è®¾å¤‡ (GPUæˆ–CPU)

# Data paths
PRICE_CSV_PATH = 'datafiles/price_data/1H.csv' # åŸå§‹ä»·æ ¼æ•°æ®CSVæ–‡ä»¶è·¯å¾„
NEWS_FEATURES_FOLDER = 'crypto_new_analyzer/features' # æ–°é—»ç‰¹å¾æ–‡ä»¶å¤¹è·¯å¾„
PROCESSED_NEWS_CACHE_PATH = "cache/all_processed_news_features.pt" # å¤„ç†åçš„æ–°é—»ç‰¹å¾ç¼“å­˜æ–‡ä»¶è·¯å¾„

# Dataset parameters
COIN_NAMES = ['BTC', 'ETH', 'BNB', 'XRP', 'LTC', 'DOGE', 'SOL', 'AVAX'] # åŠ è½½çš„å¸ç§åç§°åˆ—è¡¨ (èŠ‚ç‚¹)
PRICE_SEQ_LEN = 60 # ä»·æ ¼æ—¶é—´åºåˆ—è¾“å…¥é•¿åº¦
THRESHOLD = 0.6 # æ„å»ºå›¾æ—¶è¾¹çš„é˜ˆå€¼ (ç›¸å…³æ€§é˜ˆå€¼)
NORM_TYPE = 'standard' # ä»·æ ¼æ•°æ®å½’ä¸€åŒ–ç±»å‹ ('standard', 'minmax', 'none')
TIME_ENCODING_ENABLED_IN_DATASET = True # æ•°æ®é›†ä¸­æ˜¯å¦å¯ç”¨æ—¶é—´ç¼–ç 
TIME_FREQ_IN_DATASET = 'h' # æ•°æ®é›†ä½¿ç”¨çš„æ—¶é—´é¢‘ç‡ (ç”¨äºæ—¶é—´ç¼–ç )

# TimeMixerConfigs parameters (to be instantiated later after dataset)
D_MODEL = 64 # TimeMixeræ¨¡å‹ä¸»è¦ç‰¹å¾ç»´åº¦
E_LAYERS = 2 # TimeMixerç¼–ç å™¨å±‚æ•°
N_HEADS = 4 # TimeMixer Attentionå¤´æ•°é‡
D_FF = 128 # TimeMixer å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦

DROPOUT_TIMEMIXER = 0.1 # TimeMixer Dropoutæ¯”ä¾‹
DECOMP_METHOD = 'moving_avg' # TimeMixer åˆ†è§£æ–¹æ³•
DOWN_SAMPLING_LAYERS = 2 # TimeMixer ä¸‹é‡‡æ ·å±‚æ•°
DOWN_SAMPLING_WINDOW = 2 # TimeMixer ä¸‹é‡‡æ ·çª—å£å¤§å°
DOWN_SAMPLING_METHOD = 'avg' # TimeMixer ä¸‹é‡‡æ ·æ–¹æ³•
CHANNEL_INDEPENDENCE = True # TimeMixer é€šé“ç‹¬ç«‹æ€§
MOVING_AVG = 25 # TimeMixer ç§»åŠ¨å¹³å‡çª—å£å¤§å°

# TimexerGCN specific model parameters
NEWS_PROCESSED_DIM = 32 # å¤„ç†åçš„æ–°é—»ç‰¹å¾ç»´åº¦
GCN_HIDDEN_DIM = 128 # GCNéšè—å±‚ç»´åº¦
GCN_OUTPUT_DIM = 64 # GCNè¾“å‡ºç»´åº¦

# Training parameters
BATCH_SIZE = 16 # è®­ç»ƒæ‰¹é‡å¤§å°
EPOCHS = 20 # æ€»è®­ç»ƒè½®æ•°
LEARNING_RATE = 0.0005 # å­¦ä¹ ç‡
WEIGHT_DECAY = 1e-5 # æƒé‡è¡°å‡ (L2æ­£åˆ™åŒ–)
VALIDATION_SPLIT_RATIO = 0.15 # éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹
TEST_SPLIT_RATIO = 0.15 # æµ‹è¯•é›†åˆ’åˆ†æ¯”ä¾‹
FORCE_RECOMPUTE_NEWS = False # æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—å¹¶ç¼“å­˜æ–°é—»ç‰¹å¾

BEST_MODEL_PATH = "Project1/cache/best_timemixer_gcn_model.pt" # æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„

def evaluate_model(model, data_loader, criterion, edge_index, device):
    model.eval()
    total_loss = 0.0
    total_correct = 0
    total_samples = 0
    
    # ä¸ºæ¯ä¸ªå¸ç§åˆ›å»ºç»Ÿè®¡å˜é‡
    coin_correct = {coin: 0 for coin in COIN_NAMES}
    coin_total = {coin: 0 for coin in COIN_NAMES}
    
    all_targets_flat = []
    all_preds_flat = []

    with torch.no_grad():
        for batch_data in tqdm(data_loader, desc="Evaluating"):
            price_seq = batch_data['price_seq'].to(device)
            x_mark_enc = batch_data['price_seq_mark'].to(device)
            news_features = batch_data['news_features'].to(device)
            target_prices = batch_data['target_price'].to(device)

            target_labels = (target_prices > 0).long()
            
            outputs = model(price_seq, x_mark_enc, edge_index, news_features)
            
            loss = criterion(outputs.view(-1, model.mlp[-1].out_features), target_labels.view(-1))
            total_loss += loss.item() * price_seq.size(0)
            
            preds = torch.argmax(outputs, dim=-1)  # [batch, num_nodes]
            
            # è®¡ç®—æ€»ä½“æ­£ç¡®æ•°å’Œæ ·æœ¬æ•°
            total_correct += (preds == target_labels).sum().item()
            total_samples += target_labels.numel()
            
            # è®¡ç®—æ¯ä¸ªå¸ç§çš„æ­£ç¡®æ•°å’Œæ ·æœ¬æ•°
            for i, coin in enumerate(COIN_NAMES):
                coin_correct[coin] += (preds[:, i] == target_labels[:, i]).sum().item()
                coin_total[coin] += preds[:, i].numel()

            all_targets_flat.extend(target_labels.view(-1).cpu().numpy())
            all_preds_flat.extend(preds.view(-1).cpu().numpy())

    avg_loss = total_loss / len(data_loader.dataset)
    total_accuracy = total_correct / total_samples if total_samples > 0 else 0
    
    # è®¡ç®—æ¯ä¸ªå¸ç§çš„å‡†ç¡®ç‡
    coin_accuracies = {coin: coin_correct[coin] / coin_total[coin] if coin_total[coin] > 0 else 0 
                      for coin in COIN_NAMES}
    
    return avg_loss, total_accuracy, coin_accuracies

if __name__ == '__main__':
    print(f"Using device: {DEVICE}")
    # 1. åŠ è½½å’Œé¢„å¤„ç†ä»·æ ¼æ•°æ®
    print("\n--- 1. Loading and Preprocessing Price Data ---")
    if not os.path.exists(PRICE_CSV_PATH):
        print(f"é”™è¯¯: ä»·æ ¼æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°äº {PRICE_CSV_PATH}")
        exit()
    if not os.path.exists(NEWS_FEATURES_FOLDER):
        print(f"é”™è¯¯: æ–°é—»ç‰¹å¾æ–‡ä»¶å¤¹æœªæ‰¾åˆ°äº {NEWS_FEATURES_FOLDER}")
        exit()
    price_df_original_load = pd.read_csv(PRICE_CSV_PATH, index_col=0, parse_dates=True)
    expected_csv_columns = [f"{coin}-USDT" for coin in COIN_NAMES]
    missing_cols = [col for col in expected_csv_columns if col not in price_df_original_load.columns]
    if missing_cols:
        print(f"é”™è¯¯: CSV '{PRICE_CSV_PATH}' ç¼ºå°‘åˆ—: {missing_cols}")
        exit()
    rename_map = {f"{coin}-USDT": coin for coin in COIN_NAMES}
    price_df_processed_cols = price_df_original_load.rename(columns=rename_map)
    price_df_final_cols = price_df_processed_cols[COIN_NAMES]
    # 2. æ„å»ºå›¾
    print("\n--- 2. Defining Edge Index ---")
    edge_index = generate_edge_index(price_df_final_cols, THRESHOLD).to(DEVICE)
    # 3. å½’ä¸€åŒ–
    print(f"\n--- 3. Applying Data Normalization (Type: {NORM_TYPE}) ---")
    num_total_samples = len(price_df_final_cols)
    fit_train_size = int(num_total_samples * (1 - VALIDATION_SPLIT_RATIO - TEST_SPLIT_RATIO))
    if fit_train_size <= 0:
        print(f"é”™è¯¯: æ•°æ®é›†å¤ªå°æˆ–åˆ’åˆ†æ¯”ä¾‹ä¸å½“. Fit train size: {fit_train_size}")
        exit()
    price_df_for_scaler_fit = price_df_final_cols.iloc[:fit_train_size]
    price_df_to_normalize = price_df_final_cols.copy()
    if NORM_TYPE == 'standard':
        scaler = StandardScaler()
        price_df_values_full = scaler.fit_transform(price_df_to_normalize)
        price_df = pd.DataFrame(price_df_values_full, columns=price_df_to_normalize.columns, index=price_df_to_normalize.index)
    elif NORM_TYPE == 'minmax':
        scaler = MinMaxScaler()
        price_df_values_full = scaler.fit_transform(price_df_to_normalize)
        price_df = pd.DataFrame(price_df_values_full, columns=price_df_to_normalize.columns, index=price_df_to_normalize.index)
    elif NORM_TYPE == 'none':
        price_df = price_df_to_normalize
    else:
        price_df = price_df_to_normalize
        print(f"è­¦å‘Š: æœªçŸ¥çš„NORM_TYPE '{NORM_TYPE}'ã€‚æœªå¯¹ä»·æ ¼æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚")
    # 4. åŠ è½½æ–°é—»ç‰¹å¾å’Œæ•°æ®é›†
    print("\n--- 4. Creating Datasets and DataLoaders ---")
    news_data = load_news_data(NEWS_FEATURES_FOLDER, COIN_NAMES)
    dataset = CryptoDataset(
        price_data_df=price_df,
        news_data_dict=news_data,
        seq_len=PRICE_SEQ_LEN,
        processed_news_features_path=PROCESSED_NEWS_CACHE_PATH,
        force_recompute_news=FORCE_RECOMPUTE_NEWS,
        time_encoding_enabled=TIME_ENCODING_ENABLED_IN_DATASET,
        time_freq=TIME_FREQ_IN_DATASET
    )
    NUM_NODES = dataset.num_coins
    NEWS_FEATURE_DIM = dataset.news_feature_dim
    ACTUAL_NUM_TIME_FEATURES = dataset.num_actual_time_features
    print(f"Dataset created. Nodes: {NUM_NODES}, NewsFeatDim: {NEWS_FEATURE_DIM}, TimeFeatDim: {ACTUAL_NUM_TIME_FEATURES}, Samples: {len(dataset)}")
    if len(dataset) == 0:
        print("é”™è¯¯: æ•°æ®é›†ä¸ºç©ºã€‚")
        exit()
    total_size = len(dataset)
    test_size = int(TEST_SPLIT_RATIO * total_size)
    val_size = int(VALIDATION_SPLIT_RATIO * total_size)
    train_size = total_size - test_size - val_size
    if train_size <= 0 or val_size <= 0 or test_size <= 0:
        print(f"é”™è¯¯: æ•°æ®é›†å¤ªå°æ— æ³•åˆ’åˆ†. Train: {train_size}, Val: {val_size}, Test: {test_size}")
        exit()
    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    # 5. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨
    print("\n--- 5. Initializing Model, Loss, Optimizer ---")

    timemixer_model_configs = TimeMixerConfigs(
        num_nodes=NUM_NODES,
        price_seq_len=PRICE_SEQ_LEN,
        num_time_features=ACTUAL_NUM_TIME_FEATURES,
        d_model=D_MODEL,
        e_layers=E_LAYERS,
        n_heads=N_HEADS,
        d_ff=D_FF,
        dropout=DROPOUT_TIMEMIXER,
        freq=TIME_FREQ_IN_DATASET,
        decomp_method=DECOMP_METHOD,
        down_sampling_layers=DOWN_SAMPLING_LAYERS,
        down_sampling_window=DOWN_SAMPLING_WINDOW,
        down_sampling_method=DOWN_SAMPLING_METHOD,
        channel_independence=CHANNEL_INDEPENDENCE,
        moving_avg=MOVING_AVG
    )
    print(f"TimeMixer Configs: enc_in={timemixer_model_configs.enc_in}, seq_len={timemixer_model_configs.seq_len}, d_model={timemixer_model_configs.d_model}, num_time_feat={timemixer_model_configs.num_time_features}")
    
    model = TimeMixerGCN(
        configs=timemixer_model_configs,
        hidden_dim=GCN_HIDDEN_DIM,
        output_dim=GCN_OUTPUT_DIM,
        news_feature_dim=NEWS_FEATURE_DIM,
        news_processed_dim=NEWS_PROCESSED_DIM,
        num_classes=2
    ).to(DEVICE)
    print(model)
    print(f"Model Params: {sum(p.numel() for p in model.parameters())}")

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)

    # --- 6. Training Loop --- # è®­ç»ƒå¾ªç¯
    print("\n--- 6. Starting Training ---")
    best_val_loss = float('inf')
    epochs_no_improve = 0
    patience_early_stopping = 10

    for epoch in range(EPOCHS):
        model.train()
        epoch_train_loss = 0.0
        epoch_train_correct = 0
        epoch_train_samples = 0
        
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} Training", leave=False)
        for batch_data in train_pbar:
            price_seq = batch_data['price_seq'].to(DEVICE)
            x_mark_enc = batch_data['price_seq_mark'].to(DEVICE)
            news_features = batch_data['news_features'].to(DEVICE)
            target_prices = batch_data['target_price'].to(DEVICE)

            target_labels = (target_prices > 0).long()
            
            optimizer.zero_grad()
            outputs = model(price_seq, x_mark_enc, edge_index, news_features)
            
            loss = criterion(outputs.view(-1, model.mlp[-1].out_features), target_labels.view(-1))
            loss.backward()
            optimizer.step()
            
            epoch_train_loss += loss.item() * price_seq.size(0)
            
            preds = torch.argmax(outputs, dim=-1)
            epoch_train_correct += (preds == target_labels).sum().item()
            epoch_train_samples += target_labels.numel()
            
            train_pbar.set_postfix(loss=loss.item())

        avg_train_loss = epoch_train_loss / len(train_dataset)
        train_accuracy = epoch_train_correct / epoch_train_samples if epoch_train_samples > 0 else 0
        
        val_loss, val_accuracy, val_coin_accuracies = evaluate_model(model, val_loader, criterion, edge_index, DEVICE)
        
        print(f"\nEpoch {epoch+1}/{EPOCHS}")
        print(f"Train - Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.4f}")
        print(f"Val   - Loss: {val_loss:.4f}, Acc: {val_accuracy:.4f}")
        print("Val Coin Accuracies:")
        for coin, acc in val_coin_accuracies.items():
            print(f"  {coin}: {acc:.4f}")
        print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        scheduler.step(val_loss)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            if BEST_MODEL_PATH and not os.path.exists(os.path.dirname(BEST_MODEL_PATH)) and os.path.dirname(BEST_MODEL_PATH) != '':
                os.makedirs(os.path.dirname(BEST_MODEL_PATH))
            torch.save(model.state_dict(), BEST_MODEL_PATH)
            print(f"ğŸš€ New best model saved to {BEST_MODEL_PATH} (Val Loss: {best_val_loss:.4f})")
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience_early_stopping:
                print(f"â³ Early stopping after {patience_early_stopping} epochs with no improvement.")
                break
    
    # --- 7. Testing Step --- # æµ‹è¯•æ­¥éª¤
    print("\n--- 7. Starting Testing with Best Model ---")
    if os.path.exists(BEST_MODEL_PATH):
        model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE))
        print(f"Loaded best model from {BEST_MODEL_PATH}")
    else:
        print(f"Warning: Best model {BEST_MODEL_PATH} not found. Testing with last model state.")

    test_loss, test_accuracy, test_coin_accuracies = evaluate_model(model, test_loader, criterion, edge_index, DEVICE)
    print(f"\nâœ… Test Results:")
    print(f"Overall - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}")
    print("Coin-wise Accuracies:")
    for coin, acc in test_coin_accuracies.items():
        print(f"  {coin}: {acc:.4f}")

    print("\n--- Script Finished ---") 