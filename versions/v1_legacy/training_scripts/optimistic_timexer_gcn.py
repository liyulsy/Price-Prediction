import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import pandas as pd
from tqdm import tqdm
import os
import sys
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import optuna
import json

# Model and Dataset Imports
from models.MixModel.timexer_gcn import TimexerGCN
from scripts.analysis.crypto_new_analyzer.dataset import CryptoDataset, load_news_data
from dataloader.gnn_loader import generate_edge_index

# Add the project root to Python path
current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()
project_root = os.path.join(current_dir, '..', '..')
sys.path.append(project_root)


# --- TimeXer Configuration Class ---
class TimeXerConfigs:
    # TimeXeræ¨¡å‹é…ç½®å‚æ•°
    def __init__(self, num_nodes, price_seq_len, num_time_features,
                 d_model=64, pred_len=1, label_len_ratio=0.5, 
                 dropout=0.1, n_heads=4, d_ff=128, e_layers=2, factor=5,
                 patch_len=12, stride=6, freq='h', output_attention=False, embed_type='timeF',
                 use_norm: bool = False):
        self.enc_in = num_nodes # ç¼–ç å™¨è¾“å…¥ç‰¹å¾ç»´åº¦ (å³èŠ‚ç‚¹æ•°)
        self.dec_in = num_nodes # è§£ç å™¨è¾“å…¥ç‰¹å¾ç»´åº¦ (åœ¨æœ¬æ¨¡å‹ä¸­ä¸ç¼–ç å™¨ç›¸åŒ)
        self.c_out = num_nodes # è¾“å‡ºç‰¹å¾ç»´åº¦ (å³èŠ‚ç‚¹æ•°)
        self.d_model = d_model # æ¨¡å‹çš„ä¸»è¦ç‰¹å¾ç»´åº¦ (patch embeddingåçš„ç»´åº¦)
        self.seq_len = price_seq_len # è¾“å…¥åºåˆ—é•¿åº¦
        self.pred_len = pred_len # é¢„æµ‹é•¿åº¦ (æœ¬æ¨¡å‹é¢„æµ‹ä¸‹ä¸€æ—¶é—´æ­¥)
        self.label_len = int(price_seq_len * label_len_ratio) # æ ‡ç­¾é•¿åº¦ï¼Œç”¨äºTimeXerçš„Decoder (æœ¬æ¨¡å‹æœªä½¿ç”¨Decoder)
        self.output_attention = output_attention # æ˜¯å¦è¾“å‡ºæ³¨æ„åŠ›æƒé‡ (æœ¬æ¨¡å‹æœªä½¿ç”¨Decoder)
        self.embed = embed_type # æ—¶é—´ç‰¹å¾ç¼–ç ç±»å‹ (å¦‚ 'timeF', 'fixed', 'learned')
        self.freq = freq # æ—¶é—´åºåˆ—é¢‘ç‡ (å¦‚ 'h' å°æ—¶)
        self.dropout = dropout # Dropoutæ¯”ä¾‹
        self.n_heads = n_heads # Attentionå¤´çš„æ•°é‡
        self.d_ff = d_ff # å‰é¦ˆç½‘ç»œçš„éšè—å±‚ç»´åº¦
        self.factor = factor # Attentionç›¸å…³å‚æ•° (é€šå¸¸ç”¨äºè‡ªæ³¨æ„åŠ›)
        self.activation = 'gelu' # æ¿€æ´»å‡½æ•°
        self.e_layers = e_layers # ç¼–ç å™¨çš„å±‚æ•°
        self.patch_len = patch_len # Patchçš„é•¿åº¦
        self.stride = stride # Patchçš„æ»‘åŠ¨æ­¥é•¿
        self.use_norm = use_norm # æ˜¯å¦ä½¿ç”¨å½’ä¸€åŒ–å±‚
        self.num_time_features = num_time_features # æ—¶é—´ç‰¹å¾çš„æ•°é‡

# --- Main Configuration and Hyperparameters ---
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # ä½¿ç”¨çš„è®¾å¤‡ (GPUæˆ–CPU)

# Data paths
PRICE_CSV_PATH = 'datafiles/price_data/1H.csv' # åŸå§‹ä»·æ ¼æ•°æ®CSVæ–‡ä»¶è·¯å¾„
NEWS_FEATURES_FOLDER = 'crypto_new_analyzer/features' # æ–°é—»ç‰¹å¾æ–‡ä»¶å¤¹è·¯å¾„
PROCESSED_NEWS_GLOBAL_CACHE_PATH = "cache/all_processed_news_feature_10days.pt" # å¤„ç†åçš„æ–°é—»ç‰¹å¾ç¼“å­˜æ–‡ä»¶è·¯å¾„
BEST_MODEL_PATH_TEMPLATE = "cache/hpo_timexer/trial_{}_best_model.pt" # æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„æ¨¡æ¿ (ç”¨äºHPO)

# Dataset parameters
COIN_NAMES = ['BTC', 'ETH', 'BNB', 'XRP', 'LTC', 'DOGE', 'SOL', 'AVAX'] # åŠ è½½çš„å¸ç§åç§°åˆ—è¡¨ (èŠ‚ç‚¹)
PRICE_SEQ_LEN = 180 # ä»·æ ¼æ—¶é—´åºåˆ—è¾“å…¥é•¿åº¦
THRESHOLD = 0.6 # æ„å»ºå›¾æ—¶è¾¹çš„é˜ˆå€¼ (ç›¸å…³æ€§é˜ˆå€¼)
NORM_TYPE = 'standard' # ä»·æ ¼æ•°æ®å½’ä¸€åŒ–ç±»å‹ ('standard', 'minmax', 'none')
TIME_ENCODING_ENABLED_IN_DATASET = True # æ•°æ®é›†ä¸­æ˜¯å¦å¯ç”¨æ—¶é—´ç¼–ç 
TIME_FREQ_IN_DATASET = 'h' # æ•°æ®é›†ä½¿ç”¨çš„æ—¶é—´é¢‘ç‡ (ç”¨äºæ—¶é—´ç¼–ç )

# TimeXerConfigs parameters (to be instantiated later after dataset)
# è¿™äº›æ˜¯TimeXerçš„é»˜è®¤é…ç½®ï¼Œåœ¨HPOæ—¶ä¼šè¢«è¦†ç›–
D_MODEL = 64 
PATCH_LEN = 24
STRIDE = 12
E_LAYERS = 2
N_HEADS = 4
D_FF = 128
DROPOUT_TIMEXER = 0.1

# TimexerGCN specific model parameters
NEWS_PROCESSED_DIM = 32 # å¤„ç†åçš„æ–°é—»ç‰¹å¾ç»´åº¦
GCN_HIDDEN_DIM = 128 # GCNéšè—å±‚ç»´åº¦
GCN_OUTPUT_DIM = 64 # GCNè¾“å‡ºç»´åº¦
MODEL_DROPOUT = 0.3 # TimexerGCNæ¨¡å‹æ•´ä½“Dropoutæ¯”ä¾‹ ( currently not explicitly used in TimexerGCN class itself, but good to note)

# Training parameters
BATCH_SIZE = 16 # è®­ç»ƒæ‰¹é‡å¤§å°
EPOCHS = 20 # æ€»è®­ç»ƒè½®æ•° (ç”¨äºéHPOæ¨¡å¼ï¼Œæˆ–åœ¨HPOå¤–è¿è¡Œ)
LEARNING_RATE = 0.0005 # å­¦ä¹ ç‡
WEIGHT_DECAY = 1e-5 # æƒé‡è¡°å‡ (L2æ­£åˆ™åŒ–)
VALIDATION_SPLIT_RATIO = 0.15 # éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹
TEST_SPLIT_RATIO = 0.15 # æµ‹è¯•é›†åˆ’åˆ†æ¯”ä¾‹
FORCE_RECOMPUTE_NEWS_GLOBAL = False # æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—å¹¶ç¼“å­˜æ–°é—»ç‰¹å¾

BEST_MODEL_PATH = "cache/best_timexer_gcn_model_v2.pt" # æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„ (ç”¨äºéHPOæ¨¡å¼)

# HPO specific settings
EPOCHS_PER_TRIAL = 15 # æ¯ä¸ªOptuna trialè®­ç»ƒçš„è½®æ•°
N_WARMUP_STEPS_PRUNING = 5 # Optunaå‰ªæå‰çƒ­èº«æ­¥æ•°

def evaluate_model(model, data_loader, criterion, edge_index, device):
    model.eval()
    total_loss = 0.0
    total_correct = 0
    total_samples = 0
    
    all_targets_flat = []
    all_preds_flat = []

    with torch.no_grad():
        for batch_data in tqdm(data_loader, desc="Evaluating"):
            price_seq = batch_data['price_seq'].to(device)
            x_mark_enc = batch_data['price_seq_mark'].to(device)
            news_features = batch_data['news_features'].to(device)
            target_prices = batch_data['target_price'].to(device)

            target_labels = (target_prices > 0).long()
            
            outputs = model(price_seq, x_mark_enc, edge_index, news_features)
            
            loss = criterion(outputs.view(-1, model.mlp[-1].out_features), target_labels.view(-1))
            total_loss += loss.item() * price_seq.size(0)
            
            preds = torch.argmax(outputs, dim=-1)
            total_correct += (preds == target_labels).sum().item()
            total_samples += target_labels.numel()

            all_targets_flat.extend(target_labels.view(-1).cpu().numpy())
            all_preds_flat.extend(preds.view(-1).cpu().numpy())

    avg_loss = total_loss / len(data_loader.dataset)
    accuracy = total_correct / total_samples if total_samples > 0 else 0
    
    return avg_loss, accuracy

def objective(trial):
    print(f"\n--- Starting Optuna Trial {trial.number} ---")
    
    # Ensure cache directories exist ONLY for model saving per trial
    # The news cache is global and its directory should exist or be created once outside
    os.makedirs(os.path.dirname(BEST_MODEL_PATH_TEMPLATE.format(trial.number)), exist_ok=True) # ç¡®ä¿ä¿å­˜æ¨¡å‹çš„ç›®å½•å­˜åœ¨

    # --- 1. Hyperparameter Sampling ---
    # Dataset/Preprocessing HPs
    price_seq_len_hp = trial.suggest_int('price_seq_len', 60, 240, step=30) # ä»·æ ¼åºåˆ—è¾“å…¥é•¿åº¦é‡‡æ ·
    scheduler_patience_hp = trial.suggest_int('scheduler_patience', 3, 7) # å­¦ä¹ ç‡è°ƒåº¦å™¨è€å¿ƒå€¼é‡‡æ ·

    # TimeXerç›¸å…³å‚æ•°é‡‡æ ·
    d_model_hp = trial.suggest_categorical('d_model', [32, 48, 64, 96, 128]) # TimeXeræ¨¡å‹ä¸»è¦ç‰¹å¾ç»´åº¦é‡‡æ ·
    patch_len_hp = trial.suggest_categorical('patch_len', [6, 12, 24, 36, 48]) # TimeXer Patché•¿åº¦é‡‡æ ·
    stride_hp = trial.suggest_categorical('stride', [3, 6, 12, 18, 24]) # TimeXer Patchæ»‘åŠ¨æ­¥é•¿é‡‡æ ·
    e_layers_hp = trial.suggest_int('e_layers', 1, 4) # TimeXerç¼–ç å™¨å±‚æ•°é‡‡æ ·
    n_heads_hp = trial.suggest_categorical('n_heads', [2, 4, 6, 8]) # TimeXer Attentionå¤´æ•°é‡é‡‡æ ·
    d_ff_hp = trial.suggest_categorical('d_ff', [32, 64, 128, 192, 256]) # TimeXer å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦é‡‡æ ·
    dropout_hp = trial.suggest_float('dropout', 0.05, 0.5, step=0.05) # TimeXer Dropoutæ¯”ä¾‹é‡‡æ ·

    print(f"[Trial {trial.number}] TimeXerå‚æ•°: d_model={d_model_hp}, patch_len={patch_len_hp}, stride={stride_hp}, e_layers={e_layers_hp}, n_heads={n_heads_hp}, d_ff={d_ff_hp}, dropout={dropout_hp}") # æ‰“å°é‡‡æ ·çš„TimeXerå‚æ•°
    
    # --- 2. Data Loading and Preprocessing ---
    # Load raw price data (could be done once outside objective if it's large and static)
    if not os.path.exists(PRICE_CSV_PATH):
        print(f"é”™è¯¯: ä»·æ ¼æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°äº {PRICE_CSV_PATH}") # æ£€æŸ¥ä»·æ ¼æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        exit()
    if not os.path.exists(NEWS_FEATURES_FOLDER):
        print(f"é”™è¯¯: æ–°é—»ç‰¹å¾æ–‡ä»¶å¤¹æœªæ‰¾åˆ°äº {NEWS_FEATURES_FOLDER}") # æ£€æŸ¥æ–°é—»ç‰¹å¾æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        exit()
        
    price_df_original_load = pd.read_csv(PRICE_CSV_PATH, index_col=0, parse_dates=True) # è¯»å–åŸå§‹ä»·æ ¼æ•°æ®
    expected_csv_columns = [f"{coin}-USDT" for coin in COIN_NAMES] # é¢„æœŸçš„CSVåˆ—å
    missing_cols = [col for col in expected_csv_columns if col not in price_df_original_load.columns] # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼ºå¤±åˆ—
    if missing_cols:
        print(f"é”™è¯¯: CSV '{PRICE_CSV_PATH}' ç¼ºå°‘åˆ—: {missing_cols}") # æ‰“å°ç¼ºå¤±åˆ—ä¿¡æ¯å¹¶é€€å‡º
        exit()
    rename_map = {f"{coin}-USDT": coin for coin in COIN_NAMES} # æ„å»ºåˆ—åæ˜ å°„
    price_df_processed_cols = price_df_original_load.rename(columns=rename_map) # é‡å‘½ååˆ—
    price_df_final_cols = price_df_processed_cols[COIN_NAMES] # é€‰æ‹©ç›®æ ‡å¸ç§åˆ—

    # --- 3. Data Normalization (same as before) ---
    print(f"\n--- 3. Applying Data Normalization (Type: {NORM_TYPE}) ---") # æ‰“å°å½’ä¸€åŒ–ç±»å‹ä¿¡æ¯
    num_total_samples = len(price_df_final_cols) # æ€»æ ·æœ¬æ•°
    fit_train_size = int(num_total_samples * (1 - VALIDATION_SPLIT_RATIO - TEST_SPLIT_RATIO)) # è®¡ç®—ç”¨äºfit scalerçš„è®­ç»ƒé›†å¤§å°
    if fit_train_size <= 0:
        print(f"é”™è¯¯: æ•°æ®é›†å¤ªå°æˆ–åˆ’åˆ†æ¯”ä¾‹ä¸å½“. Fit train size: {fit_train_size}") # æ£€æŸ¥æ•°æ®é›†å¤§å°æ˜¯å¦è¶³å¤Ÿ
        exit()
    price_df_for_scaler_fit = price_df_final_cols.iloc[:fit_train_size] # ç”¨äºfit scalerçš„æ•°æ®
    price_df_to_normalize = price_df_final_cols.copy() # å¾…å½’ä¸€åŒ–çš„æ•°æ®å‰¯æœ¬
    if NORM_TYPE == 'standard':
        scaler = StandardScaler() # æ ‡å‡†åŒ–Scaler
        price_df_values_full = scaler.fit_transform(price_df_to_normalize) # Fitå¹¶è½¬æ¢
        price_df_normalized = pd.DataFrame(price_df_values_full, columns=price_df_to_normalize.columns, index=price_df_to_normalize.index) # è½¬æ¢ä¸ºDataFrame
    elif NORM_TYPE == 'minmax':
        scaler = MinMaxScaler() # MinMaxScaler
        price_df_values_full = scaler.fit_transform(price_df_to_normalize) # Fitå¹¶è½¬æ¢
        price_df_normalized = pd.DataFrame(price_df_values_full, columns=price_df_to_normalize.columns, index=price_df_to_normalize.index) # è½¬æ¢ä¸ºDataFrame
    elif NORM_TYPE == 'none':
        price_df_normalized = price_df_to_normalize # ä¸è¿›è¡Œå½’ä¸€åŒ–
    else:
        price_df_normalized = price_df_to_normalize
        print(f"è­¦å‘Š: æœªçŸ¥çš„NORM_TYPE '{NORM_TYPE}'ã€‚æœªå¯¹ä»·æ ¼æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚") # æœªçŸ¥å½’ä¸€åŒ–ç±»å‹è­¦å‘Š

    news_data = load_news_data(NEWS_FEATURES_FOLDER, COIN_NAMES) # åŠ è½½æ–°é—»æ•°æ®
    
    dataset = CryptoDataset(
        price_data_df=price_df_normalized, # å½’ä¸€åŒ–åçš„ä»·æ ¼æ•°æ®
        news_data_dict=news_data, # æ–°é—»æ•°æ®å­—å…¸
        seq_len=price_seq_len_hp, # è¾“å…¥åºåˆ—é•¿åº¦ (æ¥è‡ªHPOé‡‡æ ·)
        processed_news_features_path=PROCESSED_NEWS_GLOBAL_CACHE_PATH, # å¤„ç†åçš„æ–°é—»ç‰¹å¾ç¼“å­˜è·¯å¾„
        force_recompute_news=FORCE_RECOMPUTE_NEWS_GLOBAL, # æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—æ–°é—»ç‰¹å¾
        time_encoding_enabled=TIME_ENCODING_ENABLED_IN_DATASET, # æ˜¯å¦å¯ç”¨æ—¶é—´ç¼–ç 
        time_freq=TIME_FREQ_IN_DATASET # æ—¶é—´é¢‘ç‡ (ç”¨äºæ—¶é—´ç¼–ç )
    ) # åˆ›å»ºCryptoDatasetå®ä¾‹
    
    if len(dataset) == 0:
        print("é”™è¯¯: æ•°æ®é›†ä¸ºç©ºã€‚") # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
        exit()

    total_size = len(dataset) # æ•°æ®é›†æ€»å¤§å°
    test_size = int(TEST_SPLIT_RATIO * total_size) # æµ‹è¯•é›†å¤§å°
    val_size = int(VALIDATION_SPLIT_RATIO * total_size) # éªŒè¯é›†å¤§å°
    train_size = total_size - test_size - val_size # è®­ç»ƒé›†å¤§å°
    if train_size <= 0 or val_size <= 0 or test_size <= 0:
        print(f"é”™è¯¯: æ•°æ®é›†å¤ªå°æ— æ³•åˆ’åˆ†. Train: {train_size}, Val: {val_size}, Test: {test_size}") # æ£€æŸ¥æ•°æ®é›†åˆ’åˆ†æ˜¯å¦æœ‰æ•ˆ
        exit()
    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size]) # åˆ’åˆ†æ•°æ®é›†
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0) # è®­ç»ƒé›†DataLoader
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0) # éªŒè¯é›†DataLoader
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0) # æµ‹è¯•é›†DataLoader
    edge_index = generate_edge_index(price_df_final_cols, THRESHOLD).to(DEVICE) # ç”Ÿæˆå¹¶ç§»åŠ¨edge_indexåˆ°è®¾å¤‡

    # --- 5. Initialize Model, Loss, Optimizer ---
    print("\n--- 5. Initializing Model, Loss, Optimizer ---") # æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
    
    timexer_model_configs = TimeXerConfigs(
        num_nodes=dataset.num_coins, # èŠ‚ç‚¹æ•°
        price_seq_len=price_seq_len_hp, # ä»·æ ¼åºåˆ—é•¿åº¦ (æ¥è‡ªHPOé‡‡æ ·)
        num_time_features=dataset.num_actual_time_features, # æ—¶é—´ç‰¹å¾æ•°é‡
        d_model=d_model_hp, # TimeXerç‰¹å¾ç»´åº¦ (æ¥è‡ªHPOé‡‡æ ·)
        patch_len=patch_len_hp, # Patché•¿åº¦ (æ¥è‡ªHPOé‡‡æ ·)
        stride=stride_hp, # Patchæ­¥é•¿ (æ¥è‡ªHPOé‡‡æ ·)
        e_layers=e_layers_hp, # TimeXerç¼–ç å™¨å±‚æ•° (æ¥è‡ªHPOé‡‡æ ·)
        n_heads=n_heads_hp, # Attentionå¤´æ•°é‡ (æ¥è‡ªHPOé‡‡æ ·)
        d_ff=d_ff_hp, # å‰é¦ˆç½‘ç»œç»´åº¦ (æ¥è‡ªHPOé‡‡æ ·)
        dropout=dropout_hp, # Dropoutæ¯”ä¾‹ (æ¥è‡ªHPOé‡‡æ ·)
        freq=TIME_FREQ_IN_DATASET # æ—¶é—´é¢‘ç‡
    ) # åˆ›å»ºTimeXerConfigså®ä¾‹
    print(f"TimeXer Configs: enc_in={timexer_model_configs.enc_in}, seq_len={timexer_model_configs.seq_len}, d_model={timexer_model_configs.d_model}, num_time_feat={timexer_model_configs.num_time_features}") # æ‰“å°TimeXeré…ç½®

    model = TimexerGCN(
        configs=timexer_model_configs, # TimeXeré…ç½®
        hidden_dim=GCN_HIDDEN_DIM, # GCNéšè—å±‚ç»´åº¦
        output_dim=GCN_OUTPUT_DIM, # GCNè¾“å‡ºç»´åº¦
        news_feature_dim=dataset.news_feature_dim, # æ–°é—»ç‰¹å¾åŸå§‹ç»´åº¦
        news_processed_dim=NEWS_PROCESSED_DIM, # æ–°é—»ç‰¹å¾å¤„ç†åç»´åº¦
    ).to(DEVICE) # åˆ›å»ºTimexerGCNæ¨¡å‹å®ä¾‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    print(model) # æ‰“å°æ¨¡å‹ç»“æ„

    criterion = nn.CrossEntropyLoss() # äº¤å‰ç†µæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) # Adamä¼˜åŒ–å™¨
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=scheduler_patience_hp, factor=0.5) # å­¦ä¹ ç‡è°ƒåº¦å™¨

    # --- 6. Training Loop ---
    print("\n--- 6. Starting Training ---") # æ‰“å°è®­ç»ƒå¼€å§‹ä¿¡æ¯
    best_val_loss_for_trial = float('inf') # è®°å½•å½“å‰trialçš„æœ€ä½³éªŒè¯æŸå¤±
    best_val_accuracy_for_trial = 0.0 # è®°å½•å½“å‰trialçš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡
    epochs_no_improve_trial = 0 # è®°å½•éªŒè¯æŸå¤±è¿ç»­æœªæ”¹å–„çš„epochæ•°
    
    for epoch in range(EPOCHS_PER_TRIAL): # éå†æ¯ä¸ªtrialçš„è®­ç»ƒepoch
        model.train() # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        epoch_train_loss = 0.0 # å½“å‰epochè®­ç»ƒæŸå¤±æ€»å’Œ
        epoch_train_correct = 0 # å½“å‰epochè®­ç»ƒæ­£ç¡®é¢„æµ‹æ•°
        epoch_train_samples = 0 # å½“å‰epochè®­ç»ƒæ€»æ ·æœ¬æ•°
        
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS_PER_TRIAL} Training", leave=False) # è®­ç»ƒè¿›åº¦æ¡
        for batch_data in train_pbar: # éå†è®­ç»ƒé›†æ‰¹æ¬¡
            price_seq = batch_data['price_seq'].to(DEVICE) # ä»·æ ¼åºåˆ—æ•°æ®
            x_mark_enc = batch_data['price_seq_mark'].to(DEVICE) # æ—¶é—´æ ‡è®°æ•°æ®
            news_features = batch_data['news_features'].to(DEVICE) # æ–°é—»ç‰¹å¾æ•°æ®
            target_prices = batch_data['target_price'].to(DEVICE) # ç›®æ ‡ä»·æ ¼

            target_labels = (target_prices > 0).long() # å°†ç›®æ ‡ä»·æ ¼è½¬æ¢ä¸ºæ ‡ç­¾ (ä¸Šæ¶¨/ä¸‹è·Œ)
            
            optimizer.zero_grad() # æ¢¯åº¦æ¸…é›¶
            outputs = model(price_seq, x_mark_enc, edge_index, news_features) # æ¨¡å‹å‰å‘ä¼ æ’­
            
            loss = criterion(outputs.view(-1, model.mlp[-1].out_features), target_labels.view(-1)) # è®¡ç®—æŸå¤±
            loss.backward() # åå‘ä¼ æ’­
            optimizer.step() # æ›´æ–°æ¨¡å‹å‚æ•°
            
            epoch_train_loss += loss.item() * price_seq.size(0) # ç´¯åŠ è®­ç»ƒæŸå¤±
            
            preds = torch.argmax(outputs, dim=-1) # è·å–é¢„æµ‹ç»“æœ
            epoch_train_correct += (preds == target_labels).sum().item() # ç´¯åŠ æ­£ç¡®é¢„æµ‹æ•°
            epoch_train_samples += target_labels.numel() # ç´¯åŠ æ€»æ ·æœ¬æ•°
            
            train_pbar.set_postfix(loss=loss.item()) # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º

        avg_train_loss = epoch_train_loss / len(train_dataset) # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        train_accuracy = epoch_train_correct / epoch_train_samples if epoch_train_samples > 0 else 0 # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
        
        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, edge_index, DEVICE) # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
        
        print(f"Epoch {epoch+1}/{EPOCHS_PER_TRIAL} - Train Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_accuracy:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}") # æ‰“å°epochç»“æœ
        
        scheduler.step(val_loss) # æ ¹æ®éªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡
        
        if val_loss < best_val_loss_for_trial: # å¦‚æœå½“å‰éªŒè¯æŸå¤±æ˜¯å½“å‰trialæœ€ä½³
            best_val_loss_for_trial = val_loss # æ›´æ–°æœ€ä½³éªŒè¯æŸå¤±
            best_val_accuracy_for_trial = val_accuracy # æ›´æ–°æœ€ä½³éªŒè¯å‡†ç¡®ç‡
            torch.save(model.state_dict(), BEST_MODEL_PATH_TEMPLATE.format(trial.number)) # ä¿å­˜æœ€ä½³æ¨¡å‹å‚æ•°
            print(f"ğŸš€ New best model saved to {BEST_MODEL_PATH_TEMPLATE.format(trial.number)} (Val Loss: {best_val_loss_for_trial:.4f})") # æ‰“å°æ¨¡å‹ä¿å­˜ä¿¡æ¯
            epochs_no_improve_trial = 0 # é‡ç½®æœªæ”¹å–„epochè®¡æ•°
        else:
            epochs_no_improve_trial += 1 # å¢åŠ æœªæ”¹å–„epochè®¡æ•°
            if epochs_no_improve_trial >= scheduler_patience_hp: # å¦‚æœè¾¾åˆ°è€å¿ƒå€¼
                print(f"â³ Early stopping after {scheduler_patience_hp} epochs with no improvement.") # æ‰“å°æ—©åœä¿¡æ¯
                break # æ—©åœ
        
        if trial.should_prune(): # å¦‚æœOptunaå†³å®šå‰ªæå½“å‰trial
            raise optuna.exceptions.TrialPruned(f"Trial {trial.number} pruned at epoch {epoch+1}.") # æŠ›å‡ºTrialPrunedå¼‚å¸¸
            
    # ä¿å­˜trialçš„æœ€ä½³éªŒè¯æŸå¤±ã€å‡†ç¡®ç‡å’Œå‚æ•°
    metrics = {
        "trial_number": trial.number, # trialç¼–å·
        "best_val_loss": best_val_loss_for_trial, # æœ€ä½³éªŒè¯æŸå¤±
        "best_val_accuracy": best_val_accuracy_for_trial, # æœ€ä½³éªŒè¯å‡†ç¡®ç‡
        "params": dict(trial.params) # trialå‚æ•°å­—å…¸
    }
    metrics_path = BEST_MODEL_PATH_TEMPLATE.format(trial.number).replace(".pt", "_metrics.json") # æŒ‡æ ‡ä¿å­˜æ–‡ä»¶è·¯å¾„
    with open(metrics_path, "w") as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False) # å°†æŒ‡æ ‡ä¿å­˜ä¸ºJSONæ–‡ä»¶
    print(f"[Trial {trial.number}] Metrics saved to {metrics_path}") # æ‰“å°æŒ‡æ ‡ä¿å­˜è·¯å¾„
    return best_val_loss_for_trial # è¿”å›æœ€ä½³éªŒè¯æŸå¤±ä½œä¸ºtrialçš„ç»“æœå€¼

# --- Main Execution for HPO ---
if __name__ == '__main__':
    print(f"Using device: {DEVICE}") # æ‰“å°ä½¿ç”¨çš„è®¾å¤‡

    # Ensure the global news cache directory exists once
    if PROCESSED_NEWS_GLOBAL_CACHE_PATH:
        global_cache_dir = os.path.dirname(PROCESSED_NEWS_GLOBAL_CACHE_PATH)
        if global_cache_dir and not os.path.exists(global_cache_dir): # Ensure dirname is not empty
             os.makedirs(global_cache_dir, exist_ok=True) # åˆ›å»ºå…¨å±€æ–°é—»ç¼“å­˜ç›®å½•
             print(f"Ensured global news cache directory exists: {global_cache_dir}") # æ‰“å°ç¼“å­˜ç›®å½•åˆ›å»ºä¿¡æ¯
    
    study = optuna.create_study(direction='minimize', # ä¼˜åŒ–æ–¹å‘ï¼šæœ€å°åŒ– (éªŒè¯æŸå¤±)
                                pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=N_WARMUP_STEPS_PRUNING, interval_steps=1)) # ä½¿ç”¨MedianPrunerè¿›è¡Œå‰ªæ
    
    try:
        study.optimize(objective, n_trials=20) # è¿è¡ŒOptunaä¼˜åŒ–ï¼ŒæŒ‡å®šobjectiveå‡½æ•°å’Œtrialæ•°é‡
    except KeyboardInterrupt:
        print("HPO interrupted. Proceeding with the best trial found so far.") # æ•è·ä¸­æ–­ä¿¡å·

    print("Number of finished trials: ", len(study.trials)) # æ‰“å°å®Œæˆçš„trialæ•°é‡
    print("Best trial:") # æ‰“å°æœ€ä½³trialä¿¡æ¯
    trial = study.best_trial # è·å–æœ€ä½³trial
    print("  Value: ", trial.value) # æ‰“å°æœ€ä½³trialçš„å€¼ (æœ€ä½³éªŒè¯æŸå¤±)
    print("  Params: ") # æ‰“å°æœ€ä½³trialçš„å‚æ•°
    for key, value in trial.params.items():
        print("    {}: {}".format(key, value)) # æ‰“å°æ¯ä¸ªå‚æ•°åŠå…¶å€¼