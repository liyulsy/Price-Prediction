#!/usr/bin/env python3
"""
è°ƒè¯•ç‰ˆæœ¬çš„è®­ç»ƒè„šæœ¬ - ç”¨äºè¯Šæ–­è®­ç»ƒä¸­æ–­é—®é¢˜
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, random_split
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
import traceback

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# ç®€åŒ–çš„é…ç½®
DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
BATCH_SIZE = 8  # éå¸¸å°çš„æ‰¹æ¬¡å¤§å°
PRICE_SEQ_LEN = 20  # çŸ­åºåˆ—
EPOCHS = 2  # å°‘é‡epochç”¨äºæµ‹è¯•

print(f"ğŸ”§ è°ƒè¯•æ¨¡å¼å¯åŠ¨")
print(f"è®¾å¤‡: {DEVICE}")
print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
print(f"åºåˆ—é•¿åº¦: {PRICE_SEQ_LEN}")

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨"""
    if torch.cuda.is_available():
        memory_allocated = torch.cuda.memory_allocated(DEVICE) / 1024**3
        memory_reserved = torch.cuda.memory_reserved(DEVICE) / 1024**3
        print(f"GPUå†…å­˜ - å·²åˆ†é…: {memory_allocated:.2f}GB, å·²ä¿ç•™: {memory_reserved:.2f}GB")
        return memory_allocated
    return 0

def main():
    """ä¸»å‡½æ•°"""
    try:
        print("\n=== æ­¥éª¤1: æ£€æŸ¥åˆå§‹GPUå†…å­˜ ===")
        check_gpu_memory()
        
        print("\n=== æ­¥éª¤2: åŠ è½½æ•°æ® ===")
        # åŠ è½½ä»·æ ¼æ•°æ®
        price_path = 'scripts/analysis/crypto_analysis/data/processed_data/1H/all_1H.csv'
        if not os.path.exists(price_path):
            print(f"âŒ ä»·æ ¼æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {price_path}")
            return
            
        price_df_raw = pd.read_csv(price_path, index_col=0, parse_dates=True)
        coin_names = ['BTC', 'ETH', 'BNB', 'XRP']  # åªä½¿ç”¨4ä¸ªå¸ç§
        rename_map = {f"{coin}-USDT": coin for coin in coin_names}
        price_df_full = price_df_raw.rename(columns=rename_map)[coin_names]
        
        print(f"âœ… ä»·æ ¼æ•°æ®åŠ è½½æˆåŠŸ: {price_df_full.shape}")
        
        # æ•°æ®å½’ä¸€åŒ–
        scaler = StandardScaler()
        price_values = scaler.fit_transform(price_df_full)
        price_df = pd.DataFrame(price_values, columns=price_df_full.columns, index=price_df_full.index)
        
        print("\n=== æ­¥éª¤3: åˆ›å»ºç®€å•æ•°æ®é›† ===")
        # åˆ›å»ºç®€å•çš„æ•°æ®é›†
        sequences = []
        targets = []
        
        for i in range(PRICE_SEQ_LEN, len(price_df) - 1):
            seq = price_df.iloc[i-PRICE_SEQ_LEN:i].values
            target = price_df.iloc[i+1].values
            sequences.append(seq)
            targets.append(target)
            
            if len(sequences) >= 1000:  # é™åˆ¶æ•°æ®é‡
                break
        
        sequences = torch.FloatTensor(sequences)
        targets = torch.FloatTensor(targets)
        
        print(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸ: {sequences.shape}, {targets.shape}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataset = torch.utils.data.TensorDataset(sequences, targets)
        train_size = int(0.8 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
        
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
        
        print(f"è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}")
        
        print("\n=== æ­¥éª¤4: åˆ›å»ºç®€å•æ¨¡å‹ ===")
        check_gpu_memory()
        
        # ç®€å•çš„LSTMæ¨¡å‹
        class SimpleLSTM(nn.Module):
            def __init__(self, input_size, hidden_size, output_size):
                super().__init__()
                self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
                self.fc = nn.Linear(hidden_size, output_size)
                
            def forward(self, x):
                lstm_out, _ = self.lstm(x)
                output = self.fc(lstm_out[:, -1, :])
                return output
        
        model = SimpleLSTM(input_size=len(coin_names), hidden_size=32, output_size=len(coin_names))
        model = model.to(DEVICE)
        
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        check_gpu_memory()
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        
        print("\n=== æ­¥éª¤5: å¼€å§‹è®­ç»ƒ ===")
        
        for epoch in range(EPOCHS):
            print(f"\n--- Epoch {epoch+1}/{EPOCHS} ---")
            
            # è®­ç»ƒ
            model.train()
            train_loss = 0
            train_batches = 0
            
            for batch_idx, (batch_seq, batch_target) in enumerate(train_loader):
                try:
                    batch_seq = batch_seq.to(DEVICE)
                    batch_target = batch_target.to(DEVICE)
                    
                    optimizer.zero_grad()
                    output = model(batch_seq)
                    loss = criterion(output, batch_target)
                    loss.backward()
                    optimizer.step()
                    
                    train_loss += loss.item()
                    train_batches += 1
                    
                    if batch_idx % 10 == 0:
                        print(f"  æ‰¹æ¬¡ {batch_idx}: Loss = {loss.item():.4f}")
                        check_gpu_memory()
                        
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print(f"âŒ GPUå†…å­˜ä¸è¶³åœ¨æ‰¹æ¬¡ {batch_idx}: {e}")
                        torch.cuda.empty_cache()
                        return
                    else:
                        raise e
                        
            avg_train_loss = train_loss / train_batches
            print(f"å¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            
            # éªŒè¯
            model.eval()
            val_loss = 0
            val_batches = 0
            
            with torch.no_grad():
                for batch_seq, batch_target in val_loader:
                    batch_seq = batch_seq.to(DEVICE)
                    batch_target = batch_target.to(DEVICE)
                    
                    output = model(batch_seq)
                    loss = criterion(output, batch_target)
                    val_loss += loss.item()
                    val_batches += 1
                    
            avg_val_loss = val_loss / val_batches
            print(f"å¹³å‡éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
            
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
            check_gpu_memory()
        
        print("\nâœ… è°ƒè¯•è®­ç»ƒå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•è®­ç»ƒå¤±è´¥: {e}")
        traceback.print_exc()
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        print("\nğŸ”§ å»ºè®®çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. è¿›ä¸€æ­¥å‡å°‘æ‰¹æ¬¡å¤§å°")
        print("2. å‡å°‘åºåˆ—é•¿åº¦")
        print("3. ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹")
        print("4. æ£€æŸ¥æ•°æ®é¢„å¤„ç†")
        print("5. ç¡®ä¿æ­£ç¡®çš„condaç¯å¢ƒ")

if __name__ == "__main__":
    main()
