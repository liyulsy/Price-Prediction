import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
from models.MixModel.lstm_gcn import LstmGcn
from dataloader.gnn_loader import load_gnn_data, create_gnn_dataloaders

file_path = 'Project1/datafiles/1H.csv'
input_dim = 512
hidden_dim = 128
output_dim = 32
batch_size = 32
epochs = 10
threshold = 0.6

# num_nodes = 8
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
coin_features_list, coin_labels_list, edge_index = load_gnn_data(
    file_path=file_path,
    input_dim=input_dim,
    threshold=threshold,
    task="classification",
    norm_type='standard'
)
coin_train_loaders, coin_test_loaders = create_gnn_dataloaders(
    coin_features_list,
    coin_labels_list,
    batch_size=batch_size,
    test_size=0.2,
    task="classification"
)
    
    # åˆå§‹åŒ–æ¨¡å‹
model = LstmGcn(1, hidden_dim, output_dim).to(device)
    
# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', patience=3, factor=0.5, verbose=True
)

for epoch in range(epochs):
    model.train()
    epoch_loss = 0.0

    # tqdm å¤–å±‚æ˜¾ç¤º Epoch
    batch_iterator = zip(*coin_train_loaders)
    num_batches = min(len(loader) for loader in coin_train_loaders)  # æœ€å°çš„ batch æ•°é‡ä½œä¸ºè¿­ä»£æ¬¡æ•°
    pbar = tqdm(batch_iterator, total=num_batches, desc=f"Epoch {epoch+1}/{epochs}")

    for batches in pbar:
        coin_data_list = []
        target_list = []

        for data, target in batches:
            coin_data_list.append(data.to(device))
            target_list.append(target.to(device))

        input_data = torch.stack(coin_data_list, dim=1)
        target = torch.stack(target_list, dim=1)

        optimizer.zero_grad()
        output = model(input_data, edge_index=edge_index.to(device))
        output_flat = output.view(-1, 2)
        target_flat = target.long().view(-1)

        loss = criterion(output_flat, target_flat)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        pbar.set_postfix(loss=loss.item())  # æ¯ä¸ª batch æ›´æ–°ä¸€æ¬¡ loss

    avg_loss = epoch_loss / num_batches
    # æ·»åŠ scheduleræ­¥è¿›
    scheduler.step(avg_loss)
    
    # æ‰“å°å½“å‰å­¦ä¹ ç‡
    current_lr = optimizer.param_groups[0]['lr']
    print(f"âœ… Epoch {epoch+1}/{epochs} completed. Avg Loss: {avg_loss:.4f}, Total Loss: {epoch_loss:.4f}ï¼ŒLR: {current_lr:.6f}")

model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
total_correct = 0
total_samples = 0
total_loss = 0.0

# åˆå§‹åŒ–æ€»è®¡æ•°å™¨
total_target_zeros = 0
total_target_ones = 0
total_pred_zeros = 0
total_pred_ones = 0

# tqdm å¤–å±‚æ˜¾ç¤º Epoch
batch_iterator = zip(*coin_test_loaders)
num_batches = min(len(loader) for loader in coin_test_loaders)  # æœ€å°çš„ batch æ•°é‡ä½œä¸ºè¿­ä»£æ¬¡æ•°
pbar = tqdm(batch_iterator, total=num_batches, desc="Testing")

with torch.no_grad():  # æµ‹è¯•æ—¶ä¸è®¡ç®—æ¢¯åº¦
    for batches in pbar:
        coin_data_list = []
        target_list = []

        for data, target in batches:
            coin_data_list.append(data.to(device))
            target_list.append(target.to(device))

        # å †å æˆ [batch_size, num_coins, input_dim]
        input_data = torch.stack(coin_data_list, dim=1)
        target = torch.stack(target_list, dim=1)

        # å‰å‘ä¼ æ’­
        output = model(input_data, edge_index=edge_index.to(device))  # è¾“å‡ºå½¢çŠ¶ [batch_size, num_coins, num_classes]
        output_flat = output.view(-1, 2)
        target_flat = target.long().view(-1)

        # è®¡ç®—æŸå¤±
        loss = criterion(output_flat, target_flat)
        total_loss += loss.item()

        # å‡†ç¡®ç‡è®¡ç®—
        preds = torch.argmax(output_flat, dim=1)
        correct = (preds == target_flat).sum().item()

        total_correct += correct
        total_samples += target_flat.size(0)

        # æœ¬ batch ä¸­ 0ã€1 çš„æ•°é‡
        target_zeros = (target_flat == 0).sum().item()
        target_ones = (target_flat == 1).sum().item()
        pred_zeros = (preds == 0).sum().item()
        pred_ones = (preds == 1).sum().item()

        # ç´¯åŠ æ€»æ•°
        total_target_zeros += target_zeros
        total_target_ones += target_ones
        total_pred_zeros += pred_zeros
        total_pred_ones += pred_ones

        # æ›´æ–°è¿›åº¦æ¡çš„åç¼€ï¼Œæ˜¾ç¤ºæ‰¹æ¬¡çš„æŸå¤±ã€ç›®æ ‡å’Œé¢„æµ‹ä¸­ 0 å’Œ 1 çš„æ•°é‡
        pbar.set_postfix(loss=loss.item())

    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = total_loss / num_batches

# è¾“å‡ºæ€»æµ‹è¯•é›†ç»“æœ
accuracy = total_correct / total_samples
print(f"\nâœ… Test completed.")
print(f"ğŸ“Š Avg Loss: {avg_loss:.4f}, Total Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}")
print(f"ğŸ”¢ Total Target - 0s: {total_target_zeros}, 1s: {total_target_ones}")
print(f"ğŸ”¢ Total Predicted - 0s: {total_pred_zeros}, 1s: {total_pred_ones}")